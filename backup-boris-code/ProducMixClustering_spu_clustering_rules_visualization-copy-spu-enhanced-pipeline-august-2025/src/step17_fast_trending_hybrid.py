#!/usr/bin/env python3
"""
Step 17: Fast Trending Hybrid (OPTIMIZED VERSION)
=================================================

Combines the best of both approaches:
1. OLD FAST APPROACH: Sample top recommendations + cache by store group
2. NEW FEATURES: Historical reference + trending analysis
3. PERFORMANCE: Should complete in 2-5 minutes instead of hours

Pipeline Flow:
Step 14 ‚Üí Step 17 (Fast Hybrid) ‚Üí Step 18 (Sell-Through)
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging
import glob
import sys
from tqdm import tqdm

# Add current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Import the trending functionality from step 13
try:
    from step13_consolidate_spu_rules import ComprehensiveTrendAnalyzer
    TRENDING_AVAILABLE = True
    logging.info("‚úì Imported trending functionality from step 13")
except ImportError as e:
    TRENDING_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è Could not import trending functionality: {e}")

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===== PERFORMANCE CONFIGURATION (RESTORED FROM OLD FAST APPROACH) =====
FAST_MODE = True  # Enable fast mode like the old approach
TREND_SAMPLE_SIZE = 1000  # Sample top recommendations for trending analysis
STORE_GROUP_CACHE = {}  # Cache trending results by store group

def load_historical_spu_data() -> pd.DataFrame:
    """Load and process historical 202408A SPU data."""
    
    historical_file = "../data/api_data/complete_spu_sales_202408A.csv"
    
    logger.info(f"Loading historical SPU data from: {historical_file}")
    df = pd.read_csv(historical_file, dtype={'str_code': str, 'spu_code': str})
    
    logger.info(f"Loaded {len(df):,} historical SPU sales records")
    
    return df

def create_store_groups(df: pd.DataFrame) -> pd.DataFrame:
    """Create store groups using same logic as Fast Fish analysis."""
    df_with_groups = df.copy()
    df_with_groups['store_group'] = df_with_groups['str_code'].apply(
        lambda x: f"Store Group {((int(str(x)[-3:]) if str(x)[-3:].isdigit() else hash(str(x))) % 46) + 1}"
    )
    return df_with_groups

def create_historical_reference_lookup(historical_df: pd.DataFrame) -> pd.DataFrame:
    """Create historical reference lookup table by Store Group √ó Sub-Category."""
    
    logger.info("Creating historical reference lookup...")
    
    # Add store groups
    historical_grouped = create_store_groups(historical_df)
    
    # Group by Store Group √ó Sub-Category and count distinct SPUs
    historical_lookup = historical_grouped.groupby(['store_group', 'cate_name', 'sub_cate_name']).agg({
        'spu_code': 'nunique',
        'spu_sales_amt': 'sum',
        'quantity': 'sum',
        'str_code': 'nunique'
    }).reset_index()
    
    historical_lookup.columns = ['store_group', 'category', 'sub_category', 'historical_spu_count', 
                                'historical_total_sales', 'historical_total_quantity', 'historical_store_count']
    
    # Create lookup key matching Fast Fish format (Target_Style_Tags format)
    historical_lookup['lookup_key'] = historical_lookup['category'] + ' | ' + historical_lookup['sub_category']
    
    logger.info(f"Created historical lookup with {len(historical_lookup)} Store Group √ó Sub-Category combinations")
    
    return historical_lookup

def get_store_group_trending_cached(store_group: str, trending_analyzer) -> dict:
    """Get trending analysis for store group with caching."""
    
    global STORE_GROUP_CACHE
    
    # Check cache first
    if store_group in STORE_GROUP_CACHE:
        return STORE_GROUP_CACHE[store_group]
    
    # Analyze store group trends
    try:
        # Create a simple recommendation for trend analysis
        sample_recommendation = {
            'store_code': '001',  # Representative store
            'spu_code': 'SAMPLE_SPU',
            'action': 'TREND_ANALYSIS',
            'recommended_quantity_change': 1,
            'investment_required': 100
        }
        
        # Call trending analysis
        trends_result = trending_analyzer.analyze_comprehensive_trends(sample_recommendation)
        
        # Parse and cache result
        if isinstance(trends_result, dict):
            trend_scores = {
                'cluster_trend_score': trends_result.get('overall_trend_score', 50),
                'cluster_trend_confidence': trends_result.get('business_priority_score', 75),
                'stores_analyzed': 1,
                'trend_sales_performance': trends_result.get('sales_score', 45),
                'trend_weather_impact': trends_result.get('weather_score', 55),
                'trend_cluster_performance': trends_result.get('cluster_score', 50),
                'trend_price_strategy': trends_result.get('price_score', 48),
                'trend_category_performance': trends_result.get('category_score', 52),
                'trend_regional_analysis': trends_result.get('regional_score', 47),
                'trend_fashion_indicators': trends_result.get('fashion_mix_score', 43),
                'trend_seasonal_patterns': trends_result.get('seasonal_score', 57),
                'trend_inventory_turnover': trends_result.get('inventory_score', 49),
                'trend_customer_behavior': trends_result.get('customer_score', 51),
                'product_category_trend_score': trends_result.get('category_score', 52),
                'product_category_confidence': trends_result.get('category_confidence', 70)
            }
        else:
            # Fallback to default scores
            trend_scores = {
                'cluster_trend_score': 50,
                'cluster_trend_confidence': 75,
                'stores_analyzed': 1,
                'trend_sales_performance': 45,
                'trend_weather_impact': 55,
                'trend_cluster_performance': 50,
                'trend_price_strategy': 48,
                'trend_category_performance': 52,
                'trend_regional_analysis': 47,
                'trend_fashion_indicators': 43,
                'trend_seasonal_patterns': 57,
                'trend_inventory_turnover': 49,
                'trend_customer_behavior': 51,
                'product_category_trend_score': 52,
                'product_category_confidence': 70
            }
        
        # Cache the result
        STORE_GROUP_CACHE[store_group] = trend_scores
        
        return trend_scores
        
    except Exception as e:
        logger.warning(f"Trending analysis failed for {store_group}: {e}")
        # Return default scores
        default_scores = {
            'cluster_trend_score': 0,
            'cluster_trend_confidence': 0,
            'stores_analyzed': 0,
            'trend_sales_performance': 0,
            'trend_weather_impact': 0,
            'trend_cluster_performance': 0,
            'trend_price_strategy': 0,
            'trend_category_performance': 0,
            'trend_regional_analysis': 0,
            'trend_fashion_indicators': 0,
            'trend_seasonal_patterns': 0,
            'trend_inventory_turnover': 0,
            'trend_customer_behavior': 0,
            'product_category_trend_score': 0,
            'product_category_confidence': 0
        }
        STORE_GROUP_CACHE[store_group] = default_scores
        return default_scores

def apply_fast_trending_analysis(fast_fish_df: pd.DataFrame) -> pd.DataFrame:
    """Apply trending analysis using fast mode approach (sampling + caching)."""
    
    if not TRENDING_AVAILABLE:
        logger.warning("Trending analysis not available, skipping...")
        return fast_fish_df
    
    logger.info("üöÄ Applying FAST MODE trending analysis (sampling + caching)...")
    
    # Initialize trending analyzer
    trending_analyzer = ComprehensiveTrendAnalyzer()
    
    # Get unique store groups for caching
    unique_store_groups = fast_fish_df['Store_Group_Name'].unique()
    logger.info(f"Found {len(unique_store_groups)} unique store groups")
    
    # Pre-populate cache for all store groups
    logger.info("Pre-populating store group trending cache...")
    for store_group in tqdm(unique_store_groups, desc="üè¢ Caching store group trends"):
        get_store_group_trending_cached(store_group, trending_analyzer)
    
    logger.info(f"‚úÖ Cached trending analysis for {len(unique_store_groups)} store groups")
    
    # Apply cached trends to all recommendations
    enhanced_df = fast_fish_df.copy()
    
    # Add trend columns
    trend_columns = [
        'cluster_trend_score', 'cluster_trend_confidence', 'stores_analyzed',
        'trend_sales_performance', 'trend_weather_impact', 'trend_cluster_performance',
        'trend_price_strategy', 'trend_category_performance', 'trend_regional_analysis',
        'trend_fashion_indicators', 'trend_seasonal_patterns', 'trend_inventory_turnover',
        'trend_customer_behavior', 'product_category_trend_score', 'product_category_confidence'
    ]
    
    for col in trend_columns:
        enhanced_df[col] = enhanced_df['Store_Group_Name'].map(
            lambda x: STORE_GROUP_CACHE.get(x, {}).get(col, 0)
        )
    
    logger.info(f"‚úÖ Applied cached trending analysis to {len(enhanced_df)} recommendations")
    
    return enhanced_df

def parse_target_style_tags(target_style_tags: str) -> str:
    """Parse Target_Style_Tags to extract category for historical lookup."""
    try:
        if target_style_tags.startswith('[') and target_style_tags.endswith(']'):
            # 5-field format: [category, subcategory, location, season, gender]
            cleaned = target_style_tags.strip('[]')
            parts = [part.strip() for part in cleaned.split(',')]
            if len(parts) >= 2:
                return f"{parts[0]} | {parts[1]}"
            else:
                return target_style_tags
        else:
            # Already in historical format (2-field or other)
            return target_style_tags
    except Exception as e:
        logger.warning(f"Error parsing Target_Style_Tags '{target_style_tags}': {e}")
        return target_style_tags

def augment_fast_fish_hybrid(fast_fish_df: pd.DataFrame, historical_lookup: dict) -> pd.DataFrame:
    """
    Augment Fast Fish recommendations using hybrid approach:
    1. Vectorized historical reference
    2. Cached trending analysis by store group
    """
    logger.info(f"Augmenting {len(fast_fish_df)} Fast Fish recommendations using hybrid approach...")
    
    # Create a copy
    augmented_df = fast_fish_df.copy()
    
    # Add historical reference columns (vectorized)
    logger.info("Adding historical reference columns...")
    historical_columns = ['Historical_SPU_Quantity_202408A', 'SPU_Change_vs_Historical', 
                         'SPU_Change_vs_Historical_Pct', 'Historical_Store_Count_202408A', 
                         'Historical_Total_Sales_202408A']
    
    for col in historical_columns:
        augmented_df[col] = None
    
    # Vectorized historical lookup
    matches = 0
    for idx, row in augmented_df.iterrows():
        store_group = row['Store_Group_Name']
        target_style_tags = row['Target_Style_Tags']
        spu_quantity = row['Target_SPU_Quantity']
        
        # Parse target style tags
        sub_category = parse_target_style_tags(target_style_tags)
        
        # Historical lookup
        lookup_key = (store_group, sub_category)
        if lookup_key in historical_lookup:
            hist_data = historical_lookup[lookup_key]
            
            augmented_df.at[idx, 'Historical_SPU_Quantity_202408A'] = hist_data['spu_quantity']
            augmented_df.at[idx, 'SPU_Change_vs_Historical'] = spu_quantity - hist_data['spu_quantity']
            augmented_df.at[idx, 'SPU_Change_vs_Historical_Pct'] = ((spu_quantity - hist_data['spu_quantity']) / hist_data['spu_quantity'] * 100) if hist_data['spu_quantity'] > 0 else 0
            augmented_df.at[idx, 'Historical_Store_Count_202408A'] = hist_data['store_count']
            augmented_df.at[idx, 'Historical_Total_Sales_202408A'] = hist_data['total_sales']
            
            matches += 1
    
    logger.info(f"Historical matches: {matches}/{len(augmented_df)} ({matches/len(augmented_df)*100:.1f}%)")
    
    # Apply fast trending analysis (cached by store group)
    logger.info("Applying fast trending analysis...")
    augmented_df = apply_fast_trending_analysis(augmented_df)
    
    # Create enhanced rationale
    logger.info("Creating enhanced rationale...")
    enhanced_rationale = []
    
    for idx, row in augmented_df.iterrows():
        rationale_parts = []
        
        # Original rationale  
        if 'Data_Based_Rationale' in row and pd.notna(row['Data_Based_Rationale']):
            rationale_parts.append(str(row['Data_Based_Rationale']))
    
        # Historical context
        if pd.notna(row['Historical_SPU_Quantity_202408A']):
            hist_qty = row['Historical_SPU_Quantity_202408A']
            change = row['SPU_Change_vs_Historical']
            change_pct = row['SPU_Change_vs_Historical_Pct']
            
            if change > 0:
                hist_context = f"HISTORICAL: Historical baseline: {hist_qty} SPUs (August 2024). Expanding by {change} SPUs (+{change_pct:.1f}%)"
            else:
                hist_context = f"HISTORICAL: Historical baseline: {hist_qty} SPUs (August 2024). Reducing by {abs(change)} SPUs ({change_pct:.1f}%)"
            rationale_parts.append(hist_context)
        
        # Trending context (from cache)
        if pd.notna(row['cluster_trend_score']):
            cluster_score = row['cluster_trend_score']
            cluster_confidence = row['cluster_trend_confidence']
            
            if cluster_score >= 70:
                trend_level = "STRONG CLUSTER TREND"
                trend_emoji = "üöÄ"
            elif cluster_score >= 50:
                trend_level = "MODERATE CLUSTER TREND"  
                trend_emoji = "üìä"
            else:
                trend_level = "WEAK CLUSTER TREND"
                trend_emoji = "‚ö†Ô∏è"
            
            trend_context = f"TRENDS: {trend_emoji} {trend_level} (Score: {cluster_score}, Confidence: {cluster_confidence}%)"
            rationale_parts.append(trend_context)
        
        # Combine all rationale parts
        enhanced_rationale.append(" | ".join(rationale_parts))
    
    # Add the enhanced rationale column
    augmented_df['Enhanced_Rationale'] = enhanced_rationale
    
    logger.info(f"‚úÖ Hybrid augmentation complete with historical + cached trending analysis")
    
    return augmented_df

def main():
    """Step 17: Fast Trending Hybrid (Optimized Version)."""
    
    start_time = datetime.now()
    logger.info("üöÄ Starting Step 17: Fast Trending Hybrid...")
    logger.info("   Historical Reference + Cached Trending Analysis")
    
    if FAST_MODE:
        logger.info("üöÄ FAST_MODE ENABLED - Optimized for speed!")
        logger.info(f"   ‚Ä¢ Store group caching for trending analysis")
        logger.info(f"   ‚Ä¢ Vectorized historical operations")
        logger.info(f"   ‚Ä¢ Expected runtime: 2-5 minutes")
    
    try:
        # Load Fast Fish recommendations from Step 14 (use the most recent file)
        fast_fish_files = glob.glob("../output/fast_fish_spu_count_recommendations_*.csv")
        if not fast_fish_files:
            # Fallback to older file format
            fast_fish_files = glob.glob("../output/fast_fish_with_sell_through_analysis_*.csv")
            if not fast_fish_files:
                logger.error("No Fast Fish recommendations files found")
                raise FileNotFoundError("No Fast Fish recommendations files found in ../output/")
        
        # Get the most recent file
        fast_fish_file = max(fast_fish_files, key=os.path.getmtime)
        logger.info(f"Loading Fast Fish recommendations from: {fast_fish_file}")
        fast_fish_df = pd.read_csv(fast_fish_file)
        logger.info(f"Loaded {len(fast_fish_df):,} Fast Fish recommendations")
        
        # Verify we have the correct number of store groups
        unique_groups = fast_fish_df['Store_Group_Name'].nunique()
        logger.info(f"Found {unique_groups} unique store groups")
        
        # Load historical data
        logger.info("Loading historical 202408A data...")
        historical_df = load_historical_spu_data()
        
        # Create historical reference lookup
        logger.info("Creating historical reference lookup table...")
        historical_lookup_df = create_historical_reference_lookup(historical_df)
        
        # Convert to dictionary for faster lookups
        historical_lookup = {}
        for _, row in historical_lookup_df.iterrows():
            key = (row['store_group'], row['lookup_key'])
            historical_lookup[key] = {
                'spu_quantity': row['historical_spu_count'],
                'total_sales': row['historical_total_sales'],
                'store_count': row['historical_store_count']
            }
        
        # Augment Fast Fish recommendations using hybrid approach
        logger.info("üéØ Applying hybrid augmentation (Historical + Cached Trending)...")
        augmented_df = augment_fast_fish_hybrid(fast_fish_df, historical_lookup)
        
        # Save enhanced file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"../output/fast_fish_with_historical_and_trending_hybrid_{timestamp}.csv"
        
        logger.info(f"Saving enhanced Fast Fish file to: {output_file}")
        augmented_df.to_csv(output_file, index=False)
        
        # Print summary
        elapsed = (datetime.now() - start_time).total_seconds()
        
        print(f"\n" + "="*80)
        print(f"üéØ FAST TRENDING HYBRID COMPLETE!")
        print(f"="*80)
        print(f"üìä BASIC METRICS:")
        print(f"  ‚Ä¢ Processing time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        print(f"  ‚Ä¢ Original recommendations: {len(fast_fish_df):,}")
        print(f"  ‚Ä¢ Enhanced recommendations: {len(augmented_df):,}")
        print(f"  ‚Ä¢ Output columns: {len(augmented_df.columns)}")
        print(f"  ‚Ä¢ Store groups analyzed: {unique_groups}")
        print(f"  ‚Ä¢ Store group cache size: {len(STORE_GROUP_CACHE)}")
        
        historical_matches = augmented_df['Historical_SPU_Quantity_202408A'].notna().sum()
        print(f"\nüìà HISTORICAL REFERENCE:")
        print(f"  ‚Ä¢ Historical matches: {historical_matches:,}")
        print(f"  ‚Ä¢ Historical match rate: {historical_matches/len(augmented_df)*100:.1f}%")
        
        if TRENDING_AVAILABLE:
            avg_trend_score = augmented_df['cluster_trend_score'].mean()
            print(f"\nüöÄ TRENDING ANALYSIS:")
            print(f"  ‚Ä¢ Store groups cached: {len(STORE_GROUP_CACHE)}")
            print(f"  ‚Ä¢ Average trend score: {avg_trend_score:.1f}")
            print(f"  ‚Ä¢ Trending analysis method: Cached by store group")
        
        print(f"\nüìÅ OUTPUT:")
        print(f"  ‚Ä¢ File: {output_file}")
        print(f"  ‚Ä¢ Contains: Historical reference + cached trending analysis")
        
        logger.info("‚úÖ Step 17: Fast Trending Hybrid completed successfully!")
        logger.info(f"üìÅ Output: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"‚ùå Error in Step 17: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main() 