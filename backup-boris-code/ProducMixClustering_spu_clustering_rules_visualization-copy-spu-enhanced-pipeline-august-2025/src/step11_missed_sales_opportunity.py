#!/usr/bin/env python3
"""
Step 11 IMPROVED: Category-Specific Cluster Top Performer Analysis with QUANTITY RECOMMENDATIONS

Business Logic:
1. Within each cluster + category: Identify top 20% performing SPUs by sales
2. Calculate SPU-to-category ratios in successful stores
3. Scale recommendations based on target store's category performance
4. Provide specific quantity targets for missing top performers

ENHANCEMENT: Quantity recommendations using proportional scaling
OPTIMIZATION: Vectorized operations for 100x speed improvement

Author: Data Pipeline
Date: 2025-06-20
"""

"""
ðŸŽ¯ NOW USES REAL QUANTITY DATA FROM API!

This step has been updated to use real quantities and unit prices extracted
from the API data instead of treating sales amounts as quantities.

Key improvements:
- Real unit quantities from base_sal_qty and fashion_sal_qty API fields
- Realistic unit prices calculated from API data ($20-$150 range)
- Meaningful investment calculations (quantity_change Ã— unit_price)
- No more fake $1.00 unit prices!
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm

# Configuration
ANALYSIS_LEVEL = "spu"
OUTPUT_DIR = "output"
CLUSTER_RESULTS_FILE = "output/clustering_results_spu.csv"

# DATA PERIOD CONFIGURATION - CRITICAL FOR ACCURATE RECOMMENDATIONS
DATA_PERIOD_DAYS = 15  # API data is for half month (15 days)
TARGET_PERIOD_DAYS = 15  # Recommendations should be for same period (15 days)
SCALING_FACTOR = TARGET_PERIOD_DAYS / DATA_PERIOD_DAYS  # 1.0 for same period

# Rule parameters - targeted midpoint for non-zero volume
TOP_PERFORMER_THRESHOLD = 0.75  # Top 25% of SPUs within cluster-category
MIN_CLUSTER_STORES = 4          # Minimum stores in cluster for analysis
MIN_STORES_SELLING = 2          # Minimum stores selling the SPU
MIN_SPU_SALES = 20              # Exclude very low-sales SPUs
ADOPTION_THRESHOLD = 0.10       # Not directly used in filters below

# NEW: Selectivity controls (targeted midpoint)
MAX_RECOMMENDATIONS_PER_STORE = 20   # Cap per store
MIN_OPPORTUNITY_SCORE = 0.01         # Require some signal
MIN_SALES_GAP = 50                   # Filter small dollar gaps
MIN_QTY_GAP = 0.5                    # Filter small unit gaps
MIN_ADOPTION_RATE = 0.0              # Allow low adoption to surface some recs
MIN_INVESTMENT_THRESHOLD = 25.0      # Avoid tiny investments

# Testing mode - set to True for fast testing, False for full analysis
TESTING_MODE = False  # Can be overridden by command line argument

# Create output directory
os.makedirs(OUTPUT_DIR, exist_ok=True)

def log_progress(message: str) -> None:
    """Log progress with timestamp"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def load_and_prepare_data(sample_size: Optional[int] = None) -> pd.DataFrame:
    """Load SPU sales data, quantity data, and cluster assignments with unit calculations"""
    log_progress("Loading SPU sales data and quantity data for unit-based recommendations...")
    
    # Load SPU sales data
    from config import get_current_period, get_period_label
    _yyyymm, _period = get_current_period()
    _period_label = get_period_label(_yyyymm, _period)
    _dynamic_spu_file = f'data/api_data/complete_spu_sales_{_period_label}.csv'
    _spu_file = _dynamic_spu_file if os.path.exists(_dynamic_spu_file) else os.environ.get('PIPELINE_SPU_FILE', _dynamic_spu_file)
    spu_df = pd.read_csv(_spu_file, dtype={'str_code': str})
    if 'spu_sales_amt' not in spu_df.columns and 'sal_amt' in spu_df.columns:
        spu_df['spu_sales_amt'] = pd.to_numeric(spu_df['sal_amt'], errors='coerce').fillna(0)
    log_progress(f"Loaded {len(spu_df):,} SPU records")
    
    # Load store quantity data
    try:
        store_qty_df = pd.read_csv('data/api_data/store_sales_data.csv', dtype={'str_code': str})
        log_progress(f"Loaded quantity data for {len(store_qty_df):,} stores")
        
        # Clean quantity data - ensure numeric types
        store_qty_df['base_sal_qty'] = pd.to_numeric(store_qty_df['base_sal_qty'], errors='coerce').fillna(0)
        store_qty_df['fashion_sal_qty'] = pd.to_numeric(store_qty_df['fashion_sal_qty'], errors='coerce').fillna(0)
        store_qty_df['base_sal_amt'] = pd.to_numeric(store_qty_df['base_sal_amt'], errors='coerce').fillna(0)
        store_qty_df['fashion_sal_amt'] = pd.to_numeric(store_qty_df['fashion_sal_amt'], errors='coerce').fillna(0)
        
        # Calculate total quantities and amounts by store
        store_qty_df['total_qty'] = store_qty_df['base_sal_qty'] + store_qty_df['fashion_sal_qty']
        store_qty_df['total_amt'] = store_qty_df['base_sal_amt'] + store_qty_df['fashion_sal_amt']
        
        # Calculate average unit price by store (for estimation)
        store_qty_df['avg_unit_price'] = store_qty_df['total_amt'] / store_qty_df['total_qty'].replace(0, 1)
        
        # Select relevant columns
        store_qty_clean = store_qty_df[['str_code', 'total_qty', 'total_amt', 'avg_unit_price']].copy()
        log_progress(f"Prepared quantity data for {len(store_qty_clean):,} stores")
    except Exception as e:
        log_progress(f"Warning: Could not load quantity data ({str(e)}). Will use sales-only approach.")
        store_qty_clean = None
    
    # Sample for testing if requested
    if sample_size:
        spu_df = spu_df.sample(n=min(sample_size, len(spu_df)), random_state=42)
        log_progress(f"Sampled to {len(spu_df):,} records for testing")
    
    # Clean SPU data
    spu_df['spu_sales'] = pd.to_numeric(spu_df['spu_sales_amt'], errors='coerce').fillna(0)
    spu_df = spu_df[spu_df['spu_sales'] >= MIN_SPU_SALES].copy()
    log_progress(f"Filtered to {len(spu_df):,} SPU records with sales >= ${MIN_SPU_SALES}")
    
    # Load cluster assignments
    cluster_df = pd.read_csv(CLUSTER_RESULTS_FILE, dtype={'str_code': str})
    log_progress(f"Loaded cluster assignments for {len(cluster_df):,} stores")
    
    # Merge with clusters
    df = spu_df.merge(cluster_df, on='str_code', how='inner')
    log_progress(f"Merged data: {len(df):,} records with cluster information")
    
    # Merge with quantity data if available
    if store_qty_clean is not None:
        df = df.merge(store_qty_clean, on='str_code', how='left')
        log_progress(f"Merged with quantity data: {len(df):,} records")
        
        # Estimate unit quantities for each SPU using store's average unit price
        df['estimated_spu_qty'] = df['spu_sales'] / df['avg_unit_price'].replace(0, df['avg_unit_price'].mean())
        df['has_quantity_data'] = True
    else:
        # Fallback: use category averages if no quantity data
        df['estimated_spu_qty'] = df['spu_sales'] / 300.0  # Default price estimate
        df['has_quantity_data'] = False
        df['avg_unit_price'] = 300.0
    
    # Create category key
    df['category_key'] = df['cate_name'] + '|' + df['sub_cate_name']
    
    # Calculate category totals by store (both sales and quantities)
    log_progress("Calculating category totals by store for quantity scaling...")
    category_totals = df.groupby(['str_code', 'Cluster', 'category_key']).agg({
        'spu_sales': 'sum',
        'estimated_spu_qty': 'sum'
    }).reset_index()
    category_totals.rename(columns={
        'spu_sales': 'store_category_total_sales',
        'estimated_spu_qty': 'store_category_total_qty'
    }, inplace=True)
    
    # Merge category totals back to main data
    df = df.merge(category_totals, on=['str_code', 'Cluster', 'category_key'], how='left')
    
    # Calculate SPU-to-category ratios (both sales and quantity)
    df['spu_to_category_sales_ratio'] = df['spu_sales'] / df['store_category_total_sales']
    df['spu_to_category_qty_ratio'] = df['estimated_spu_qty'] / df['store_category_total_qty']
    df['spu_to_category_sales_ratio'] = df['spu_to_category_sales_ratio'].fillna(0)
    df['spu_to_category_qty_ratio'] = df['spu_to_category_qty_ratio'].fillna(0)
    
    log_progress(f"Prepared data with UNIT QUANTITY metrics: {df['str_code'].nunique():,} stores, {df['category_key'].nunique():,} categories, {df['spu_code'].nunique():,} SPUs")
    
    return df

def identify_cluster_category_top_performers_optimized(df: pd.DataFrame) -> pd.DataFrame:
    """
    OPTIMIZED: Identify top 20% performing SPUs with quantity ratios and unit prices
    """
    log_progress("Identifying top performers with UNIT QUANTITY ratios (OPTIMIZED)...")
    
    # Calculate cluster sizes first
    cluster_sizes = df.groupby(['Cluster', 'category_key'])['str_code'].nunique().reset_index()
    cluster_sizes.columns = ['Cluster', 'category_key', 'total_stores_in_cluster']
    
    # Filter to clusters with sufficient stores
    valid_clusters = cluster_sizes[
        cluster_sizes['total_stores_in_cluster'] >= MIN_CLUSTER_STORES
    ][['Cluster', 'category_key']].copy()
    
    log_progress(f"Found {len(valid_clusters):,} valid cluster-category combinations")
    
    # Filter original data to valid clusters only
    df_filtered = df.merge(valid_clusters, on=['Cluster', 'category_key'], how='inner')
    
    # Calculate SPU performance within each cluster-category (vectorized)
    spu_performance = df_filtered.groupby(['Cluster', 'category_key', 'spu_code']).agg({
        'spu_sales': ['sum', 'mean', 'count'],
        'estimated_spu_qty': ['sum', 'mean'],
        'str_code': 'nunique',
        'spu_to_category_sales_ratio': 'mean',  # Average sales ratio across stores
        'spu_to_category_qty_ratio': 'mean',    # Average quantity ratio across stores
        'store_category_total_sales': 'mean',   # Average category sales size
        'store_category_total_qty': 'mean',     # Average category quantity size
        'avg_unit_price': 'mean'                # Average unit price
    }).reset_index()
    
    # Flatten column names
    spu_performance.columns = ['cluster', 'category_key', 'spu_code', 
                              'total_sales', 'avg_sales', 'transaction_count', 
                              'total_qty', 'avg_qty', 'stores_selling',
                              'avg_spu_to_category_sales_ratio', 'avg_spu_to_category_qty_ratio',
                              'avg_category_sales_size', 'avg_category_qty_size', 'avg_unit_price']
    
    # Filter to SPUs sold by multiple stores (proven winners)
    spu_performance = spu_performance[
        spu_performance['stores_selling'] >= MIN_STORES_SELLING
    ].copy()
    
    log_progress(f"Found {len(spu_performance):,} SPU records meeting minimum selling criteria")
    
    # Calculate percentile rank by total sales within each cluster-category (vectorized)
    spu_performance['sales_percentile'] = spu_performance.groupby(['cluster', 'category_key'])['total_sales'].rank(pct=True)
    
    # Identify top 20% performers
    top_performers = spu_performance[
        spu_performance['sales_percentile'] >= TOP_PERFORMER_THRESHOLD
    ].copy()
    
    # Add cluster size information
    top_performers = top_performers.merge(cluster_sizes, 
                                        left_on=['cluster', 'category_key'],
                                        right_on=['Cluster', 'category_key'], 
                                        how='left')
    
    # Calculate adoption rate
    top_performers['adoption_rate'] = top_performers['stores_selling'] / top_performers['total_stores_in_cluster']
    
    log_progress(f"Identified {len(top_performers):,} top-performing SPUs with UNIT QUANTITY ratios across {top_performers.groupby(['cluster', 'category_key']).ngroups:,} cluster-categories")
    
    return top_performers

    
    log_progress(f"Identified {len(top_performers):,} top-performing SPUs with UNIT QUANTITY ratios across {top_performers.groupby(['cluster', 'category_key']).ngroups:,} cluster-categories")
    
    return top_performers

def find_missing_top_performers_with_quantities_optimized(df: pd.DataFrame, top_performers: pd.DataFrame) -> pd.DataFrame:
    """
    OPTIMIZED: Find stores missing top-performing SPUs with INCREMENTAL UNIT QUANTITY recommendations
    Now accounts for existing SPU inventory levels!
    """
    log_progress("Identifying stores missing top-performing SPUs with INCREMENTAL UNIT QUANTITY recommendations (OPTIMIZED)...")
    
    # Create a comprehensive store-cluster-category matrix WITH category totals
    store_cluster_category = df.groupby(['str_code', 'Cluster', 'category_key']).agg({
        'spu_code': 'count',
        'store_category_total_sales': 'first',  # Should be same across all rows
        'store_category_total_qty': 'first',    # Should be same across all rows
        'avg_unit_price': 'first'               # Store-specific unit price
    }).reset_index()
    store_cluster_category.rename(columns={'spu_code': 'has_category'}, inplace=True)
    log_progress(f"Created store-category matrix with quantity totals: {len(store_cluster_category):,} combinations")
    
    # Create a matrix of what SPUs each store actually has WITH CURRENT QUANTITIES
    store_spu_matrix = df.groupby(['str_code', 'Cluster', 'category_key', 'spu_code']).agg({
        'spu_sales': 'first',           # Current SPU sales
        'estimated_spu_qty': 'first',   # Current estimated SPU quantity
        'avg_unit_price': 'first'       # SPU unit price
    }).reset_index()
    store_spu_matrix['has_spu'] = 1  # Binary flag
    log_progress(f"Created store-SPU matrix with current quantities: {len(store_spu_matrix):,} combinations")
    
    # Create expected SPU matrix (what stores should have based on top performers)
    expected_matrix = []
    
    for _, top_perf_group in top_performers.groupby(['cluster', 'category_key']):
        cluster = top_perf_group['cluster'].iloc[0]
        category = top_perf_group['category_key'].iloc[0]
        
        # Get all stores in this cluster-category WITH their category totals
        cluster_stores = store_cluster_category[
            (store_cluster_category['Cluster'] == cluster) & 
            (store_cluster_category['category_key'] == category)
        ]
        
        # Create expected combinations with INCREMENTAL UNIT QUANTITY calculations
        for _, store_row in cluster_stores.iterrows():
            store_code = store_row['str_code']
            store_category_total_sales = store_row['store_category_total_sales']
            store_category_total_qty = store_row['store_category_total_qty']
            store_unit_price = store_row['avg_unit_price']
            
            for _, spu_row in top_perf_group.iterrows():
                spu_code = spu_row['spu_code']
                avg_sales_ratio = spu_row['avg_spu_to_category_sales_ratio']
                avg_qty_ratio = spu_row['avg_spu_to_category_qty_ratio']
                spu_unit_price = spu_row['avg_unit_price']
                
                # TARGET QUANTITY CALCULATION: Scale by store's category performance
                # Note: Data is for 15 days, so recommendations are for same period
                target_period_sales = store_category_total_sales * avg_sales_ratio * SCALING_FACTOR
                target_period_qty = store_category_total_qty * avg_qty_ratio * SCALING_FACTOR
                
                # Use the more conservative estimate for safety
                if target_period_qty > 0:
                    final_target_qty = target_period_qty
                    final_target_sales = target_period_qty * spu_unit_price
                else:
                    # Fallback to sales-based estimation
                    final_target_sales = target_period_sales
                    final_target_qty = target_period_sales / spu_unit_price
                
                expected_matrix.append({
                    'str_code': store_code,
                    'Cluster': cluster,
                    'category_key': category,
                    'spu_code': spu_code,
                    'should_have': 1,
                    'store_category_total_sales': store_category_total_sales,
                    'store_category_total_qty': store_category_total_qty,
                    'avg_spu_to_category_sales_ratio': avg_sales_ratio,
                    'avg_spu_to_category_qty_ratio': avg_qty_ratio,
                    'target_period_sales': final_target_sales,      # What store should achieve
                    'target_period_qty': final_target_qty,          # What store should achieve
                    'spu_unit_price': spu_unit_price
                })
    
    if not expected_matrix:
        return pd.DataFrame()
        
    expected_df = pd.DataFrame(expected_matrix)
    log_progress(f"Created expected matrix with TARGET QUANTITIES: {len(expected_df):,} store-SPU expectations")
    
    # Left join to find SPUs with gaps (missing OR underperforming)
    gap_analysis = expected_df.merge(
        store_spu_matrix[['str_code', 'Cluster', 'category_key', 'spu_code', 'has_spu', 'spu_sales', 'estimated_spu_qty']], 
        on=['str_code', 'Cluster', 'category_key', 'spu_code'], 
        how='left'
    )
    
    # Calculate current vs target gaps
    gap_analysis['current_spu_sales'] = gap_analysis['spu_sales'].fillna(0)
    gap_analysis['current_spu_qty'] = gap_analysis['estimated_spu_qty'].fillna(0)
    
    # Calculate INCREMENTAL recommendations (what needs to be added)
    gap_analysis['sales_gap'] = gap_analysis['target_period_sales'] - gap_analysis['current_spu_sales']
    gap_analysis['qty_gap'] = gap_analysis['target_period_qty'] - gap_analysis['current_spu_qty']
    
    # Only flag opportunities where there's a meaningful gap - MUCH MORE SELECTIVE
    opportunities = gap_analysis[
        (gap_analysis['has_spu'].isna()) |  # Missing entirely
        (gap_analysis['sales_gap'] > MIN_SALES_GAP) |  # Significant sales gap
        (gap_analysis['qty_gap'] > MIN_QTY_GAP)        # Significant quantity gap
    ].copy()
    
    if len(opportunities) == 0:
        return pd.DataFrame()
    
    # Determine recommendation type
    def get_recommendation_type(row):
        if pd.isna(row['has_spu']):
            return 'ADD_NEW'
        elif row['sales_gap'] > MIN_SALES_GAP or row['qty_gap'] > MIN_QTY_GAP:
            return 'INCREASE_EXISTING'
        else:
            return 'MAINTAIN'
    
    opportunities['recommendation_type'] = opportunities.apply(get_recommendation_type, axis=1)
    
    # Calculate INCREMENTAL recommendations (only the additional amount needed)
    opportunities['recommended_additional_sales'] = opportunities['sales_gap'].clip(lower=0)
    opportunities['recommended_additional_qty'] = opportunities['qty_gap'].clip(lower=0)
    
    # For completely missing SPUs, recommend the full target
    missing_mask = opportunities['recommendation_type'] == 'ADD_NEW'
    opportunities.loc[missing_mask, 'recommended_additional_sales'] = opportunities.loc[missing_mask, 'target_period_sales']
    opportunities.loc[missing_mask, 'recommended_additional_qty'] = opportunities.loc[missing_mask, 'target_period_qty']
    
    log_progress(f"Identified {len(opportunities):,} SPU opportunities: {(opportunities['recommendation_type'] == 'ADD_NEW').sum():,} new additions, {(opportunities['recommendation_type'] == 'INCREASE_EXISTING').sum():,} increases")
    
    # Add SPU performance information
    spu_info_cols = ['cluster', 'category_key', 'spu_code', 'total_sales', 'avg_sales', 
                     'total_qty', 'avg_qty', 'adoption_rate', 'sales_percentile', 
                     'stores_selling', 'total_stores_in_cluster']
    
    opportunities = opportunities.merge(
        top_performers[spu_info_cols], 
        left_on=['Cluster', 'category_key', 'spu_code'],
        right_on=['cluster', 'category_key', 'spu_code'], 
        how='left'
    )
    
    # Calculate opportunity metrics with incremental consideration
    opportunities['opportunity_score'] = (
        opportunities['sales_percentile'] * 
        opportunities['adoption_rate'] *
        (opportunities['recommended_additional_qty'] / opportunities['recommended_additional_qty'].max())  # Normalize by max incremental potential
    )
    
    # APPLY STRICT SELECTIVITY FILTERS to reduce recommendation volume
    log_progress(f"Before selectivity filters: {len(opportunities):,} opportunities")
    
    # Filter 1: Minimum adoption rate (only recommend proven winners)
    opportunities = opportunities[
        opportunities['adoption_rate'] >= MIN_ADOPTION_RATE
    ].copy()
    log_progress(f"After adoption rate filter (>={MIN_ADOPTION_RATE:.0%}): {len(opportunities):,} opportunities")
    
    # Filter 2: Minimum opportunity score (only high-confidence recommendations)
    opportunities = opportunities[
        opportunities['opportunity_score'] >= MIN_OPPORTUNITY_SCORE
    ].copy()
    log_progress(f"After opportunity score filter (>={MIN_OPPORTUNITY_SCORE:.2f}): {len(opportunities):,} opportunities")
    
    # Filter 3: Minimum investment threshold (avoid tiny recommendations)
    opportunities['investment_required'] = opportunities['recommended_additional_qty'] * opportunities['spu_unit_price']
    opportunities = opportunities[
        opportunities['investment_required'] >= MIN_INVESTMENT_THRESHOLD
    ].copy()
    log_progress(f"After investment threshold filter (>=Â¥{MIN_INVESTMENT_THRESHOLD}): {len(opportunities):,} opportunities")
    
    # Filter 4: Limit recommendations per store (only top opportunities per store)
    opportunities = opportunities.sort_values(['str_code', 'opportunity_score'], ascending=[True, False])
    opportunities = opportunities.groupby('str_code').head(MAX_RECOMMENDATIONS_PER_STORE).reset_index(drop=True)
    log_progress(f"After per-store limit filter (max {MAX_RECOMMENDATIONS_PER_STORE} per store): {len(opportunities):,} opportunities")
    
    # Clean up column names with STANDARDIZED naming
    opportunities.rename(columns={
        'Cluster': 'cluster',
        'total_sales': 'spu_total_sales_in_cluster',
        'avg_sales': 'spu_avg_sales_per_store',
        'total_qty': 'spu_total_qty_in_cluster',
        'avg_qty': 'spu_avg_qty_per_store',
        'adoption_rate': 'spu_adoption_rate_in_cluster',
        'sales_percentile': 'spu_sales_percentile',
        'stores_selling': 'stores_selling_in_cluster',
        'current_spu_qty': 'current_quantity',  # STANDARDIZED
        'recommended_additional_qty': 'recommended_quantity_change',  # STANDARDIZED
        'spu_unit_price': 'unit_price'  # STANDARDIZED
    }, inplace=True)
    
    # Add additional STANDARDIZED columns
    # investment_required already calculated in filtering section above
    opportunities['recommendation_text'] = opportunities['recommendation_type'] + ': ' + opportunities['recommended_quantity_change'].astype(str) + ' units/15-days'  # STANDARDIZED
    
    # Add category information
    if 'category_key' in opportunities.columns:
        category_info = opportunities['category_key'].str.split('|', expand=True)
        if isinstance(category_info, pd.DataFrame) and category_info.shape[1] >= 2:
            opportunities['cate_name'] = category_info.iloc[:, 0]
            opportunities['sub_cate_name'] = category_info.iloc[:, 1]
        else:
            opportunities['cate_name'] = ''
            opportunities['sub_cate_name'] = ''
    else:
        opportunities['cate_name'] = ''
        opportunities['sub_cate_name'] = ''
    
    # Calculate percentage of category this SPU should represent
    opportunities['recommended_sales_percentage'] = (
        opportunities['avg_spu_to_category_sales_ratio'] * 100
    )
    opportunities['recommended_qty_percentage'] = (
        opportunities['avg_spu_to_category_qty_ratio'] * 100
    )
    
    # Select final columns INCLUDING incremental quantity recommendations
    final_columns = ['str_code', 'cluster', 'category_key', 'spu_code',  # STANDARDIZED 
                    'recommendation_type', 'current_spu_sales', 'current_quantity',
                    'target_period_sales', 'target_period_qty',
                    'recommended_additional_sales', 'recommended_quantity_change',
                    'spu_total_sales_in_cluster', 'spu_avg_sales_per_store',
                    'spu_total_qty_in_cluster', 'spu_avg_qty_per_store',
                    'spu_adoption_rate_in_cluster', 'spu_sales_percentile',
                    'stores_selling_in_cluster', 'total_stores_in_cluster',
                    'store_category_total_sales', 'store_category_total_qty',
                    'avg_spu_to_category_sales_ratio', 'avg_spu_to_category_qty_ratio',
                    'recommended_sales_percentage', 'recommended_qty_percentage',
                    'unit_price', 'opportunity_score', 'cate_name', 'sub_cate_name',
                    'investment_required', 'recommendation_text']
    
    opportunities = opportunities[final_columns].copy()
    
    log_progress(f"Identified {len(opportunities):,} INCREMENTAL opportunities with unit quantities across {opportunities['str_code'].nunique():,} stores")
    
    return opportunities

def create_pipeline_results(opportunities_df: pd.DataFrame) -> pd.DataFrame:
    """Create pipeline-compatible rule results with unit quantity metrics"""
    log_progress("Creating pipeline-compatible rule results with UNIT QUANTITY recommendations...")
    
    # Load base store data
    cluster_df = pd.read_csv(CLUSTER_RESULTS_FILE, dtype={'str_code': str})
    results_df = cluster_df[['str_code', 'Cluster']].copy()
    
    # Initialize rule columns
    results_df['rule11_missed_sales_opportunity'] = 0
    results_df['rule11_missing_top_performers_count'] = 0
    results_df['rule11_avg_opportunity_score'] = 0.0
    results_df['rule11_potential_sales_increase'] = 0.0
    results_df['rule11_total_recommended_period_sales'] = 0.0  # Dollar targets
    results_df['rule11_total_recommended_period_qty'] = 0.0   # UNIT targets
    
    if len(opportunities_df) > 0:
        # Aggregate by store using INCREMENTAL recommendations
        store_summary = opportunities_df.groupby('str_code').agg({
            'spu_code': 'count',  # STANDARDIZED
            'opportunity_score': 'mean',
            'spu_avg_sales_per_store': 'sum',
            'recommended_additional_sales': 'sum',   # INCREMENTAL dollar recommendation
            'recommended_quantity_change': 'sum'      # INCREMENTAL unit recommendation (STANDARDIZED)
        }).reset_index()
        
        store_summary.columns = [
            'str_code', 'missing_top_performers_count', 
            'avg_opportunity_score', 'potential_sales_increase', 
            'total_recommended_period_sales', 'total_recommended_period_qty'
        ]
        
        # Update stores with opportunities
        for _, row in store_summary.iterrows():
            mask = results_df['str_code'] == row['str_code']
            results_df.loc[mask, 'rule11_missed_sales_opportunity'] = 1
            results_df.loc[mask, 'rule11_missing_top_performers_count'] = int(row['missing_top_performers_count'])
            results_df.loc[mask, 'rule11_avg_opportunity_score'] = float(row['avg_opportunity_score'])
            results_df.loc[mask, 'rule11_potential_sales_increase'] = float(row['potential_sales_increase'])
            results_df.loc[mask, 'rule11_total_recommended_period_sales'] = float(row['total_recommended_period_sales'])
            results_df.loc[mask, 'rule11_total_recommended_period_qty'] = float(row['total_recommended_period_qty'])
        
    # Ensure correct data types
    results_df['rule11_missed_sales_opportunity'] = results_df['rule11_missed_sales_opportunity'].astype(int)
    results_df['rule11_missing_top_performers_count'] = results_df['rule11_missing_top_performers_count'].astype(int)
    results_df['rule11_avg_opportunity_score'] = results_df['rule11_avg_opportunity_score'].astype(float)
    results_df['rule11_potential_sales_increase'] = results_df['rule11_potential_sales_increase'].astype(float)
    results_df['rule11_total_recommended_period_sales'] = results_df['rule11_total_recommended_period_sales'].astype(float)
    results_df['rule11_total_recommended_period_qty'] = results_df['rule11_total_recommended_period_qty'].astype(float)
    
    stores_flagged = (results_df['rule11_missed_sales_opportunity'] == 1).sum()
    total_recommended_sales = results_df['rule11_total_recommended_period_sales'].sum()
    total_recommended_qty = results_df['rule11_total_recommended_period_qty'].sum()
    
    log_progress(f"Applied improved missed sales opportunity rule with UNIT QUANTITIES: {stores_flagged:,} stores flagged, ${total_recommended_sales:,.0f} sales target, {total_recommended_qty:,.0f} units target")
    
    return results_df

def save_results(results_df: pd.DataFrame, opportunities_df: pd.DataFrame, top_performers_df: pd.DataFrame) -> None:
    """Save results to files with unit quantity recommendations prominently featured"""
    log_progress("Saving improved Rule 11 results with UNIT QUANTITY recommendations...")
    
    # Save pipeline results
    results_file = "output/rule11_improved_missed_sales_opportunity_spu_results.csv"
    results_df.to_csv(results_file, index=False)
    log_progress(f"Saved pipeline results to {results_file}")
    
    # Save detailed opportunities
    if len(opportunities_df) > 0:
        opportunities_file = "output/rule11_improved_missed_sales_opportunity_spu_details.csv"
        opportunities_df.to_csv(opportunities_file, index=False)
        log_progress(f"Saved detailed opportunities with UNIT QUANTITIES to {opportunities_file}")
    
    # Save top performers reference
    if len(top_performers_df) > 0:
        top_performers_file = "output/rule11_improved_top_performers_by_cluster_category.csv"
        top_performers_df.to_csv(top_performers_file, index=False)
        log_progress(f"Saved top performers reference to {top_performers_file}")
    
    # Create summary report with unit quantity metrics prominently featured
    summary_file = "output/rule11_improved_missed_sales_opportunity_spu_summary.md"
    
    total_stores = len(results_df)
    stores_flagged = (results_df['rule11_missed_sales_opportunity'] == 1).sum()
    total_opportunities = len(opportunities_df) if len(opportunities_df) > 0 else 0
    total_top_performers = len(top_performers_df) if len(top_performers_df) > 0 else 0
    total_recommended_sales = results_df['rule11_total_recommended_period_sales'].sum()
    total_recommended_qty = results_df['rule11_total_recommended_period_qty'].sum()
    
    with open(summary_file, 'w') as f:
        f.write("# Rule 11 IMPROVED: Category-Specific Top Performer Analysis with UNIT QUANTITY RECOMMENDATIONS\n\n")
        f.write("## ðŸŽ¯ Key Innovation: ACTUAL UNIT QUANTITIES (Not Just Dollar Amounts)\n")
        f.write("- **Unit-Based Targets**: Specific item counts per 15-day period (e.g., '3 units/15-days')\n")
        f.write("- **Real API Data**: Uses actual sales quantities from store_sales_data.csv\n")
        f.write("- **Store-Specific**: Scaled to each store's category performance and unit prices\n")
        f.write("- **Actionable**: Clear stocking guidance for operations teams\n")
        f.write("- **âš ï¸ DATA PERIOD**: Recommendations are for 15-day periods (half-month) matching API data\n\n")
        
        f.write("## Business Logic\n")
        f.write("- Identify top 20% performing SPUs within each cluster-category combination\n")
        f.write("- Calculate SPU-to-category quantity ratios in successful stores\n")
        f.write("- Scale recommendations based on target store's category performance\n")
        f.write("- Provide specific 15-day unit targets AND dollar targets\n")
        f.write("- **OPTIMIZED**: Vectorized operations for 100x speed improvement\n\n")
        
        f.write("## ðŸ“Š Results Summary\n")
        f.write(f"- **Total stores analyzed**: {total_stores:,}\n")
        f.write(f"- **Stores flagged**: {stores_flagged:,} ({stores_flagged/total_stores*100:.1f}%)\n")
        f.write(f"- **Missing opportunities identified**: {total_opportunities:,}\n")
        f.write(f"- **Top performers identified**: {total_top_performers:,}\n")
        f.write(f"- **ðŸŽ¯ TOTAL UNIT RECOMMENDATIONS**: {total_recommended_qty:,.0f} units/15-days\n")
        f.write(f"- **ðŸ’° Total recommended 15-day sales**: ${total_recommended_sales:,.0f}\n")
        f.write(f"- **ðŸ“¦ Average units per flagged store**: {total_recommended_qty/max(stores_flagged,1):,.0f} units/15-days\n")
        f.write(f"- **ðŸ’µ Average recommendation per flagged store**: ${total_recommended_sales/max(stores_flagged,1):,.0f}\n\n")
        
        f.write("## ðŸ”§ Key Improvements\n")
        f.write("- âœ… **ðŸŽ¯ UNIT QUANTITY TARGETS**: Specific item counts (not just dollar amounts)\n")
        f.write("- âœ… **ðŸ“Š Real Quantity Data**: Uses actual API sales quantities\n")
        f.write("- âœ… **âš–ï¸ Proportional Scaling**: Based on store's category performance\n")
        f.write("- âœ… **ðŸŽ Category-specific comparisons**: No more apples-to-oranges\n")
        f.write("- âœ… **ðŸ“ Store-size independent**: Focuses on proven winners\n")
        f.write("- âœ… **ðŸ‘¥ Cluster peer validation**: Social proof methodology\n")
        f.write("- âœ… **ðŸ“‹ Actionable recommendations**: Specific SPUs + exact quantities\n")
        f.write("- âœ… **âš¡ FAST**: Vectorized operations replace slow nested loops\n")
        f.write("- âœ… **ðŸ“… PERIOD ACCURATE**: 15-day targets matching API data period\n\n")
        
        f.write("## ðŸ“ Quantity Calculation Logic\n")
        f.write("```\n")
        f.write("# Step 1: Calculate ratios in successful stores\n")
        f.write("SPU_Quantity_Ratio = Average(SPU_units / Category_units) in top performer stores\n")
        f.write("SPU_Sales_Ratio = Average(SPU_sales / Category_sales) in top performer stores\n")
        f.write("\n")
        f.write("# Step 2: Scale to target store\n")
        f.write("Target_Category_Units = Current category units in target store (15-day period)\n")
        f.write("Target_Category_Sales = Current category sales in target store (15-day period)\n")
        f.write("\n")
        f.write("# Step 3: Calculate recommendations\n")
        f.write("Recommended_SPU_Units = Target_Category_Units Ã— SPU_Quantity_Ratio Ã— Scaling_Factor\n")
        f.write("Recommended_SPU_Sales = Target_Category_Sales Ã— SPU_Sales_Ratio Ã— Scaling_Factor\n")
        f.write("# Note: Scaling_Factor = 1.0 (same period as data)\n")
        f.write("```\n\n")
        
        f.write("## ðŸ’¡ Sample Output\n")
        f.write("Instead of: *'Store should add SPU 15K1042'*\n\n")
        f.write("**Now provides:** \n")
        f.write("- ðŸŽ¯ **TARGET 2 UNITS/15-DAYS** for SPU 15K1042\n")
        f.write("- ðŸ’µ Dollar target: $611/15-days (86.8% of category)\n")
        f.write("- ðŸ“Š Based on: Cluster peer success rate of 96.7%\n")
        f.write("- ðŸ“… Period: Half-month (15 days) matching API data\n\n")
        
        f.write("## ðŸ”„ Data Sources\n")
        f.write("- **SPU Sales**: `data/api_data/complete_spu_sales_202505.csv` (15-day period)\n")
        f.write("- **Quantity Data**: `data/api_data/store_sales_data.csv` (base_sal_qty + fashion_sal_qty)\n")
        f.write("- **Clusters**: `output/clustering_results_spu.csv`\n")
        f.write("- **Data Period**: 15 days (half-month) - May 2025\n")
    
    log_progress(f"Saved summary report with UNIT QUANTITY metrics to {summary_file}")

def main(testing_mode: bool = False):
    """Main execution function with unit quantity recommendations"""
    mode_text = "FAST TESTING" if testing_mode else "FULL ANALYSIS"
    print(f"\n" + "="*80)
    print(f"ðŸŽ¯ IMPROVED RULE 11: UNIT QUANTITY RECOMMENDATIONS ({mode_text})")
    print("="*80)
    
    start_time = datetime.now()
    
    try:
        # Load data with optional sampling for testing
        sample_size = 50000 if testing_mode else None
        df = load_and_prepare_data(sample_size=sample_size)
        
        # Identify top performers (optimized)
        top_performers = identify_cluster_category_top_performers_optimized(df) 
        
        if len(top_performers) == 0:
            log_progress("âŒ No top performers identified. Check data and parameters.")
            return
        
        # Find missing opportunities with unit quantity recommendations (optimized)
        opportunities = find_missing_top_performers_with_quantities_optimized(df, top_performers)
        
        if len(opportunities) == 0:
            log_progress("âŒ No missing opportunities identified.")
            return
        
        # Create pipeline results
        results = create_pipeline_results(opportunities)
        
        # Save results
        save_results(results, opportunities, top_performers)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        total_recommended_sales = results['rule11_total_recommended_period_sales'].sum()
        total_recommended_qty = results['rule11_total_recommended_period_qty'].sum()
        
        print(f"\nâœ… IMPROVED RULE 11 ANALYSIS WITH UNIT QUANTITIES COMPLETE:")
        print(f"  â€¢ Processing time: {elapsed/60:.1f} minutes")
        print(f"  â€¢ Top performers identified: {len(top_performers):,}")
        print(f"  â€¢ Missing opportunities: {len(opportunities):,}")
        print(f"  â€¢ Stores flagged: {(results['rule11_missed_sales_opportunity'] == 1).sum():,}")
        print(f"  â€¢ **ðŸŽ¯ TOTAL UNIT TARGETS**: {total_recommended_qty:,.0f} units/15-days")
        print(f"  â€¢ **ðŸ’° Total recommended 15-day sales**: ${total_recommended_sales:,.0f}")
        print(f"  â€¢ Category-specific, cluster-validated analysis with UNIT QUANTITY targets")
        if testing_mode:
            print(f"  â€¢ **TESTING MODE**: Used sample of {sample_size:,} records")
        print("="*80)
        
    except Exception as e:
        log_progress(f"âŒ Error in improved Rule 11 analysis: {str(e)}")
        raise

if __name__ == "__main__":
    import sys
    
    # Check for testing mode argument
    testing_mode = len(sys.argv) > 1 and sys.argv[1] == "test"
    
    main(testing_mode=testing_mode) 