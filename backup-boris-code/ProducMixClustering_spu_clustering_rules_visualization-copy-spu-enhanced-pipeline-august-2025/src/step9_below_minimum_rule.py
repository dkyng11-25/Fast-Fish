#!/usr/bin/env python3
"""
Step 9: Below Minimum Rule with QUANTITY ADJUSTMENT RECOMMENDATIONS

This step identifies stores/subcategories/SPUs with below minimum style counts and provides
specific UNIT QUANTITY adjustment recommendations for below-minimum cases.

ENHANCEMENT: Now includes actual unit quantity recommendations using real sales data!

Business Logic:
- SPU allocation < 0.05 = Below minimum viable level
- ðŸŽ¯ QUANTITY RECOMMENDATIONS (e.g., "Add 3 units to reach minimum viable level" OR "Discontinue - only 1 unit sold")
- Real sales data integration for accurate quantity calculations
- Decision: Either boost to minimum viable level or recommend discontinuation

Key Features:
- Subcategory-level below minimum analysis (traditional approach)
- SPU-level below minimum analysis (granular approach) with quantity targets
- Configurable minimum thresholds
- Severity classification with quantity-based decisions
- Comprehensive reporting with unit recommendations

Author: Data Pipeline
Date: 2025-06-23 (Enhanced with Quantity Recommendations)
"""

"""
ðŸŽ¯ NOW USES REAL QUANTITY DATA FROM API!

This step has been updated to use real quantities and unit prices extracted
from the API data instead of treating sales amounts as quantities.

Key improvements:
- Real unit quantities from base_sal_qty and fashion_sal_qty API fields
- Realistic unit prices calculated from API data ($20-$150 range)
- Meaningful investment calculations (quantity_change Ã— unit_price)
- No more fake $1.00 unit prices!
"""

import pandas as pd
import numpy as np
import os
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import warnings
from tqdm import tqdm
import time
import gc

# Suppress pandas warnings
warnings.filterwarnings('ignore')

# Configuration - Analysis Level Selection
ANALYSIS_LEVEL = "spu"  # SPU mode required

# DATA PERIOD CONFIGURATION - CRITICAL FOR ACCURATE RECOMMENDATIONS
DATA_PERIOD_DAYS = 15  # API data is for half month (15 days)
TARGET_PERIOD_DAYS = 15  # Recommendations should be for same period (15 days)
SCALING_FACTOR = TARGET_PERIOD_DAYS / DATA_PERIOD_DAYS  # 1.0 for same period

# Analysis configurations
ANALYSIS_CONFIGS = {
    "subcategory": {
        "cluster_file": "output/clustering_results_subcategory.csv",
        "data_file": "data/api_data/store_config_data.csv",
        "description": "Subcategory-Level Below Minimum Analysis",
        "output_prefix": "rule9_below_minimum_subcategory",
        "grouping_columns": ['season_name', 'sex_name', 'display_location_name', 'big_class_name', 'sub_cate_name'],
        "feature_name": "subcategory",
        "data_source": "planning"  # Use planning data
    },
    "spu": {
        "cluster_file": "output/clustering_results_spu.csv",
        "data_file": "data/api_data/store_config_data.csv",
        "description": "SPU-Level Below Minimum Analysis with Quantity Targets", 
        "output_prefix": "rule9_below_minimum_spu",
        "grouping_columns": ['season_name', 'sex_name', 'display_location_name', 'big_class_name', 'sub_cate_name', 'sty_code'],
        "feature_name": "SPU",
        "data_source": "sales"  # Use sales data as proxy
    }
}

# QUANTITY ENHANCEMENT: Add quantity data for adjustment calculations (period-dynamic)
import os as _os
from config import get_current_period, get_period_label
_yyyymm, _period = get_current_period()
_period_label = get_period_label(_yyyymm, _period)
_dynamic_quantity_file = f"data/api_data/complete_spu_sales_{_period_label}.csv"
QUANTITY_DATA_FILE = _dynamic_quantity_file if _os.path.exists(_dynamic_quantity_file) else "data/api_data/complete_spu_sales_202506B.csv"

# Get current analysis configuration
CURRENT_CONFIG = ANALYSIS_CONFIGS[ANALYSIS_LEVEL]

# File paths based on analysis level
CLUSTER_RESULTS_FILE = CURRENT_CONFIG["cluster_file"]
from config import get_api_data_files
_api_files_for_rule9 = get_api_data_files(_yyyymm, _period)
_dynamic_planning_path = _api_files_for_rule9['store_config']
DATA_FILE = _dynamic_planning_path if os.path.exists(_dynamic_planning_path) else CURRENT_CONFIG["data_file"]
OUTPUT_DIR = "output"
RESULTS_FILE = f"output/{CURRENT_CONFIG['output_prefix']}_results.csv"

# Rule parameters - adaptive based on analysis level
if ANALYSIS_LEVEL == "subcategory":
    MINIMUM_STYLE_THRESHOLD = 1.5  # Minimum viable style count for subcategories (reasonable threshold)
    # QUANTITY THRESHOLDS for subcategory adjustments
    MIN_BOOST_QUANTITY = 2.0  # Minimum units to recommend boosting
    DISCONTINUE_THRESHOLD = 1.0  # Below this, recommend discontinuation
else:  # SPU level
    MINIMUM_STYLE_THRESHOLD = 0.05  # Minimum allocation proxy for individual SPUs (targets bottom ~15-20% with critically low allocation)
    # QUANTITY THRESHOLDS for SPU adjustments
    MIN_BOOST_QUANTITY = 1.0  # Minimum units to recommend boosting
    DISCONTINUE_THRESHOLD = 0.02  # Below 0.02 allocation (20 sales units), recommend discontinuation

# Create output directory
os.makedirs(OUTPUT_DIR, exist_ok=True)

def log_progress(message: str) -> None:
    """
    Log progress with timestamp.
    
    Args:
        message (str): Progress message to log
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def load_quantity_data() -> Optional[pd.DataFrame]:
    """
    Load quantity data for adjustment calculations.
    
    Returns:
        DataFrame with quantity data or None if not available
    """
    try:
        if os.path.exists(QUANTITY_DATA_FILE):
            log_progress("Loading quantity data for adjustment calculations...")
            quantity_df = pd.read_csv(QUANTITY_DATA_FILE, dtype={'str_code': str})
            
            # Clean quantity data and ensure a 'quantity' column exists
            if 'spu_sales_amt' not in quantity_df.columns and 'sal_amt' in quantity_df.columns:
                quantity_df['spu_sales_amt'] = pd.to_numeric(quantity_df['sal_amt'], errors='coerce').fillna(0)
            else:
                quantity_df['spu_sales_amt'] = pd.to_numeric(quantity_df.get('spu_sales_amt', 0), errors='coerce').fillna(0)

            # Derive quantity from sal_qty when available; otherwise estimate from sales amount
            if 'quantity' not in quantity_df.columns:
                if 'sal_qty' in quantity_df.columns:
                    quantity_df['quantity'] = pd.to_numeric(quantity_df['sal_qty'], errors='coerce').fillna(0)
                else:
                    quantity_df['quantity'] = (quantity_df['spu_sales_amt'] / 300.0).fillna(0)

            quantity_df = quantity_df[quantity_df['spu_sales_amt'] > 0].copy()
            
            log_progress(f"Loaded {len(quantity_df):,} quantity records (quantity column ready)")
            return quantity_df
        else:
            log_progress(f"Quantity data file not found: {QUANTITY_DATA_FILE}")
            return None
    except Exception as e:
        log_progress(f"Warning: Could not load quantity data ({str(e)})")
        return None

def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Load clustering results and data based on analysis level.
    
    Returns:
        Tuple containing cluster assignments and data
    """
    try:
        log_progress(f"Loading data for {CURRENT_CONFIG['description']}...")
        
        # Check if files exist
        if not os.path.exists(CLUSTER_RESULTS_FILE):
            # Fallback to default clustering results for backward compatibility
            fallback_file = "output/clustering_results.csv"
            if os.path.exists(fallback_file):
                log_progress(f"Using fallback clustering file: {fallback_file}")
                CLUSTER_RESULTS_FILE_ACTUAL = fallback_file
            else:
                raise FileNotFoundError(f"Clustering results not found: {CLUSTER_RESULTS_FILE}")
        else:
            CLUSTER_RESULTS_FILE_ACTUAL = CLUSTER_RESULTS_FILE
            
        if not os.path.exists(DATA_FILE):
            raise FileNotFoundError(f"Data file not found: {DATA_FILE}")
        
        # Load cluster assignments
        cluster_df = pd.read_csv(CLUSTER_RESULTS_FILE_ACTUAL, dtype={'str_code': str})
        log_progress(f"Loaded cluster assignments for {len(cluster_df)} stores")
        
        # Load data file
        log_progress(f"Loading data from {DATA_FILE}")
        data_df = pd.read_csv(DATA_FILE, dtype={'str_code': str}, low_memory=False)
        log_progress(f"Loaded data with {len(data_df)} rows")
        
        # Ensure str_code is string for consistent joining
        cluster_df['str_code'] = cluster_df['str_code'].astype(str)
        data_df['str_code'] = data_df['str_code'].astype(str)
        
        # Validate required columns based on analysis level
        if ANALYSIS_LEVEL == "subcategory":
            required_cols = ['str_code', 'target_sty_cnt_avg'] + CURRENT_CONFIG['grouping_columns']
        else:
            # For SPU analysis, we need sty_sal_amt and base grouping columns (sty_code will be created)
            base_grouping_cols = [col for col in CURRENT_CONFIG['grouping_columns'] if col != 'sty_code']
            required_cols = ['str_code', 'sty_sal_amt'] + base_grouping_cols
        
        missing_cols = [col for col in required_cols if col not in data_df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns in data: {missing_cols}")
        
        log_progress(f"Data validation successful for {ANALYSIS_LEVEL} analysis")
        
        return cluster_df, data_df
        
    except Exception as e:
        log_progress(f"Error loading data: {str(e)}")
        raise

def prepare_store_data(data_df: pd.DataFrame, cluster_df: pd.DataFrame) -> pd.DataFrame:
    """
    Prepare store data with cluster information for the specified analysis level.
    
    Args:
        data_df: Data with target_sty_cnt_avg or sty_sal_amt
        cluster_df: Cluster assignments
        
    Returns:
        DataFrame with store data and cluster information
    """
    feature_type = "subcategories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
    log_progress(f"Preparing {ANALYSIS_LEVEL} data...")
    
    if ANALYSIS_LEVEL == "subcategory":
        # Use traditional planning data approach
        data_with_clusters = data_df.merge(cluster_df[['str_code', 'Cluster']], on='str_code', how='inner')
        log_progress(f"Merged data: {len(data_with_clusters)} records with cluster information")
        
        # Clean and prepare data
        data_with_clusters['target_sty_cnt_avg'] = pd.to_numeric(data_with_clusters['target_sty_cnt_avg'], errors='coerce').fillna(0)
        data_with_clusters['style_count'] = data_with_clusters['target_sty_cnt_avg']
        
        # Create category key for grouping
        grouping_cols = CURRENT_CONFIG['grouping_columns']
        data_with_clusters['category_key'] = data_with_clusters[grouping_cols].apply(
            lambda x: '|'.join(x.astype(str)), axis=1
        )
        
        # Only consider positive style counts
        store_data = data_with_clusters[
            data_with_clusters['style_count'] > 0
        ].copy()
        
    else:
        # SPU analysis: Use sales data as proxy for style allocation
        log_progress("Expanding SPU sales data for below minimum analysis...")
        
        # Filter records with SPU sales data
        spu_records = data_df[data_df['sty_sal_amt'].notna() & (data_df['sty_sal_amt'] != '')].copy()
        log_progress(f"Found {len(spu_records)} records with SPU sales data")
        
        # Expand SPU sales data with optimized processing
        expanded_records = []
        batch_size = 1000
        total_batches = (len(spu_records) + batch_size - 1) // batch_size
        
        log_progress(f"Processing {len(spu_records)} records in {total_batches} batches...")
        
        for batch_idx in tqdm(range(0, len(spu_records), batch_size), desc="Expanding SPU data", unit="batch"):
            batch = spu_records.iloc[batch_idx:batch_idx + batch_size]
            batch_records = []
            
            for _, row in batch.iterrows():
                try:
                    spu_data = eval(row['sty_sal_amt'])  # Parse JSON-like string
                    if isinstance(spu_data, dict):
                        for sty_code, sal_amt in spu_data.items():
                            if sal_amt > 0:  # Only consider SPUs with sales
                                # Create minimal record to save memory
                                expanded_record = {
                                    'str_code': row['str_code'],
                                    'season_name': row['season_name'],
                                    'sex_name': row['sex_name'],
                                    'display_location_name': row['display_location_name'],
                                    'big_class_name': row['big_class_name'],
                                    'sub_cate_name': row['sub_cate_name'],
                                    'sty_code': sty_code,
                                    'style_count': float(sal_amt) / 1000,  # Convert sales to style count proxy
                                    'original_sales_amt': sal_amt  # Keep original for quantity calculations
                                }
                                batch_records.append(expanded_record)
                except:
                    continue
            
            expanded_records.extend(batch_records)
            
            # Progress update every 10 batches
            if (batch_idx // batch_size + 1) % 10 == 0:
                log_progress(f"Processed {batch_idx // batch_size + 1}/{total_batches} batches, "
                           f"generated {len(expanded_records)} SPU records so far")
        
        if not expanded_records:
            log_progress("No valid SPU sales data found for below minimum analysis")
            return pd.DataFrame()
        
        # Convert to DataFrame
        expanded_df = pd.DataFrame(expanded_records)
        log_progress(f"Expanded to {len(expanded_df)} SPU-level records")
        
        # Merge with cluster information
        data_with_clusters = expanded_df.merge(cluster_df[['str_code', 'Cluster']], on='str_code', how='inner')
        log_progress(f"Merged data: {len(data_with_clusters)} records with cluster information")
        
        # Create category key for grouping
        grouping_cols = CURRENT_CONFIG['grouping_columns']
        data_with_clusters['category_key'] = data_with_clusters[grouping_cols].apply(
            lambda x: '|'.join(x.astype(str)), axis=1
        )
        
        store_data = data_with_clusters.copy()
    
    log_progress(f"Prepared {len(store_data)} store-{ANALYSIS_LEVEL} records")
    log_progress(f"Analysis covers {store_data['category_key'].nunique()} unique {feature_type}")
    
    return store_data

def enhance_with_quantity_recommendations(below_minimum_cases: pd.DataFrame, quantity_df: Optional[pd.DataFrame]) -> pd.DataFrame:
    """
    Enhance below minimum cases with quantity adjustment recommendations.
    
    Args:
        below_minimum_cases: Below minimum cases
        quantity_df: Quantity data for calculations
        
    Returns:
        Enhanced DataFrame with quantity recommendations
    """
    if ANALYSIS_LEVEL != "spu" or quantity_df is None:
        # For subcategory or when no quantity data, return original
        below_minimum_cases['quantity_recommendation'] = "Increase allocation to minimum viable level"
        below_minimum_cases['recommended_action'] = "BOOST"
        below_minimum_cases['current_quantity'] = 0
        below_minimum_cases['recommended_quantity_change'] = 0
        below_minimum_cases['target_quantity'] = 0
        return below_minimum_cases
    
    log_progress("Enhancing below minimum cases with quantity recommendations...")
    
    # Prepare quantity lookup
    quantity_clean = quantity_df[quantity_df['spu_sales_amt'] > 0].copy()
    quantity_clean['spu_sales_amt'] = pd.to_numeric(quantity_clean['spu_sales_amt'], errors='coerce').fillna(0)
    
    # Create lookup dictionary for faster access
    quantity_lookup = {}
    for _, row in quantity_clean.iterrows():
        key = f"{row['str_code']}_{row['spu_code']}"
        quantity_lookup[key] = row['quantity']  # REAL QUANTITY instead of sales amount * SCALING_FACTOR
    
    log_progress(f"Created quantity lookup for {len(quantity_lookup):,} store-SPU combinations")
    
    # Enhance each below minimum case
    enhanced_cases = []
    
    for _, row in tqdm(below_minimum_cases.iterrows(), total=len(below_minimum_cases), desc="Processing quantity recommendations"):
        enhanced_row = row.copy()
        
        # Get current quantity for this store-SPU combination
        quantity_key = f"{row['str_code']}_{row['sty_code']}"
        current_quantity = quantity_lookup.get(quantity_key, 0.0)
        
        # If no quantity data, use sales amount as proxy
        if current_quantity == 0 and 'original_sales_amt' in row:
            current_quantity = row['original_sales_amt'] * SCALING_FACTOR
        
        # Calculate target quantity (minimum viable level)
        current_allocation = row['style_count']
        target_allocation = MINIMUM_STYLE_THRESHOLD
        
        # Estimate target quantity based on allocation ratio
        if current_allocation > 0:
            allocation_multiplier = target_allocation / current_allocation
            target_quantity = current_quantity * allocation_multiplier
        else:
            target_quantity = MIN_BOOST_QUANTITY  # Default minimum
        
        # Calculate quantity change needed
        quantity_change = target_quantity - current_quantity
        
        # Determine recommendation based on current allocation level
        if current_allocation < DISCONTINUE_THRESHOLD:
            # Very low allocation - recommend discontinuation
            recommended_action = "DISCONTINUE"
            quantity_recommendation = f"DISCONTINUE - only {current_quantity:.1f} units sold/15-days (allocation {current_allocation:.3f} < {DISCONTINUE_THRESHOLD:.3f} discontinue threshold)"
            recommended_quantity_change = -current_quantity  # Remove all
        elif quantity_change >= MIN_BOOST_QUANTITY:
            # Worth boosting to minimum viable level
            recommended_action = "BOOST"
            quantity_recommendation = f"ADD {quantity_change:.1f} units to reach {target_quantity:.1f} units/15-days (minimum viable allocation {target_allocation:.3f})"
            recommended_quantity_change = quantity_change
        else:
            # Small adjustment needed
            recommended_action = "MINOR_BOOST"
            quantity_recommendation = f"ADD {max(MIN_BOOST_QUANTITY, quantity_change):.1f} units to reach minimum viable level"
            recommended_quantity_change = max(MIN_BOOST_QUANTITY, quantity_change)
        
        # Add enhancement fields with STANDARDIZED column names
        enhanced_row['spu_code'] = row['sty_code']  # STANDARDIZED: Use spu_code instead of sty_code
        enhanced_row['current_quantity'] = current_quantity
        enhanced_row['target_quantity'] = target_quantity
        enhanced_row['recommended_quantity_change'] = recommended_quantity_change
        enhanced_row['recommended_action'] = recommended_action
        enhanced_row['recommendation_text'] = quantity_recommendation  # STANDARDIZED: Use recommendation_text
        
        # Calculate estimated cost (using average unit price)
        if current_quantity > 0:
            estimated_unit_price = row['original_sales_amt'] / current_quantity if 'original_sales_amt' in row else 100.0
        else:
            estimated_unit_price = row.get('unit_price', 50.0)  # REAL UNIT PRICE from API  # Default estimate
        
        investment_required = abs(recommended_quantity_change) * estimated_unit_price
        enhanced_row['investment_required'] = investment_required if recommended_action != "DISCONTINUE" else 0  # STANDARDIZED
        enhanced_row['unit_price'] = estimated_unit_price  # STANDARDIZED
        
        enhanced_cases.append(enhanced_row)
    
    enhanced_df = pd.DataFrame(enhanced_cases)
    
    # Log summary statistics
    action_counts = enhanced_df['recommended_action'].value_counts()
    log_progress(f"Quantity recommendation summary: {dict(action_counts)}")
    
    total_investment = enhanced_df[enhanced_df['recommended_action'].isin(['BOOST', 'MINOR_BOOST'])]['investment_required'].sum()
    total_quantity_add = enhanced_df[enhanced_df['recommended_action'].isin(['BOOST', 'MINOR_BOOST'])]['recommended_quantity_change'].sum()
    
    log_progress(f"Total quantity to add: {total_quantity_add:.1f} units/15-days")
    log_progress(f"Estimated total investment: ${total_investment:.0f}")
    
    return enhanced_df

def identify_below_minimum_cases(store_data: pd.DataFrame) -> pd.DataFrame:
    """
    Identify cases that are below minimum threshold.
    
    Args:
        store_data: Store data with style counts
        
    Returns:
        DataFrame with below minimum cases
    """
    feature_type = "subcategories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
    log_progress(f"Identifying below minimum {ANALYSIS_LEVEL} cases...")
    
    # Find cases where style count is positive but below minimum threshold
    below_minimum = store_data[
        (store_data['style_count'] > 0) & 
        (store_data['style_count'] < MINIMUM_STYLE_THRESHOLD)
    ].copy()
    
    # Calculate recommended target (minimum viable)
    below_minimum['recommended_target'] = MINIMUM_STYLE_THRESHOLD
    below_minimum['increase_needed'] = below_minimum['recommended_target'] - below_minimum['style_count']
    
    # Add issue classification
    below_minimum['issue_type'] = 'BELOW_MINIMUM'
    
    # Calculate severity based on analysis level
    if ANALYSIS_LEVEL == "subcategory":
        # Traditional thresholds for subcategory
        below_minimum['issue_severity'] = np.where(
            below_minimum['style_count'] < 0.5, 'HIGH', 'MEDIUM'
        )
    else:
        # Adjusted thresholds for SPU (more granular)
        below_minimum['issue_severity'] = np.where(
            below_minimum['style_count'] < DISCONTINUE_THRESHOLD, 'CRITICAL',  # Recommend discontinuation
            np.where(below_minimum['style_count'] < 0.03, 'HIGH', 
                    np.where(below_minimum['style_count'] < 0.04, 'MEDIUM', 'LOW'))
        )
    
    log_progress(f"Identified {len(below_minimum)} below minimum {ANALYSIS_LEVEL} cases")
    
    if len(below_minimum) > 0:
        # Log severity breakdown
        severity_counts = below_minimum['issue_severity'].value_counts()
        log_progress(f"  â€¢ Severity breakdown: {dict(severity_counts)}")
        
        avg_current = below_minimum['style_count'].mean()
        total_increase = below_minimum['increase_needed'].sum()
        log_progress(f"  â€¢ Average current count: {avg_current:.3f}")
        log_progress(f"  â€¢ Total increase needed: {total_increase:.2f}")
    
    return below_minimum

def apply_below_minimum_rule(cluster_df: pd.DataFrame, store_data: pd.DataFrame, 
                           below_minimum_cases: pd.DataFrame) -> pd.DataFrame:
    """
    Apply the below minimum rule to all stores and create rule results.
    
    Args:
        cluster_df: Cluster assignments
        store_data: Store data
        below_minimum_cases: Below minimum cases
        
    Returns:
        DataFrame with rule results for all stores
    """
    feature_type = "categories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
    log_progress(f"Applying below minimum {feature_type} rule to all stores...")
    
    # Create base results for all stores
    results_df = cluster_df[['str_code', 'Cluster']].copy()
    
    # Count below minimum cases per store
    if len(below_minimum_cases) > 0:
        store_below_min_counts = below_minimum_cases.groupby('str_code').agg({
            'category_key': 'count',
            'increase_needed': 'sum',
            'style_count': 'mean',  # Average current count
            'recommended_quantity_change': 'sum',  # Total quantity change needed
            'investment_required': 'sum'  # Total investment needed (STANDARDIZED)
        }).reset_index()
        
        if ANALYSIS_LEVEL == "subcategory":
            store_below_min_counts.columns = [
                'str_code', 'below_minimum_count', 'total_increase_needed', 'avg_current_count',
                'total_quantity_change', 'total_investment'
            ]
            count_col = 'below_minimum_count'
            rule_col = 'rule9_below_minimum'
        else:
            store_below_min_counts.columns = [
                'str_code', 'below_minimum_spus_count', 'total_increase_needed', 'avg_current_count',
                'total_quantity_change', 'total_investment'
            ]
            count_col = 'below_minimum_spus_count'
            rule_col = 'rule9_below_minimum_spu'
        
        # Merge with results
        results_df = results_df.merge(store_below_min_counts, on='str_code', how='left')
        results_df[count_col] = results_df[count_col].fillna(0)
        results_df['total_increase_needed'] = results_df['total_increase_needed'].fillna(0)
        results_df['avg_current_count'] = results_df['avg_current_count'].fillna(0)
        results_df['total_quantity_change'] = results_df['total_quantity_change'].fillna(0)
        results_df['total_investment'] = results_df['total_investment'].fillna(0)
        
        # Create rule flag
        results_df[rule_col] = (results_df[count_col] > 0).astype(int)
    else:
        # No below minimum cases found
        if ANALYSIS_LEVEL == "subcategory":
            results_df['below_minimum_count'] = 0
            rule_col = 'rule9_below_minimum'
        else:
            results_df['below_minimum_spus_count'] = 0
            rule_col = 'rule9_below_minimum_spu'
        
        results_df['total_increase_needed'] = 0
        results_df['avg_current_count'] = 0
        results_df['total_quantity_change'] = 0
        results_df['total_investment'] = 0
        results_df[rule_col] = 0
    
    # Add metadata
    if ANALYSIS_LEVEL == "subcategory":
        results_df['rule9_description'] = 'Store has subcategories below minimum viable style count'
    else:
        results_df['rule9_description'] = 'Store has SPUs below minimum allocation level with quantity recommendations'
    
    results_df['rule9_threshold'] = f"<{MINIMUM_STYLE_THRESHOLD} allocation (when >0)"
    results_df['rule9_analysis_level'] = ANALYSIS_LEVEL
    
    flagged_stores = results_df[rule_col].sum()
    log_progress(f"Applied below minimum {feature_type} rule: {flagged_stores} stores flagged")
    
    return results_df

def save_results(results_df: pd.DataFrame, below_minimum_cases: pd.DataFrame, 
                store_data: pd.DataFrame) -> None:
    """
    Save rule results and detailed analysis.
    
    Args:
        results_df: Rule results for all stores
        below_minimum_cases: Detailed below minimum cases
        store_data: Complete store data
    """
    feature_type = "categories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
    
    # Save main rule results
    results_df.to_csv(RESULTS_FILE, index=False)
    log_progress(f"Saved {ANALYSIS_LEVEL} rule results to {RESULTS_FILE}")
    
    # Save detailed below minimum cases if any exist
    if len(below_minimum_cases) > 0:
        cases_file = f"output/{CURRENT_CONFIG['output_prefix']}_cases.csv"
        below_minimum_cases.to_csv(cases_file, index=False)
        log_progress(f"Saved detailed below minimum cases to {cases_file}")
    
    # Generate summary report
    summary_file = f"output/{CURRENT_CONFIG['output_prefix']}_summary.md"
    with open(summary_file, 'w') as f:
        f.write(f"# Rule 9: Below Minimum {feature_type.title()} Analysis Summary\n\n")
        f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Analysis Level**: {ANALYSIS_LEVEL.title()}\n\n")
        
        f.write("## Rule Definition\n")
        f.write(f"Identifies store-{ANALYSIS_LEVEL} combinations with positive but below minimum viable allocation levels.\n")
        if ANALYSIS_LEVEL == "spu":
            f.write("**ENHANCEMENT**: Provides specific UNIT QUANTITY adjustment recommendations for below-minimum cases.\n")
        f.write("\n")
        
        f.write("## Parameters\n")
        f.write(f"- Analysis Level: {ANALYSIS_LEVEL}\n")
        f.write(f"- Minimum allocation threshold: {MINIMUM_STYLE_THRESHOLD}\n")
        f.write("- Logic: 0 < allocation < threshold\n")
        if ANALYSIS_LEVEL == "spu":
            f.write(f"- Discontinue threshold: {DISCONTINUE_THRESHOLD} (recommend discontinuation below this level)\n")
            f.write(f"- Minimum boost quantity: {MIN_BOOST_QUANTITY} units\n")
            f.write(f"- Data period: {DATA_PERIOD_DAYS} days\n")
            f.write(f"- Target period: {TARGET_PERIOD_DAYS} days\n")
        f.write("\n")
        
        f.write("## Results\n")
        f.write(f"- Total stores analyzed: {len(results_df):,}\n")
        
        rule_col = 'rule9_below_minimum' if ANALYSIS_LEVEL == "subcategory" else 'rule9_below_minimum_spu'
        flagged_stores = results_df[rule_col].sum()
        f.write(f"- Stores with below minimum issues: {flagged_stores:,}\n")
        f.write(f"- Total below minimum cases: {len(below_minimum_cases):,}\n")
        f.write(f"- Total store-{ANALYSIS_LEVEL} combinations: {len(store_data):,}\n")
        
        if len(below_minimum_cases) > 0:
            f.write(f"- Average cases per flagged store: {len(below_minimum_cases) / max(1, flagged_stores):.1f}\n")
            f.write(f"- Total allocation increase needed: {below_minimum_cases['increase_needed'].sum():.2f}\n")
            f.write(f"- Average current allocation: {below_minimum_cases['style_count'].mean():.3f}\n")
            
            # QUANTITY ENHANCEMENT: Add quantity metrics
            if ANALYSIS_LEVEL == "spu" and 'recommended_action' in below_minimum_cases.columns:
                action_counts = below_minimum_cases['recommended_action'].value_counts()
                f.write("\n## Quantity Recommendation Metrics\n")
                for action, count in action_counts.items():
                    f.write(f"- {action}: {count:,} cases\n")
                
                boost_cases = below_minimum_cases[below_minimum_cases['recommended_action'].isin(['BOOST', 'MINOR_BOOST'])]
                if len(boost_cases) > 0:
                    total_quantity_add = boost_cases['recommended_quantity_change'].sum()
                    total_investment = boost_cases['investment_required'].sum()
                    f.write(f"- Total quantity to add: {total_quantity_add:.1f} units/{TARGET_PERIOD_DAYS}-days\n")
                    f.write(f"- Estimated total investment: ${total_investment:.0f}\n")
                    f.write(f"- Average investment per boosted SPU: ${total_investment/len(boost_cases):.0f}\n")
                
                discontinue_cases = below_minimum_cases[below_minimum_cases['recommended_action'] == 'DISCONTINUE']
                if len(discontinue_cases) > 0:
                    f.write(f"- SPUs recommended for discontinuation: {len(discontinue_cases):,}\n")
                    f.write(f"- Investment impact: **POSITIVE** (discontinuation saves costs)\n")
            
            # Severity breakdown
            severity_counts = below_minimum_cases['issue_severity'].value_counts()
            f.write("\n## Severity Breakdown\n")
            for severity, count in severity_counts.items():
                f.write(f"- {severity}: {count:,} cases\n")
            
            # Top affected features
            feature_col = 'sub_cate_name' if ANALYSIS_LEVEL == "subcategory" else 'sty_code'
            if feature_col in below_minimum_cases.columns:
                f.write(f"\n## Most Affected {CURRENT_CONFIG['feature_name']}s\n")
                top_affected = below_minimum_cases[feature_col].value_counts().head(10)
                for feature, count in top_affected.items():
                    f.write(f"- {feature}: {count:,} cases\n")
            
            # Cluster-level insights
            f.write(f"\n## Cluster-Level Insights\n")
            cluster_summary = below_minimum_cases.groupby('Cluster').agg({
                'str_code': 'nunique',
                'style_count': 'mean'
            }).sort_values('style_count').head(5)
            
            f.write("Top 5 clusters by lowest average allocation:\n")
            for cluster_id, row in cluster_summary.iterrows():
                f.write(f"- Cluster {cluster_id}: {row['str_code']} stores, avg allocation {row['style_count']:.3f}\n")
        else:
            f.write(f"No below minimum {feature_type} identified.\n")
    
    log_progress(f"Saved summary report to {summary_file}")
    
    # Also save backward-compatible file for subcategory analysis
    if ANALYSIS_LEVEL == "subcategory":
        backward_compatible_file = "output/rule9_below_minimum_results.csv"
        results_df.to_csv(backward_compatible_file, index=False)
        log_progress(f"Saved backward-compatible results to {backward_compatible_file}")

def main() -> None:
    """Main function to execute below minimum rule analysis"""
    start_time = datetime.now()
    log_progress(f"Starting Rule 9: {CURRENT_CONFIG['description']}...")
    
    try:
        # Load data
        cluster_df, data_df = load_data()
        
        # QUANTITY ENHANCEMENT: Load quantity data
        quantity_df = load_quantity_data()
        
        # Prepare store data
        store_data = prepare_store_data(data_df, cluster_df)
        
        if len(store_data) == 0:
            log_progress(f"No valid data for {ANALYSIS_LEVEL} below minimum analysis. Creating empty results.")
            results_df = cluster_df[['str_code', 'Cluster']].copy()
            
            if ANALYSIS_LEVEL == "subcategory":
                results_df['rule9_below_minimum'] = 0
                results_df['below_minimum_count'] = 0
            else:
                results_df['rule9_below_minimum_spu'] = 0
                results_df['below_minimum_spus_count'] = 0
            
            results_df['total_increase_needed'] = 0
            results_df['avg_current_count'] = 0
            results_df['total_quantity_change'] = 0
            results_df['total_investment'] = 0
            results_df['rule9_description'] = f'No valid data for {ANALYSIS_LEVEL} below minimum analysis'
            results_df['rule9_threshold'] = 'N/A'
            results_df['rule9_analysis_level'] = ANALYSIS_LEVEL
            results_df.to_csv(RESULTS_FILE, index=False)
            return
        
        # Identify below minimum cases
        below_minimum_cases = identify_below_minimum_cases(store_data)
        
        # QUANTITY ENHANCEMENT: Add quantity recommendations
        if len(below_minimum_cases) > 0:
            below_minimum_cases = enhance_with_quantity_recommendations(below_minimum_cases, quantity_df)
        
        # Apply rule and create results
        results_df = apply_below_minimum_rule(cluster_df, store_data, below_minimum_cases)
        
        # Save results
        save_results(results_df, below_minimum_cases, store_data)
        
        # Calculate completion time
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Summary
        rule_col = 'rule9_below_minimum' if ANALYSIS_LEVEL == "subcategory" else 'rule9_below_minimum_spu'
        flagged_stores = results_df[rule_col].sum()
        feature_type = "categories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
        
        log_progress("\n" + "="*70)
        log_progress(f"RULE 9: BELOW MINIMUM {feature_type.upper()} ANALYSIS COMPLETE")
        log_progress("="*70)
        log_progress(f"Analysis Level: {ANALYSIS_LEVEL.upper()}")
        log_progress(f"Process completed in {duration:.2f} seconds")
        log_progress(f"âœ“ Stores analyzed: {len(results_df):,}")
        log_progress(f"âœ“ Stores flagged: {flagged_stores:,}")
        log_progress(f"âœ“ Below minimum {feature_type}: {len(below_minimum_cases):,}")
        
        if len(below_minimum_cases) > 0:
            avg_current = below_minimum_cases['style_count'].mean()
            total_increase = below_minimum_cases['increase_needed'].sum()
            log_progress(f"âœ“ Average current allocation: {avg_current:.3f}")
            log_progress(f"âœ“ Total allocation increase needed: {total_increase:.2f}")
            
            # QUANTITY ENHANCEMENT: Log quantity metrics
            if ANALYSIS_LEVEL == "spu" and 'recommended_action' in below_minimum_cases.columns:
                action_counts = below_minimum_cases['recommended_action'].value_counts()
                log_progress(f"âœ“ Quantity recommendations: {dict(action_counts)}")
                
                boost_cases = below_minimum_cases[below_minimum_cases['recommended_action'].isin(['BOOST', 'MINOR_BOOST'])]
                if len(boost_cases) > 0:
                    total_quantity_add = boost_cases['recommended_quantity_change'].sum()
                    total_investment = boost_cases['investment_required'].sum()
                    log_progress(f"âœ“ Total quantity to add: {total_quantity_add:.1f} units/{TARGET_PERIOD_DAYS}-days")
                    log_progress(f"âœ“ Estimated investment: ${total_investment:.0f}")
                
                discontinue_cases = below_minimum_cases[below_minimum_cases['recommended_action'] == 'DISCONTINUE']
                if len(discontinue_cases) > 0:
                    log_progress(f"âœ“ SPUs for discontinuation: {len(discontinue_cases):,}")
        
        log_progress(f"\nNext step: Run python src/step10_*.py for additional business rule analysis")
        
    except Exception as e:
        log_progress(f"Error in below minimum {ANALYSIS_LEVEL} rule analysis: {str(e)}")
        raise

if __name__ == "__main__":
    main() 