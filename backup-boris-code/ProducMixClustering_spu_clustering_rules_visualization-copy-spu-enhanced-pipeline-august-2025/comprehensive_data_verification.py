#!/usr/bin/env python3
"""
Comprehensive Data Verification for Fixed CSV Output
====================================================

This script performs thorough verification to ensure:
1. ALL original data is preserved (no data loss)
2. ALL financial metrics are intact  
3. ALL recommendation logic is maintained
4. Style tags are enhanced with REAL attributes (not synthetic)
5. Output is ready for production use

This verification ensures the fixed CSV meets all requirements for business use.
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Any

def verify_data_preservation() -> Dict[str, Any]:
    """Verify all original data is preserved in the fixed CSV."""
    print("üî¨ COMPREHENSIVE DATA PRESERVATION VERIFICATION")
    print("=" * 70)
    
    # Load files
    print("üìÇ Loading files for verification...")
    original = pd.read_csv('fast_fish_with_sell_through_analysis_20250714_124522.csv')
    fixed = pd.read_csv('fast_fish_with_complete_style_attributes_20250715_105659.csv')
    config = pd.read_csv('data/data/api_data/store_config_data.csv')
    
    verification_results = {}
    
    # Basic structure verification
    print("\nüìä BASIC STRUCTURE VERIFICATION:")
    verification_results['structure'] = {
        'original_records': len(original),
        'fixed_records': len(fixed),
        'records_preserved': len(original) == len(fixed),
        'original_columns': len(original.columns),
        'fixed_columns': len(fixed.columns),
        'columns_preserved': len(original.columns) == len(fixed.columns)
    }
    
    print(f"   Records: {len(original):,} ‚Üí {len(fixed):,} {'‚úÖ' if len(original) == len(fixed) else '‚ùå'}")
    print(f"   Columns: {len(original.columns)} ‚Üí {len(fixed.columns)} {'‚úÖ' if len(original.columns) == len(fixed.columns) else '‚ùå'}")
    
    # Column verification
    print("\nüìã COLUMN PRESERVATION VERIFICATION:")
    missing_columns = set(original.columns) - set(fixed.columns)
    extra_columns = set(fixed.columns) - set(original.columns)
    
    verification_results['columns'] = {
        'missing_columns': list(missing_columns),
        'extra_columns': list(extra_columns),
        'all_columns_preserved': len(missing_columns) == 0 and len(extra_columns) == 0
    }
    
    if missing_columns:
        print(f"   ‚ùå Missing columns: {missing_columns}")
    else:
        print(f"   ‚úÖ No missing columns")
    
    if extra_columns:
        print(f"   ‚ö†Ô∏è  Extra columns: {extra_columns}")
    else:
        print(f"   ‚úÖ No extra columns")
    
    # Financial data integrity
    print("\nüí∞ FINANCIAL DATA INTEGRITY VERIFICATION:")
    financial_columns = [
        'Total_Current_Sales', 'Avg_Sales_Per_SPU', 'Historical_Total_Sales_202408A',
        'SPU_Store_Days_Inventory', 'SPU_Store_Days_Sales', 'Sell_Through_Rate'
    ]
    
    financial_verification = {}
    for col in financial_columns:
        if col in original.columns and col in fixed.columns:
            if original[col].dtype in ['float64', 'int64']:
                orig_sum = original[col].sum()
                fixed_sum = fixed[col].sum()
                orig_mean = original[col].mean()
                fixed_mean = fixed[col].mean()
                
                financial_verification[col] = {
                    'original_sum': float(orig_sum),
                    'fixed_sum': float(fixed_sum),
                    'sum_preserved': abs(orig_sum - fixed_sum) < 0.01,
                    'original_mean': float(orig_mean),
                    'fixed_mean': float(fixed_mean),
                    'mean_preserved': abs(orig_mean - fixed_mean) < 0.01
                }
                
                sum_match = '‚úÖ' if abs(orig_sum - fixed_sum) < 0.01 else '‚ùå'
                print(f"   {col}: {sum_match} Sum: {orig_sum:,.0f} ‚Üí {fixed_sum:,.0f}")
    
    verification_results['financial'] = financial_verification
    
    # Quantity data integrity
    print("\nüìà QUANTITY DATA INTEGRITY VERIFICATION:")
    quantity_columns = [
        'Current_SPU_Quantity', 'Target_SPU_Quantity', 'Historical_SPU_Quantity_202408A'
    ]
    
    quantity_verification = {}
    for col in quantity_columns:
        if col in original.columns and col in fixed.columns:
            orig_sum = original[col].sum()
            fixed_sum = fixed[col].sum()
            
            quantity_verification[col] = {
                'original_sum': int(orig_sum),
                'fixed_sum': int(fixed_sum),
                'sum_preserved': orig_sum == fixed_sum
            }
            
            match = '‚úÖ' if orig_sum == fixed_sum else '‚ùå'
            print(f"   {col}: {match} {orig_sum:,.0f} ‚Üí {fixed_sum:,.0f}")
    
    verification_results['quantities'] = quantity_verification
    
    # Store group verification
    print("\nüè™ STORE GROUP VERIFICATION:")
    orig_groups = set(original['Store_Group_Name'].unique())
    fixed_groups = set(fixed['Store_Group_Name'].unique())
    
    verification_results['store_groups'] = {
        'original_groups': list(orig_groups),
        'fixed_groups': list(fixed_groups),
        'groups_preserved': orig_groups == fixed_groups
    }
    
    print(f"   Store groups preserved: {'‚úÖ' if orig_groups == fixed_groups else '‚ùå'}")
    print(f"   Groups: {len(orig_groups)} ‚Üí {len(fixed_groups)}")
    
    return verification_results

def verify_real_data_usage() -> Dict[str, Any]:
    """Verify that real data (not synthetic) is used throughout."""
    print("\nüîç REAL DATA USAGE VERIFICATION:")
    print("-" * 50)
    
    # Load real data source
    config = pd.read_csv('data/data/api_data/store_config_data.csv')
    fixed = pd.read_csv('fast_fish_with_complete_style_attributes_20250715_105659.csv')
    
    real_data_verification = {}
    
    # Verify data source
    print(f"üìä Real data source: data/data/api_data/store_config_data.csv")
    print(f"   Records in source: {len(config):,}")
    print(f"   Unique categories: {config['big_class_name'].nunique()}")
    print(f"   Unique subcategories: {config['sub_cate_name'].nunique()}")
    
    # Verify attribute distributions
    season_dist = dict(config['season_name'].value_counts())
    gender_dist = dict(config['sex_name'].value_counts())
    location_dist = dict(config['display_location_name'].value_counts())
    
    real_data_verification = {
        'source_records': len(config),
        'unique_categories': int(config['big_class_name'].nunique()),
        'unique_subcategories': int(config['sub_cate_name'].nunique()),
        'season_distribution': season_dist,
        'gender_distribution': gender_dist,
        'location_distribution': location_dist
    }
    
    print(f"   Season distribution: {season_dist}")
    print(f"   Gender distribution: {gender_dist}")
    print(f"   Location distribution: {location_dist}")
    
    # Verify style tag enhancement
    sample_tags = fixed['Target_Style_Tags'].head(5).tolist()
    print(f"\n‚úÖ ENHANCED STYLE TAGS (Real Data Based):")
    for i, tag in enumerate(sample_tags, 1):
        print(f"   {i}. {tag}")
    
    # Check for completeness
    complete_tags = 0
    for tag in fixed['Target_Style_Tags']:
        if all(attr in str(tag) for attr in ['[', ',', ']']) and len(str(tag).split(',')) >= 4:
            complete_tags += 1
    
    completeness_rate = complete_tags / len(fixed) * 100
    print(f"\nüìä Style tag completeness: {completeness_rate:.1f}% ({complete_tags:,}/{len(fixed):,})")
    
    real_data_verification['completeness_rate'] = completeness_rate
    real_data_verification['complete_tags'] = complete_tags
    
    return real_data_verification

def verify_business_logic_preservation() -> Dict[str, Any]:
    """Verify that all business logic and calculations are preserved."""
    print("\nüßÆ BUSINESS LOGIC PRESERVATION VERIFICATION:")
    print("-" * 50)
    
    original = pd.read_csv('fast_fish_with_sell_through_analysis_20250714_124522.csv')
    fixed = pd.read_csv('fast_fish_with_complete_style_attributes_20250715_105659.csv')
    
    business_verification = {}
    
    # Verify key business columns
    business_columns = [
        'Data_Based_Rationale', 'Expected_Benefit', 'Stores_In_Group_Selling_This_Category',
        'cluster_trend_score', 'trend_sales_performance', 'Historical_Avg_Daily_SPUs_Sold_Per_Store'
    ]
    
    for col in business_columns:
        if col in original.columns and col in fixed.columns:
            # Check if data is preserved (for numeric columns)
            if original[col].dtype in ['float64', 'int64']:
                orig_sum = original[col].sum()
                fixed_sum = fixed[col].sum()
                preserved = abs(orig_sum - fixed_sum) < 0.01
            else:
                # For text columns, check if content is preserved
                preserved = (original[col].fillna('') == fixed[col].fillna('')).all()
            
            business_verification[col] = preserved
            status = '‚úÖ' if preserved else '‚ùå'
            print(f"   {col}: {status}")
    
    # Verify calculations
    print(f"\nüî¢ CALCULATION VERIFICATION:")
    
    # Check SPU quantity changes
    orig_changes = (original['Target_SPU_Quantity'] - original['Current_SPU_Quantity']).sum()
    fixed_changes = (fixed['Target_SPU_Quantity'] - fixed['Current_SPU_Quantity']).sum()
    changes_preserved = orig_changes == fixed_changes
    
    print(f"   SPU quantity changes: {'‚úÖ' if changes_preserved else '‚ùå'} ({orig_changes} ‚Üí {fixed_changes})")
    
    business_verification['spu_changes_preserved'] = changes_preserved
    business_verification['original_spu_changes'] = int(orig_changes)
    business_verification['fixed_spu_changes'] = int(fixed_changes)
    
    return business_verification

def create_production_readiness_report() -> Dict[str, Any]:
    """Create comprehensive production readiness report."""
    print("\nüìã PRODUCTION READINESS ASSESSMENT:")
    print("-" * 50)
    
    # Run all verifications
    structure_results = verify_data_preservation()
    real_data_results = verify_real_data_usage()
    business_results = verify_business_logic_preservation()
    
    # Overall assessment
    all_checks_passed = (
        structure_results['structure']['records_preserved'] and
        structure_results['structure']['columns_preserved'] and
        structure_results['columns']['all_columns_preserved'] and
        all(v.get('sum_preserved', True) for v in structure_results.get('financial', {}).values()) and
        all(v.get('sum_preserved', True) for v in structure_results.get('quantities', {}).values()) and
        real_data_results['completeness_rate'] > 95 and
        all(business_results.values())
    )
    
    production_report = {
        'timestamp': datetime.now().isoformat(),
        'overall_status': 'READY' if all_checks_passed else 'NEEDS_REVIEW',
        'structure_verification': structure_results,
        'real_data_verification': real_data_results,
        'business_logic_verification': business_results,
        'recommendations': []
    }
    
    if all_checks_passed:
        print("‚úÖ ALL VERIFICATIONS PASSED - PRODUCTION READY")
        production_report['recommendations'].append("Fixed CSV is ready for production use")
        production_report['recommendations'].append("All original data preserved with enhanced style attributes")
        production_report['recommendations'].append("Real data used throughout - no synthetic data")
    else:
        print("‚ö†Ô∏è  SOME VERIFICATIONS FAILED - REVIEW NEEDED")
        production_report['recommendations'].append("Review failed verifications before production use")
    
    return production_report

def main():
    """Main verification execution."""
    print("üöÄ COMPREHENSIVE VERIFICATION OF FIXED CSV OUTPUT")
    print("=" * 80)
    print("Verifying that ALL data is preserved and enhanced with REAL attributes")
    print("=" * 80)
    
    try:
        # Run comprehensive verification
        production_report = create_production_readiness_report()
        
        # Save verification report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"comprehensive_verification_report_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            json.dump(production_report, f, indent=2, default=str)
        
        print(f"\nüìÅ Verification report saved: {report_file}")
        print("\n" + "=" * 80)
        print("üéØ VERIFICATION COMPLETE")
        print("=" * 80)
        
        if production_report['overall_status'] == 'READY':
            print("‚úÖ FIXED CSV IS PRODUCTION READY")
            print("‚úÖ ALL ORIGINAL DATA PRESERVED")
            print("‚úÖ REAL ATTRIBUTES ADDED")
            print("‚úÖ NO SYNTHETIC DATA USED")
            print("\nüìÑ File: fast_fish_with_complete_style_attributes_20250715_105659.csv")
        else:
            print("‚ö†Ô∏è  REVIEW NEEDED BEFORE PRODUCTION")
        
        return production_report
        
    except Exception as e:
        print(f"‚ùå Error during verification: {e}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main() 