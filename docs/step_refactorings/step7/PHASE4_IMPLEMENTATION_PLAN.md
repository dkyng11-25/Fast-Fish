# Phase 4: Test Implementation - Execution Plan

**Date:** 2025-11-03  
**Status:** ğŸ“‹ READY TO EXECUTE  
**Total Scenarios:** 34 test scenarios  
**Estimated Time:** 8-12 hours

---

## ğŸ¯ Executive Summary

Phase 4 converts the test scaffold created in Phase 2 into functional tests with real assertions and mock data. The scaffold currently has 34 scenarios that all use `pytest.fail()` placeholders.

**Goal:** Convert all 34 scenarios to functional tests that validate the Phase 3 implementation.

**Key Principle:** Tests use **real mock data** (realistic structures), not synthetic assumptions.

---

## ğŸ“‹ Current State

### âœ… What We Have (Phase 2 Deliverables)

1. **Feature File:** `tests/features/step-7-missing-category-rule.feature`
   - 34 Gherkin scenarios (Given-When-Then)
   - Covers SETUP, APPLY, VALIDATE, PERSIST phases
   - Includes happy path, error cases, edge cases

2. **Test Scaffold:** `tests/step_definitions/test_step7_missing_category_rule.py`
   - 515 lines of test infrastructure
   - Mock fixtures for all repositories
   - Step definitions with `pytest.fail()` placeholders
   - Realistic mock data structures

3. **Phase 3 Implementation:** All components ready
   - 14 files, 2,821 LOC
   - All components compile and are ready to test

---

## ğŸ¯ Phase 4 Objectives

### Primary Goals

1. **Convert Scaffold to Functional Tests**
   - Replace all `pytest.fail()` with real assertions
   - Implement proper mock data for each scenario
   - Verify Phase 3 implementation works correctly

2. **Achieve 100% Test Pass Rate**
   - All 34 scenarios should pass
   - No skipped or xfail tests
   - Binary outcomes only (PASS/FAIL)

3. **Validate Business Logic**
   - Cluster analysis works correctly
   - Opportunity identification is accurate
   - Sell-through validation functions
   - ROI calculations are correct
   - Aggregation produces expected results

---

## ğŸ“Š Test Scenario Breakdown

### By Phase (34 scenarios total)

**SETUP Phase (5 scenarios):**
1. Load clustering data successfully
2. Load sales data successfully
3. Handle missing clustering data
4. Handle missing sales data
5. Blend seasonal sales data

**APPLY Phase (18 scenarios):**
6. Identify well-selling features
7. Handle no well-selling features
8. Identify missing opportunities
9. Handle no opportunities
10. Calculate expected sales (robust median)
11. Resolve unit prices (4-level fallback)
12. Handle missing unit prices
13. Calculate quantities correctly
14. Validate with sell-through predictions
15. Handle Fast Fish validation failure
16. Calculate ROI metrics
17. Handle missing margin rates
18. Filter by ROI thresholds
19. Aggregate to store level
20. Handle empty aggregation
21. Add store metadata
22. Calculate aggregation summary
23. Handle seasonal blending

**VALIDATE Phase (6 scenarios):**
24. Validate required columns present
25. Validate no negative quantities
26. Handle missing required columns
27. Handle negative quantities
28. Validate opportunities structure
29. Allow empty results (valid case)

**PERSIST Phase (5 scenarios):**
30. Save store results successfully
31. Save opportunity details
32. Generate summary report
33. Handle empty results in persist
34. Log summary statistics

---

## ğŸ”§ Implementation Strategy

### Methodical Approach

**DO NOT attempt all 34 scenarios at once!**

Instead, follow this incremental approach:

1. **Start with SETUP scenarios (5 tests)**
   - Simplest to implement
   - Foundation for other tests
   - Estimated: 1-2 hours

2. **Move to APPLY scenarios (18 tests)**
   - Core business logic
   - Most complex
   - Estimated: 4-6 hours

3. **Then VALIDATE scenarios (6 tests)**
   - Data quality checks
   - Error handling
   - Estimated: 1-2 hours

4. **Finally PERSIST scenarios (5 tests)**
   - Output generation
   - Reporting
   - Estimated: 1-2 hours

5. **Debug and refine (1-2 hours)**
   - Fix any failing tests
   - Verify all pass
   - Document findings

---

## ğŸ“ Conversion Template

### Current Scaffold Pattern

```python
@when("the step identifies well-selling features")
def step_identify_well_selling(test_context, step_instance):
    """Identify well-selling features per cluster."""
    # TODO: Implement actual test logic
    pytest.fail("Test not implemented yet - Phase 3 pending")
```

### Converted Functional Test Pattern

```python
@when("the step identifies well-selling features")
def step_identify_well_selling(test_context, step_instance):
    """Identify well-selling features per cluster."""
    # Execute the step
    context = StepContext()
    context.data['cluster_df'] = test_context['cluster_df']
    context.data['sales_df'] = test_context['sales_df']
    
    # Call the component directly
    well_selling = step_instance.cluster_analyzer.identify_well_selling_features(
        sales_df=context.data['sales_df'],
        cluster_df=context.data['cluster_df']
    )
    
    # Store result for assertions
    test_context['well_selling_features'] = well_selling
```

### Then Assertion Pattern

```python
@then("well-selling features are identified for each cluster")
def verify_well_selling_features(test_context):
    """Verify well-selling features were identified."""
    well_selling = test_context['well_selling_features']
    
    # Assert structure
    assert isinstance(well_selling, pd.DataFrame)
    assert len(well_selling) > 0, "Should identify some well-selling features"
    
    # Assert required columns
    required_cols = ['cluster_id', 'sub_cate_name', 'stores_selling', 
                     'total_cluster_sales', 'pct_stores_selling']
    for col in required_cols:
        assert col in well_selling.columns, f"Missing column: {col}"
    
    # Assert business logic
    assert (well_selling['pct_stores_selling'] >= 0.70).all(), \
        "All features should meet 70% threshold"
    assert (well_selling['total_cluster_sales'] >= 100.0).all(), \
        "All features should meet $100 sales threshold"
```

---

## ğŸ” Key Testing Patterns

### Pattern 1: Component Testing

**Test individual components directly:**

```python
def test_cluster_analyzer():
    """Test ClusterAnalyzer component."""
    analyzer = ClusterAnalyzer(config, logger)
    
    result = analyzer.identify_well_selling_features(
        sales_df=mock_sales,
        cluster_df=mock_clusters
    )
    
    # Assertions
    assert len(result) > 0
    assert 'cluster_id' in result.columns
```

### Pattern 2: Phase Testing

**Test complete phase execution:**

```python
def test_setup_phase():
    """Test SETUP phase loads all data."""
    context = StepContext()
    
    result_context = step_instance.setup(context)
    
    # Assertions
    assert 'cluster_df' in result_context.data
    assert 'sales_df' in result_context.data
    assert len(result_context.data['cluster_df']) > 0
```

### Pattern 3: End-to-End Testing

**Test complete step execution:**

```python
def test_complete_execution():
    """Test complete step execution."""
    context = StepContext()
    
    result_context = step_instance.execute(context)
    
    # Assertions
    assert 'results' in result_context.data
    assert 'opportunities' in result_context.data
```

### Pattern 4: Error Testing

**Test error handling:**

```python
def test_missing_data_error():
    """Test error when clustering data missing."""
    context = StepContext()
    # Don't load cluster data
    
    with pytest.raises(FileNotFoundError) as exc_info:
        step_instance.setup(context)
    
    assert "clustering" in str(exc_info.value).lower()
```

---

## ğŸ“¦ Mock Data Guidelines

### Realistic Mock Data

**DO:**
- âœ… Use realistic data structures
- âœ… Include edge cases (empty, nulls, extremes)
- âœ… Match actual data distributions
- âœ… Use real column names

**DON'T:**
- âŒ Use overly simplistic data
- âŒ Make assumptions about distributions
- âŒ Use magic numbers without context
- âŒ Create data that can't exist in reality

### Example: Realistic Clustering Data

```python
@pytest.fixture
def realistic_cluster_data():
    """Realistic clustering data matching production structure."""
    return pd.DataFrame({
        'str_code': [f'{i:04d}' for i in range(1, 101)],  # 100 stores
        'cluster_id': [1]*40 + [2]*35 + [3]*25,  # Uneven distribution
        'cluster_name': ['High_Volume']*40 + ['Medium_Volume']*35 + ['Low_Volume']*25,
        # Additional metadata that might exist
        'region': ['North']*40 + ['South']*35 + ['East']*25,
    })
```

### Example: Realistic Sales Data

```python
@pytest.fixture
def realistic_sales_data():
    """Realistic sales data with varying adoption."""
    np.random.seed(42)  # Reproducible
    
    stores = [f'{i:04d}' for i in range(1, 101)]
    categories = ['ç›´ç­’è£¤', 'é”¥å½¢è£¤', 'å–‡å­è£¤', 'é˜”è…¿è£¤', 'ç´§èº«è£¤']
    
    # Create realistic sales patterns
    data = []
    for store in stores:
        # Each store sells 2-4 categories (realistic)
        num_categories = np.random.randint(2, 5)
        store_categories = np.random.choice(categories, num_categories, replace=False)
        
        for category in store_categories:
            data.append({
                'str_code': store,
                'sub_cate_name': category,
                'sal_amt': np.random.lognormal(5, 1),  # Realistic sales distribution
                'total_qty': np.random.randint(10, 200)
            })
    
    return pd.DataFrame(data)
```

---

## ğŸ¯ Session-by-Session Plan

### Session 1: SETUP Phase Tests (1-2 hours)

**Goal:** Convert 5 SETUP scenarios

**Tasks:**
1. Review current SETUP fixtures
2. Implement data loading assertions
3. Test seasonal blending logic
4. Verify error handling for missing data
5. Run and debug SETUP tests

**Expected Outcome:** 5/5 SETUP tests passing

---

### Session 2: APPLY Phase - Part 1 (2-3 hours)

**Goal:** Convert cluster analysis and opportunity identification (9 scenarios)

**Tasks:**
1. Test well-selling feature identification
2. Test missing opportunity detection
3. Test price resolution fallback chain
4. Test quantity calculations
5. Handle edge cases (no opportunities, etc.)

**Expected Outcome:** 9/18 APPLY tests passing

---

### Session 3: APPLY Phase - Part 2 (2-3 hours)

**Goal:** Convert validation and aggregation (9 scenarios)

**Tasks:**
1. Test sell-through validation
2. Test ROI calculations
3. Test store-level aggregation
4. Test metadata enrichment
5. Handle empty results

**Expected Outcome:** 18/18 APPLY tests passing

---

### Session 4: VALIDATE Phase Tests (1-2 hours)

**Goal:** Convert 6 VALIDATE scenarios

**Tasks:**
1. Test required column validation
2. Test negative quantity detection
3. Test error raising
4. Test empty results handling
5. Verify VALIDATE returns None

**Expected Outcome:** 6/6 VALIDATE tests passing

---

### Session 5: PERSIST Phase Tests (1-2 hours)

**Goal:** Convert 5 PERSIST scenarios

**Tasks:**
1. Test file saving (mock output repo)
2. Test report generation
3. Test empty results handling
4. Test summary statistics logging
5. Verify output structure

**Expected Outcome:** 5/5 PERSIST tests passing

---

### Session 6: Debug & Refinement (1-2 hours)

**Goal:** Achieve 100% pass rate

**Tasks:**
1. Run all 34 tests
2. Debug any failures
3. Fix mock data issues
4. Verify business logic
5. Document findings

**Expected Outcome:** 34/34 tests passing âœ…

---

## ğŸ”§ Tools & Commands

### Running Tests

```bash
# Run all Step 7 tests
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py -v

# Run specific scenario
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py -k "well_selling" -v

# Run with verbose output
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py -v -s

# Run with coverage
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py --cov=src/steps --cov=src/components/missing_category -v
```

### Debugging Tests

```bash
# Run single test with print statements
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py::test_scenario_name -v -s

# Drop into debugger on failure
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py --pdb

# Show local variables on failure
uv run pytest tests/step_definitions/test_step7_missing_category_rule.py -l
```

---

## âš ï¸ Common Pitfalls & Solutions

### Pitfall 1: Mock Data Too Simple

**Problem:** Mock data doesn't reflect real complexity
```python
# Too simple
sales_data = pd.DataFrame({'str_code': ['0001'], 'sal_amt': [100]})
```

**Solution:** Use realistic distributions
```python
# Realistic
sales_data = pd.DataFrame({
    'str_code': [f'{i:04d}' for i in range(1, 51)],
    'sub_cate_name': ['ç›´ç­’è£¤', 'é”¥å½¢è£¤'] * 25,
    'sal_amt': np.random.lognormal(5, 1, 50)  # Realistic distribution
})
```

---

### Pitfall 2: Testing Implementation, Not Behavior

**Problem:** Tests check internal implementation details
```python
# Bad - tests implementation
assert step_instance.cluster_analyzer.config.min_cluster_stores_selling == 0.70
```

**Solution:** Test observable behavior
```python
# Good - tests behavior
result = step_instance.cluster_analyzer.identify_well_selling_features(...)
assert (result['pct_stores_selling'] >= 0.70).all()
```

---

### Pitfall 3: Non-Binary Test Outcomes

**Problem:** Tests have conditional logic
```python
# Bad - conditional outcome
if len(result) > 0:
    assert result['cluster_id'].notna().all()
else:
    pass  # Silent pass
```

**Solution:** Clear assertions
```python
# Good - binary outcome
assert len(result) > 0, "Should identify opportunities"
assert result['cluster_id'].notna().all(), "All cluster IDs should be present"
```

---

### Pitfall 4: Ignoring Edge Cases

**Problem:** Only testing happy path
```python
# Only tests when data exists
def test_opportunities():
    result = identify_opportunities(well_selling, clusters)
    assert len(result) > 0
```

**Solution:** Test edge cases explicitly
```python
# Tests empty case
def test_no_opportunities():
    empty_well_selling = pd.DataFrame()
    result = identify_opportunities(empty_well_selling, clusters)
    assert len(result) == 0, "Should handle empty input gracefully"
```

---

## ğŸ“Š Progress Tracking

### Test Conversion Checklist

**SETUP Phase (5 scenarios):**
- [ ] Load clustering data successfully
- [ ] Load sales data successfully
- [ ] Handle missing clustering data
- [ ] Handle missing sales data
- [ ] Blend seasonal sales data

**APPLY Phase - Part 1 (9 scenarios):**
- [ ] Identify well-selling features
- [ ] Handle no well-selling features
- [ ] Identify missing opportunities
- [ ] Handle no opportunities
- [ ] Calculate expected sales
- [ ] Resolve unit prices (4-level fallback)
- [ ] Handle missing unit prices
- [ ] Calculate quantities correctly
- [ ] Handle seasonal blending

**APPLY Phase - Part 2 (9 scenarios):**
- [ ] Validate with sell-through predictions
- [ ] Handle Fast Fish validation failure
- [ ] Calculate ROI metrics
- [ ] Handle missing margin rates
- [ ] Filter by ROI thresholds
- [ ] Aggregate to store level
- [ ] Handle empty aggregation
- [ ] Add store metadata
- [ ] Calculate aggregation summary

**VALIDATE Phase (6 scenarios):**
- [ ] Validate required columns present
- [ ] Validate no negative quantities
- [ ] Handle missing required columns
- [ ] Handle negative quantities
- [ ] Validate opportunities structure
- [ ] Allow empty results

**PERSIST Phase (5 scenarios):**
- [ ] Save store results successfully
- [ ] Save opportunity details
- [ ] Generate summary report
- [ ] Handle empty results in persist
- [ ] Log summary statistics

---

## âœ… Success Criteria

### Phase 4 is complete when:

1. âœ… All 34 test scenarios converted from scaffold
2. âœ… All 34 tests pass (100% pass rate)
3. âœ… No skipped or xfail tests
4. âœ… All assertions are meaningful and specific
5. âœ… Mock data is realistic and comprehensive
6. âœ… Edge cases are covered
7. âœ… Error handling is validated
8. âœ… Business logic is verified
9. âœ… Test code â‰¤ 500 LOC (currently 515 LOC - may need slight refactor)
10. âœ… Documentation updated (PHASE4_COMPLETE.md)

---

## ğŸ“ Documentation Requirements

### During Phase 4

**Create/Update:**
1. Test conversion notes (track issues found)
2. Mock data documentation (explain realistic patterns)
3. Edge case catalog (document discovered edge cases)

### After Phase 4

**Create:**
1. `PHASE4_COMPLETE.md` - Summary of test implementation
2. `PHASE4_TEST_RESULTS.md` - Test execution results
3. Update `README.md` - Add testing instructions

---

## ğŸ¯ Next Steps After Phase 4

Once all tests pass:

1. **Run full test suite** to ensure no regressions
2. **Generate test coverage report**
3. **Document any discovered bugs** in Phase 3 implementation
4. **Create PHASE4_COMPLETE.md** summary
5. **Prepare for production deployment**

---

**Phase 4 Implementation Plan Complete**

**Status:** ğŸ“‹ READY TO EXECUTE  
**Estimated Time:** 8-12 hours across 6 sessions  
**Expected Outcome:** 34/34 tests passing âœ…  
**Date:** 2025-11-03 10:52 AM UTC+08:00
