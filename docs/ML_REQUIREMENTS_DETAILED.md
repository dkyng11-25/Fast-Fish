# ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ë³´ì™„ í•„ìš” ì˜ì—­ ìƒì„¸ ë¶„ì„

**ì‘ì„±ì¼:** 2025-01-06  
**ëŒ€ìƒ:** ML ì§€ì‹ ë³´ìœ  ë°ì´í„° ë‹´ë‹¹ì  
**ëª©ì :** í”„ë¡œì íŠ¸ ë‚´ ML ê´€ë ¨ ê°œì„  í•„ìš” ì˜ì—­ ì‹ë³„ ë° ê°œë°œ ê°€ì´ë“œ

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ML êµ¬í˜„ í˜„í™©](#1-í˜„ì¬-ml-êµ¬í˜„-í˜„í™©)
2. [ê°œì„  í•„ìš” ì˜ì—­ (ë‚œì´ë„ë³„)](#2-ê°œì„ -í•„ìš”-ì˜ì—­)
3. [ë‹¹ì‹ ì´ ê°œë°œ ê°€ëŠ¥í•œ ML í•­ëª©](#3-ë‹¹ì‹ ì´-ê°œë°œ-ê°€ëŠ¥í•œ-ml-í•­ëª©)
4. [êµ¬í˜„ ê°€ì´ë“œ ë° ì½”ë“œ ì˜ˆì‹œ](#4-êµ¬í˜„-ê°€ì´ë“œ)
5. [í•™ìŠµ ë¦¬ì†ŒìŠ¤](#5-í•™ìŠµ-ë¦¬ì†ŒìŠ¤)

---

## 1. í˜„ì¬ ML êµ¬í˜„ í˜„í™©

### 1.1 ì´ë¯¸ êµ¬í˜„ëœ ML ì»´í¬ë„ŒíŠ¸

| ì»´í¬ë„ŒíŠ¸ | ìœ„ì¹˜ | ì•Œê³ ë¦¬ì¦˜ | ìƒíƒœ |
|----------|------|----------|------|
| **PCA ì°¨ì› ì¶•ì†Œ** | `step6_cluster_analysis.py` | sklearn.PCA | âœ… ì™„ë£Œ |
| **K-Means í´ëŸ¬ìŠ¤í„°ë§** | `step6_cluster_analysis.py` | sklearn.KMeans | âœ… ì™„ë£Œ |
| **í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë©”íŠ¸ë¦­** | `step6_cluster_analysis.py` | Silhouette, Calinski-Harabasz, Davies-Bouldin | âœ… ì™„ë£Œ |
| **ìƒí’ˆ ì—­í•  ë¶„ë¥˜** | `step25_product_role_classifier.py` | Rule-based (CORE/SEASONAL/FILLER/CLEARANCE) | âš ï¸ ê·œì¹™ ê¸°ë°˜ |
| **ìµœì í™” ì—”ì§„** | `step30_sellthrough_optimization_engine.py` | scipy.linprog, PuLP | âš ï¸ ê¸°ì´ˆ êµ¬í˜„ |

### 1.2 ì‚¬ìš© ì¤‘ì¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# í˜„ì¬ ì‚¬ìš© ì¤‘
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.optimize import linprog
from pulp import *

# ë¯¸ì‚¬ìš© (í–¥í›„ í•„ìš”)
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
# from sklearn.model_selection import train_test_split, cross_val_score
# import xgboost as xgb
# import lightgbm as lgb
```

---

## 2. ê°œì„  í•„ìš” ì˜ì—­

### ğŸŸ¢ Level 1: ì¦‰ì‹œ ê°œë°œ ê°€ëŠ¥ (ë‹¹ì‹ ì˜ í˜„ì¬ ì—­ëŸ‰)

#### 2.1 í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ê°œì„  (Silhouette â‰¥ 0.5)

**í˜„ì¬ ë¬¸ì œ:**
- í˜„ì¬ Silhouette Score < 0.5 (ê³ ê° ìš”êµ¬: â‰¥ 0.5)
- í”¼ì²˜ ì„ íƒ ìµœì í™” í•„ìš”

**í•„ìš” ML ì§€ì‹:**
- Feature Selection (í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„)
- Hyperparameter Tuning (n_clusters, n_components)
- Elbow Method, Silhouette Analysis

**êµ¬í˜„ ìœ„ì¹˜:** `src/steps/cluster_analysis_step.py`

**ê°œì„  ë°©ë²•:**
```python
# 1. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ (Elbow + Silhouette)
from sklearn.metrics import silhouette_score

def find_optimal_clusters(pca_df, k_range=(20, 60)):
    scores = []
    for k in range(k_range[0], k_range[1], 5):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(pca_df)
        score = silhouette_score(pca_df, labels)
        scores.append((k, score))
    return max(scores, key=lambda x: x[1])

# 2. PCA ì»´í¬ë„ŒíŠ¸ ìˆ˜ ìµœì í™”
def optimize_pca_components(normalized_df, variance_threshold=0.95):
    pca = PCA(n_components=None)
    pca.fit(normalized_df)
    cumsum = np.cumsum(pca.explained_variance_ratio_)
    n_components = np.argmax(cumsum >= variance_threshold) + 1
    return n_components
```

**ì˜ˆìƒ ë‚œì´ë„:** â­â­ (ì¤‘ê°„)  
**ì˜ˆìƒ ì‹œê°„:** 3-5ì¼

---

#### 2.2 í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„± ë¶„ì„ (Temporal Stability)

**í˜„ì¬ ë¬¸ì œ:**
- ì‹œì¦Œë³„ í´ëŸ¬ìŠ¤í„° ë©¤ë²„ì‹­ ë³€í™” ì¶”ì  ì—†ìŒ
- D-C Deliverable ë¯¸ì™„ë£Œ (2/10 ì ìˆ˜)

**í•„ìš” ML ì§€ì‹:**
- Jaccard Similarity
- Cluster Membership Tracking
- Time Series Clustering Comparison

**êµ¬í˜„ ìœ„ì¹˜:** ì‹ ê·œ `src/steps/cluster_stability_step.py`

**ê°œì„  ë°©ë²•:**
```python
from sklearn.metrics import jaccard_score
import numpy as np

def calculate_cluster_stability(labels_t1, labels_t2, store_ids):
    """
    ë‘ ì‹œì ì˜ í´ëŸ¬ìŠ¤í„° ë©¤ë²„ì‹­ ì•ˆì •ì„± ê³„ì‚°
    
    Returns:
        stability_score: 0-1 ì‚¬ì´ ê°’ (1 = ì™„ì „ ì•ˆì •)
    """
    # ê°™ì€ ë§¤ì¥ë“¤ë§Œ ë¹„êµ
    common_stores = set(store_ids)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ Jaccard ìœ ì‚¬ë„
    cluster_stabilities = []
    for cluster_id in np.unique(labels_t1):
        stores_t1 = set(store_ids[labels_t1 == cluster_id])
        
        # t2ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        best_jaccard = 0
        for cluster_id_t2 in np.unique(labels_t2):
            stores_t2 = set(store_ids[labels_t2 == cluster_id_t2])
            jaccard = len(stores_t1 & stores_t2) / len(stores_t1 | stores_t2)
            best_jaccard = max(best_jaccard, jaccard)
        
        cluster_stabilities.append(best_jaccard)
    
    return np.mean(cluster_stabilities)

def flag_unstable_clusters(stability_scores, threshold=0.7):
    """ì•ˆì •ì„± ì„ê³„ê°’ ë¯¸ë‹¬ í´ëŸ¬ìŠ¤í„° í”Œë˜ê·¸"""
    return [i for i, score in enumerate(stability_scores) if score < threshold]
```

**ì˜ˆìƒ ë‚œì´ë„:** â­â­ (ì¤‘ê°„)  
**ì˜ˆìƒ ì‹œê°„:** 2-3ì¼

---

#### 2.3 ë§¤ì¥ ìœ í˜• ë¶„ë¥˜ (Store Type Classification)

**í˜„ì¬ ë¬¸ì œ:**
- Fashion/Basic/Balanced ë¶„ë¥˜ ì—†ìŒ
- ê³ ê° ìš”êµ¬ì‚¬í•­ C-03 ë¯¸ì™„ë£Œ

**í•„ìš” ML ì§€ì‹:**
- Feature Engineering (íŒë§¤ ë¹„ìœ¨ ê³„ì‚°)
- Threshold-based Classification
- (ì„ íƒ) K-Means for Store Segmentation

**êµ¬í˜„ ìœ„ì¹˜:** `src/steps/` ì‹ ê·œ ë˜ëŠ” Step 1-3 í™•ì¥

**ê°œì„  ë°©ë²•:**
```python
def classify_store_type(store_sales_df):
    """
    ë§¤ì¥ ìœ í˜• ë¶„ë¥˜: Fashion / Basic / Balanced
    
    ê¸°ì¤€:
    - Fashion: íŒ¨ì…˜ ìƒí’ˆ ë¹„ìœ¨ > 60%
    - Basic: ê¸°ë³¸ ìƒí’ˆ ë¹„ìœ¨ > 60%
    - Balanced: ê·¸ ì™¸
    """
    # íŒ¨ì…˜ vs ê¸°ë³¸ ë¹„ìœ¨ ê³„ì‚°
    fashion_ratio = store_sales_df.groupby('store_code').apply(
        lambda x: (x['style'] == 'fashion').sum() / len(x)
    )
    
    # ë¶„ë¥˜
    store_types = pd.Series(index=fashion_ratio.index, dtype=str)
    store_types[fashion_ratio > 0.6] = 'FASHION'
    store_types[fashion_ratio < 0.4] = 'BASIC'
    store_types[(fashion_ratio >= 0.4) & (fashion_ratio <= 0.6)] = 'BALANCED'
    
    return store_types
```

**ì˜ˆìƒ ë‚œì´ë„:** â­ (ì‰¬ì›€)  
**ì˜ˆìƒ ì‹œê°„:** 1-2ì¼

---

### ğŸŸ¡ Level 2: í•™ìŠµ í›„ ê°œë°œ ê°€ëŠ¥ (ì•½ê°„ì˜ ì¶”ê°€ í•™ìŠµ í•„ìš”)

#### 2.4 ìƒí’ˆ ì—­í•  ë¶„ë¥˜ ML ë²„ì „

**í˜„ì¬ ë¬¸ì œ:**
- `step25_product_role_classifier.py`ê°€ ê·œì¹™ ê¸°ë°˜
- ML ê¸°ë°˜ ë¶„ë¥˜ë¡œ ì •í™•ë„ í–¥ìƒ ê°€ëŠ¥

**í•„ìš” ML ì§€ì‹:**
- Classification (Random Forest, XGBoost)
- Feature Engineering
- Cross-Validation
- Class Imbalance Handling

**ê°œì„  ë°©ë²•:**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder

def train_product_role_classifier(sales_df, labeled_products):
    """
    ML ê¸°ë°˜ ìƒí’ˆ ì—­í•  ë¶„ë¥˜ê¸° í•™ìŠµ
    
    Features:
    - total_sales: ì´ íŒë§¤ì•¡
    - sales_velocity: íŒë§¤ ì†ë„
    - store_coverage: íŒë§¤ ë§¤ì¥ ë¹„ìœ¨
    - seasonal_variance: ê³„ì ˆë³„ íŒë§¤ ë³€ë™
    - price_band: ê°€ê²©ëŒ€
    """
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    features = pd.DataFrame({
        'total_sales': sales_df.groupby('spu_code')['sales'].sum(),
        'avg_daily_sales': sales_df.groupby('spu_code')['sales'].mean(),
        'store_count': sales_df.groupby('spu_code')['store_code'].nunique(),
        'sales_std': sales_df.groupby('spu_code')['sales'].std(),
    })
    
    # ë¼ë²¨ ì¸ì½”ë”©
    le = LabelEncoder()
    y = le.fit_transform(labeled_products['role'])
    
    # ëª¨ë¸ í•™ìŠµ
    clf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        random_state=42
    )
    
    # êµì°¨ ê²€ì¦
    scores = cross_val_score(clf, features, y, cv=5, scoring='f1_weighted')
    print(f"Cross-validation F1 Score: {scores.mean():.3f} (+/- {scores.std():.3f})")
    
    clf.fit(features, y)
    return clf, le
```

**ì˜ˆìƒ ë‚œì´ë„:** â­â­â­ (ì¤‘ìƒ)  
**ì˜ˆìƒ ì‹œê°„:** 5-7ì¼

---

#### 2.5 í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§ ìë™í™”

**í˜„ì¬ ë¬¸ì œ:**
- í´ëŸ¬ìŠ¤í„° ì„¤ëª…ì´ ê¸°ìˆ ì  (ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´ ë¶€ì¡±)
- D-B Deliverable ê°œì„  í•„ìš”

**í•„ìš” ML ì§€ì‹:**
- Cluster Interpretation
- Feature Importance per Cluster
- Statistical Profiling

**ê°œì„  ë°©ë²•:**
```python
def generate_cluster_profile(cluster_id, cluster_data, original_features):
    """
    í´ëŸ¬ìŠ¤í„°ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œíŒŒì¼ ìë™ ìƒì„±
    """
    profile = {}
    
    # 1. ì£¼ìš” íŠ¹ì„± (í‰ê·  ëŒ€ë¹„ ì°¨ì´)
    cluster_mean = cluster_data.mean()
    global_mean = original_features.mean()
    
    diff = (cluster_mean - global_mean) / global_mean
    
    # ìƒìœ„ 5ê°œ íŠ¹ì„±
    top_features = diff.nlargest(5)
    bottom_features = diff.nsmallest(5)
    
    profile['dominant_features'] = top_features.to_dict()
    profile['weak_features'] = bottom_features.to_dict()
    
    # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ë¼ë²¨ ìƒì„±
    if cluster_data['temperature'].mean() > 25:
        profile['climate'] = 'ê³ ì˜¨ ì§€ì—­'
    elif cluster_data['temperature'].mean() < 10:
        profile['climate'] = 'ì €ì˜¨ ì§€ì—­'
    else:
        profile['climate'] = 'ì˜¨ëŒ€ ì§€ì—­'
    
    # 3. ì¶”ì²œ ì „ëµ
    profile['strategy'] = generate_strategy_recommendation(profile)
    
    return profile
```

**ì˜ˆìƒ ë‚œì´ë„:** â­â­ (ì¤‘ê°„)  
**ì˜ˆìƒ ì‹œê°„:** 3-4ì¼

---

### ğŸ”´ Level 3: ì „ë¬¸ ì§€ì‹ í•„ìš” (í˜‘ì—… ë˜ëŠ” ì‹¬í™” í•™ìŠµ)

#### 2.6 ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ (Demand Forecasting)

**í˜„ì¬ ë¬¸ì œ:**
- ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì—†ìŒ
- ê³ ê° ìš”êµ¬ì‚¬í•­ C-13 ë¯¸ì™„ë£Œ

**í•„ìš” ML ì§€ì‹:**
- Time Series Analysis (ARIMA, Prophet, LSTM)
- Feature Engineering for Time Series
- Backtesting & Evaluation

**êµ¬í˜„ ë³µì¡ë„:** ë†’ìŒ  
**ê¶Œì¥:** ML Engineering ì „ë¬¸ê°€ í˜‘ì—…

```python
# ê°œë…ì  êµ¬ì¡° (ì‹¤ì œ êµ¬í˜„ì€ ë³µì¡)
from prophet import Prophet
import pandas as pd

def train_demand_forecast(sales_history):
    """
    Prophet ê¸°ë°˜ ìˆ˜ìš” ì˜ˆì¸¡
    """
    # Prophet í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df = sales_history.rename(columns={'date': 'ds', 'sales': 'y'})
    
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False
    )
    
    model.fit(df)
    
    # 14ì¼ ì˜ˆì¸¡
    future = model.make_future_dataframe(periods=14)
    forecast = model.predict(future)
    
    return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
```

**ì˜ˆìƒ ë‚œì´ë„:** â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)  
**ì˜ˆìƒ ì‹œê°„:** 2-4ì£¼

---

#### 2.7 MILP ìµœì í™” ì—”ì§„ ê³ ë„í™”

**í˜„ì¬ ë¬¸ì œ:**
- `step30_sellthrough_optimization_engine.py` ê¸°ì´ˆ êµ¬í˜„
- ì‹¤ì œ ì œì•½ ì¡°ê±´ ë°˜ì˜ ë¶€ì¡±

**í•„ìš” ì§€ì‹:**
- Operations Research
- Linear Programming / MILP
- Constraint Modeling

**í˜„ì¬ êµ¬í˜„:**
```python
# step30ì—ì„œ ì‚¬ìš© ì¤‘
OBJECTIVE_WEIGHTS = {
    'sell_through_rate': 0.70,
    'revenue_impact': 0.20,
    'inventory_turnover': 0.10
}

OPTIMIZATION_CONSTRAINTS = {
    'max_capacity_utilization': 0.85,
    'min_category_diversity': 3,
    'max_role_concentration': 0.60,
}
```

**ê¶Œì¥:** Operations Research ì „ë¬¸ê°€ í˜‘ì—…

---

## 3. ë‹¹ì‹ ì´ ê°œë°œ ê°€ëŠ¥í•œ ML í•­ëª©

### ìš°ì„ ìˆœìœ„ 1: ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥

| í•­ëª© | ê´€ë ¨ Step | í•„ìš” ì§€ì‹ | ì˜ˆìƒ ì‹œê°„ |
|------|-----------|-----------|-----------|
| **Silhouette â‰¥ 0.5 ë‹¬ì„±** | Step 4-6 | PCA, K-Means íŠœë‹ | 3-5ì¼ |
| **ë§¤ì¥ ìœ í˜• ë¶„ë¥˜** | Step 1-3 | Feature Engineering | 1-2ì¼ |
| **í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„± ë¶„ì„** | Step 6 ì´í›„ | Jaccard Similarity | 2-3ì¼ |

### ìš°ì„ ìˆœìœ„ 2: í•™ìŠµ í›„ ê°œë°œ

| í•­ëª© | ê´€ë ¨ Step | í•„ìš” í•™ìŠµ | ì˜ˆìƒ ì‹œê°„ |
|------|-----------|-----------|-----------|
| **ìƒí’ˆ ì—­í•  ML ë¶„ë¥˜** | Step 25 | Random Forest, XGBoost | 5-7ì¼ |
| **í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ìë™í™”** | Step 6 | Feature Importance | 3-4ì¼ |
| **ê°€ê²© íƒ„ë ¥ì„± ë¶„ì„** | Step 26 | Regression | 4-5ì¼ |

---

## 4. êµ¬í˜„ ê°€ì´ë“œ

### 4.1 ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
uv pip install scikit-learn xgboost lightgbm

# ì‹œê³„ì—´ ë¶„ì„ (ì„ íƒ)
uv pip install prophet statsmodels

# ìµœì í™” (ì´ë¯¸ ì„¤ì¹˜ë¨)
uv pip install scipy pulp
```

### 4.2 ì½”ë“œ êµ¬ì¡° ê¶Œì¥

```
src/
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ cluster_analysis_step.py      # ê¸°ì¡´ (ê°œì„ )
â”‚   â”œâ”€â”€ cluster_stability_step.py     # ì‹ ê·œ
â”‚   â””â”€â”€ store_type_classifier_step.py # ì‹ ê·œ
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ cluster_optimizer.py      # í´ëŸ¬ìŠ¤í„° ìµœì í™”
â”‚   â”‚   â”œâ”€â”€ product_role_classifier.py # ML ë¶„ë¥˜ê¸°
â”‚   â”‚   â””â”€â”€ stability_analyzer.py     # ì•ˆì •ì„± ë¶„ì„
â”‚   â””â”€â”€ ...
â””â”€â”€ utils/
    â””â”€â”€ ml_utils.py                   # ML ìœ í‹¸ë¦¬í‹°
```

### 4.3 í…ŒìŠ¤íŠ¸ ì „ëµ

```python
# tests/test_cluster_optimizer.py
import pytest
from src.components.ml.cluster_optimizer import find_optimal_clusters

def test_silhouette_improvement():
    """Silhouette scoreê°€ 0.5 ì´ìƒì¸ì§€ ê²€ì¦"""
    # Given: ì •ê·œí™”ëœ ë§¤íŠ¸ë¦­ìŠ¤
    normalized_df = load_test_matrix()
    
    # When: ìµœì í™” ìˆ˜í–‰
    optimal_k, silhouette = find_optimal_clusters(normalized_df)
    
    # Then: ëª©í‘œ ë‹¬ì„±
    assert silhouette >= 0.5, f"Silhouette {silhouette} < 0.5"
```

---

## 5. í•™ìŠµ ë¦¬ì†ŒìŠ¤

### 5.1 í´ëŸ¬ìŠ¤í„°ë§ ê°œì„ 

- **Scikit-learn Clustering Guide**: https://scikit-learn.org/stable/modules/clustering.html
- **Silhouette Analysis**: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
- **PCA Explained**: https://scikit-learn.org/stable/modules/decomposition.html#pca

### 5.2 ë¶„ë¥˜ ëª¨ë¸

- **Random Forest**: https://scikit-learn.org/stable/modules/ensemble.html#random-forests
- **XGBoost Tutorial**: https://xgboost.readthedocs.io/en/stable/tutorials/
- **Class Imbalance**: https://imbalanced-learn.org/stable/

### 5.3 ì‹œê³„ì—´ (ì‹¬í™”)

- **Prophet**: https://facebook.github.io/prophet/
- **Time Series with sklearn**: https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split

---

## 6. ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ

```
Week 1: ë§¤ì¥ ìœ í˜• ë¶„ë¥˜ (1-2ì¼) + í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„± ë¶„ì„ (2-3ì¼)
        â†“
Week 2: Silhouette ê°œì„  (3-5ì¼)
        â†“
Week 3: í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ìë™í™” (3-4ì¼)
        â†“
Week 4+: ìƒí’ˆ ì—­í•  ML ë¶„ë¥˜ (5-7ì¼)
```

---

**ë¬¸ì„œ ë²„ì „:** 1.0  
**ë‹¤ìŒ ê²€í† ì¼:** 2025-01-13
