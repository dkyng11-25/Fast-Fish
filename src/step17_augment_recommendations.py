#!/usr/bin/env python3
"""
Step 17: Augment Fast Fish Recommendations with Historical Reference AND Comprehensive Trending Analysis
=======================================================================================================

Take the existing Fast Fish recommendations file and add:
1. Historical reference columns (existing functionality)
2. Comprehensive 10-dimension trending analysis (ENHANCED - Store Group Aggregation)

Pipeline Flow:
Step 16 â†’ Step 17 â†’ Step 18

Input:  Step 14 Fast Fish output
Output: Enhanced file with historical + trending columns (30+ total columns)

HOW TO RUN (CLI + ENV)
----------------------
Overview
- Period-aware: this step expects the Step 14 output for the specific period label (e.g., `output/enhanced_fast_fish_format_202510A.csv`).
- Why this matters: Step 17 enriches the exact recommendation rows for that half-month. Using the wrong period causes misalignment (e.g., wrong Season/Temperature/Cluster joins) and later failures in Step 18.
- Trending is optional and guarded by real data availability (no synthetic). Enable only when the granular trend data is preserved and clustering/weather files exist.

Quick Start (target 202510A)
  Command (manifest-driven input resolution):
    PYTHONPATH=. python3 src/step17_augment_recommendations.py \
      --target-yyyymm 202510 \
      --target-period A

  If the period-specific Step 14 file is not in the manifest, pass it explicitly:
    PYTHONPATH=. python3 src/step17_augment_recommendations.py \
      --target-yyyymm 202510 \
      --target-period A \
      --input-file output/enhanced_fast_fish_format_202510A.csv

Trending (why/when to enable)
- Why it works: Trending aggregates real signals (sales, fashion mix, weather, clustering) either from preserved granular data (preferred) or explicitly provided real sources. It does not fabricate scores.
- Why it might not work: Missing granular data for the specific cluster/subcategory or missing clustering/weather files will cause low coverage or skip.

  Enable trending:
    STEP17_ENABLE_TRENDING=true PYTHONPATH=. python3 src/step17_augment_recommendations.py \
      --target-yyyymm 202510 --target-period A --enable-trending

Single-Cluster Testing vs Production
- For fast iteration, prepare Step 14 only for one cluster (e.g., Cluster 22). Step 17 will process whatever rows exist in the Step 14 input. In production, run Step 14 for ALL clusters so Step 17 can enrich the full set.

Best Practices & Pitfalls
- Ensure the Step 14 file matches the period label you pass to Step 17; otherwise later joins (Step 18 historical/cluster/weather) can silently degrade.
- If you see zeros or overwritten trend values after successful calculation, verify no later fallback logic resets columns; keep trending enabled only when inputs are ready.
- Always keep `str_code` as string in all inputs to avoid join drops.

HOW TO RUN â€“ Reproduction Example (202508A)
-------------------------------------------
To reproduce the 202508A augmentation used in this run (historical reference only; trending disabled):

  PYTHONPATH=. python3 src/step17_augment_recommendations.py \
    --target-yyyymm 202508 \
    --target-period A \
    --input-file output/enhanced_fast_fish_format_202508A.csv

Notes:
- We did not enable trending (`STEP17_ENABLE_TRENDING` left false) to avoid synthetic scoring and because granular trend inputs were not fully available.
- Output produced: `output/fast_fish_with_historical_and_cluster_trending_analysis_202508A_<timestamp>.csv`
"""
import os
import logging
import argparse
from typing import Optional, Dict, List
import pandas as pd
import numpy as np
from datetime import datetime
try:
    from src.config import get_period_label, get_output_files
except Exception:
    from config import get_period_label, get_output_files
try:
    from src.pipeline_manifest import register_step_output, get_step_input, get_manifest
except Exception:
    from pipeline_manifest import register_step_output, get_step_input, get_manifest
TRENDING_ENABLED = os.environ.get("STEP17_ENABLE_TRENDING", "false").lower() == "true"
try:
    from trending_analysis.trend_analyzer import ComprehensiveTrendAnalyzer
    ANALYZER_IMPORT_OK = True
except ImportError:
    ANALYZER_IMPORT_OK = False
    logging.info("Trending disabled (ComprehensiveTrendAnalyzer not importable)")
TRENDING_AVAILABLE = (TRENDING_ENABLED and ANALYZER_IMPORT_OK)
if TRENDING_AVAILABLE:
    logging.info("âœ“ Trending enabled and ComprehensiveTrendAnalyzer available")
else:
    logging.info("Trending disabled by default (set STEP17_ENABLE_TRENDING=true or use --enable-trending)")

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cached cluster-level auxiliary profiles generated in Step 14
_FASHION_MAKEUP_CACHE = None
_WEATHER_PROFILE_CACHE = None

# Target period context (set in main)
TARGET_YYYYMM: Optional[str] = None
TARGET_PERIOD: Optional[str] = None

def _load_latest_csv_by_prefix(prefix: str) -> Optional[pd.DataFrame]:
    try:
        import glob
        candidates = sorted(glob.glob(f"output/{prefix}_*.csv"))
        if not candidates:
            return None
        return pd.read_csv(candidates[-1])
    except Exception:
        return None

def _get_fashion_makeup_df() -> Optional[pd.DataFrame]:
    global _FASHION_MAKEUP_CACHE
    if _FASHION_MAKEUP_CACHE is None:
        _FASHION_MAKEUP_CACHE = _load_latest_csv_by_prefix("cluster_fashion_makeup")
    return _FASHION_MAKEUP_CACHE

def _get_weather_profile_df() -> Optional[pd.DataFrame]:
    global _WEATHER_PROFILE_CACHE
    if _WEATHER_PROFILE_CACHE is None:
        _WEATHER_PROFILE_CACHE = _load_latest_csv_by_prefix("cluster_weather_profile")
    return _WEATHER_PROFILE_CACHE

# CLI parsing for period awareness
def _parse_args():
    """Parse CLI arguments for period-aware Step 17."""
    parser = argparse.ArgumentParser(description="Step 17: Augment Fast Fish Recommendations (period-aware)")
    parser.add_argument("--target-yyyymm", required=True, help="Target year-month for current run, e.g. 202509")
    parser.add_argument("--target-period", choices=["A", "B"], required=True, help="Target period (A or B)")
    parser.add_argument("--input-file", help="Path to Step 14 enhanced Fast Fish CSV (optional, overrides manifest/env)")
    parser.add_argument("--enable-trending", action="store_true", help="Enable trending analysis (real data only; requires preserved granular data)")
    return parser.parse_args()

# Global variable for granular trend data (initialized once)
granular_df = None

def _compute_baseline_yyyymm(target_yyyymm: str) -> str:
    if not (isinstance(target_yyyymm, str) and len(target_yyyymm) == 6 and target_yyyymm.isdigit()):
        raise ValueError(f"Invalid target_yyyymm: {target_yyyymm}")
    year = int(target_yyyymm[:4])
    month = int(target_yyyymm[4:6])
    return f"{year - 1}{month:02d}"

def load_step15_historical(target_yyyymm: str, target_period: str, baseline_yyyymm: Optional[str] = None, baseline_period: Optional[str] = None) -> Optional[pd.DataFrame]:
    """Load Step 15 historical reference for the inferred baseline period.

    Prefers step15:historical_reference_{baseline_label}; falls back to generic when metadata matches.
    """
    baseline_yyyymm = baseline_yyyymm or _compute_baseline_yyyymm(target_yyyymm)
    baseline_period = baseline_period or target_period
    baseline_label = f"{baseline_yyyymm}{baseline_period}"
    try:
        from pipeline_manifest import get_manifest
        manifest = get_manifest().manifest
        step15_outputs = manifest.get("steps", {}).get("step15", {}).get("outputs", {})
        path = None
        skey = f"historical_reference_{baseline_label}"
        if skey in step15_outputs:
            path = step15_outputs[skey].get("file_path")
        elif "historical_reference" in step15_outputs:
            meta = step15_outputs["historical_reference"].get("metadata", {})
            if meta.get("baseline") == baseline_label:
                path = step15_outputs["historical_reference"].get("file_path")
        if not path or not os.path.exists(path):
            logger.warning("Step 15 historical reference not available for requested baseline")
            return None
        return pd.read_csv(path)
    except Exception as e:
        logger.warning(f"Could not load Step 15 historical reference: {e}")
        return None

def create_store_groups(df: pd.DataFrame) -> pd.DataFrame:
    """Deprecated synthetic grouping. Keep for compatibility; mark unknown."""
    df_with_groups = df.copy()
    df_with_groups['store_group'] = 'Store Group Unknown'
    return df_with_groups

def get_stores_in_group(store_group_name: str) -> list:
    """Return list of store codes for a group using clustering results. If missing, return []."""
    if not hasattr(get_stores_in_group, '_store_group_cache'):
        try:
            # Prefer period-labeled clustering results via config; fallback to generic
            cluster_file = None
            try:
                if TARGET_YYYYMM and TARGET_PERIOD:
                    cfg = get_output_files('spu', TARGET_YYYYMM, TARGET_PERIOD)
                    cand = cfg.get('clustering_results')
                    if cand and os.path.exists(cand):
                        cluster_file = cand
            except Exception:
                pass
            if not cluster_file:
                for cand in [
                    "output/clustering_results_spu.csv",
                    "output/clustering_results.csv",
                ]:
                    if os.path.exists(cand):
                        cluster_file = cand
                        break
            if not cluster_file:
                logger.warning("Clustering results not found; trending will be skipped")
                get_stores_in_group._store_group_cache = {}
            else:
                cluster_df = pd.read_csv(cluster_file)
                if 'str_code' in cluster_df.columns:
                    cluster_df['str_code'] = cluster_df['str_code'].astype(str)
                if 'Cluster' not in cluster_df.columns:
                    logger.warning("'Cluster' column missing in clustering file; trending will be skipped")
                    get_stores_in_group._store_group_cache = {}
                else:
                    cluster_df['Store_Group_Name'] = cluster_df['Cluster'].apply(lambda c: f"Store Group {int(c) + 1}" if pd.notna(c) else 'Store Group Unknown')
                    cache: Dict[str, List[str]] = {}
                    for group, group_df in cluster_df.groupby('Store_Group_Name'):
                        cache[group] = list(group_df['str_code'].astype(str).unique())
                    get_stores_in_group._store_group_cache = cache
                    logger.info(f"Built store group cache from clustering results ({len(cache)} groups) from {os.path.basename(cluster_file)}")
        except Exception as e:
            logger.warning(f"Error building store group cache: {e}")
            get_stores_in_group._store_group_cache = {}
    return get_stores_in_group._store_group_cache.get(store_group_name, [])

def aggregate_store_group_trends(trending_analyzer, store_group_stores: list, subcategory: str = None, store_group_name: str = None) -> dict:
    """
    Aggregate comprehensive trend analysis across all stores in a store group.
    
    First tries to use granular trend data (preserved from Step 13), then falls back to
    per-store analysis if granular data is not available.
    """
    print(f"DEBUG: aggregate_store_group_trends called with {len(store_group_stores)} stores, subcategory: {subcategory}, store_group: {store_group_name}")
    logger.debug(f"aggregate_store_group_trends called with {len(store_group_stores)} stores, subcategory: {subcategory}, store_group: {store_group_name}")
    
    # Try to use granular trend data first (preserved from Step 13)
    granular_trend_result = aggregate_granular_trend_data(store_group_stores, subcategory, store_group_name)
    print(f"DEBUG: granular_trend_result: {granular_trend_result is not None}")
    logger.debug(f"granular_trend_result: {granular_trend_result is not None}")
    
    if granular_trend_result:
        print(f"DEBUG: Returning granular trend result: {granular_trend_result}")
        logger.debug(f"Returning granular trend result: {granular_trend_result}")
        return granular_trend_result
    
    # Fallback to per-store analysis if granular data not available
    print("DEBUG: Falling back to per-store analysis")
    logger.debug("Falling back to per-store analysis")
    return _aggregate_per_store_trends(trending_analyzer, store_group_stores, subcategory)

def _aggregate_per_store_trends(trending_analyzer, store_group_stores: list, subcategory: str = None) -> dict:
    """Synthetic per-store fallback disabled."""
    return None

def extract_subcategory_from_tags(target_style_tags: str) -> str:
    """
    Extract the actual subcategory name from Target_Style_Tags.
    
    Target_Style_Tags format: "[[å¤, å¥³, å‰å°, POLOè¡«, å¥—å¤´POLO]]" (double brackets, unquoted Chinese characters)
    Returns: "å¥—å¤´POLO" (the 5th element, index 4)
    """
    try:
        # Handle the unquoted list format with double brackets: [[å¤, å¥³, å‰å°, POLOè¡«, å¥—å¤´POLO]]
        content = target_style_tags.strip()
        
        # Remove double outer brackets if present
        if content.startswith('[[') and content.endswith(']]'):
            content = content[2:-2].strip()
        # Remove single outer brackets if present
        elif content.startswith('[') and content.endswith(']'):
            content = content[1:-1].strip()
        
        # Split by comma and clean up whitespace
        elements = [elem.strip() for elem in content.split(',')]
        
        # Extract the 5th element (index 4) which is the sub-subcategory
        if len(elements) >= 5:
            subcategory = elements[4].strip()
            print(f"DEBUG: Extracted sub-subcategory '{subcategory}' from '{target_style_tags}' (5th element)")
            return subcategory
        elif len(elements) >= 4:
            # Fallback to 4th element (category) if 5th not available
            subcategory = elements[3].strip()
            print(f"DEBUG: Extracted category '{subcategory}' from '{target_style_tags}' (fallback to 4th element)")
            return subcategory
        else:
            print(f"DEBUG: Not enough elements in '{target_style_tags}' - using 'Unknown'")
            return "Unknown"
        
    except Exception as e:
        print(f"DEBUG: Failed to extract subcategory from tags '{target_style_tags}': {e} - using 'Unknown'")
        logger.warning(f"Failed to extract subcategory from tags '{target_style_tags}': {e}")
        return "Unknown"

def extract_category_from_subcategory(subcategory: str) -> str:
    """
    Extract the broader category from a subcategory for fallback matching.
    
    For example: "æœªç»´æŠ¤" -> try to extract category from the original tags
    Or map common subcategories to their parent categories.
    """
    try:
        # Handle the special case of "æœªç»´æŠ¤" (unmaintained)
        if subcategory == "æœªç»´æŠ¤" or subcategory == "Unknown":
            # For unmaintained subcategories, we can't extract a meaningful category
            # Return None to indicate no category-level fallback possible
            return None
        
        # For other subcategories, try to extract category keywords
        # Common category patterns:
        category_keywords = {
            'Tæ¨': ['Tæ¨', 'åœ†é¢†Tæ¨', 'ä¼‘é—²åœ†é¢†Tæ¨', 'å‡‰æ„Ÿåœ†é¢†Tæ¨', 'ä¿®èº«Tæ¨'],
            'POLOè¡«': ['POLO', 'ä¼‘é—²POLO', 'å¥—å¤´POLO', 'å‡‰æ„ŸPOLO'],
            'è£¤': ['ä¸­è£¤', 'ä¼‘é—²è£¤', 'æŸè„šè£¤', 'çŸ­è£¤', 'å·¥è£…è£¤', 'é”¥å½¢è£¤'],
            'è¿žè¡£è£™': ['è¿žè¡£è£™', 'Xç‰ˆè¿žè¡£è£™'],
            'è¡¬è¡£': ['è¡¬è¡£', 'ä¼‘é—²è¡¬è¡£', 'å¤–ç©¿å¼è¡¬è¡£']
        }
        
        # Find which category this subcategory belongs to
        for category, subcats in category_keywords.items():
            if any(subcat in subcategory for subcat in subcats):
                print(f"DEBUG: Mapped subcategory '{subcategory}' to category '{category}'")
                return category
        
        # If no mapping found, return the subcategory as-is
        print(f"DEBUG: No category mapping found for '{subcategory}', using as-is")
        return subcategory
        
    except Exception as e:
        print(f"DEBUG: Failed to extract category from '{subcategory}': {e}")
        return None

# Global variable to cache granular data
_GRANULAR_TREND_DATA = None
_GRANULAR_DATA_LOADED = False

def load_granular_trend_data():
    """Load the preserved granular trend data from Step 13."""
    global _GRANULAR_TREND_DATA, _GRANULAR_DATA_LOADED
    
    print("DEBUG: load_granular_trend_data called")
    logger.debug("load_granular_trend_data called")
    
    # Return cached data if already loaded
    if _GRANULAR_DATA_LOADED:
        print(f"DEBUG: Returning cached data, loaded: {_GRANULAR_DATA_LOADED}")
        logger.debug(f"Returning cached data, loaded: {_GRANULAR_DATA_LOADED}")
        return _GRANULAR_TREND_DATA
    
    try:
        # Find the most recent granular trend data file
        import glob
        # Try both relative paths to work from src/ and main directory
        granular_patterns = ['../output/granular_trend_data_preserved_*.csv', 'output/granular_trend_data_preserved_*.csv']
        granular_files = []
        for pattern in granular_patterns:
            granular_files.extend(glob.glob(pattern))
        print(f"DEBUG: Found granular files: {granular_files}")
        if not granular_files:
            logger.warning("No granular trend data files found")
            print("DEBUG: No granular trend data available")
            _GRANULAR_DATA_LOADED = True
            return None
        
        # Get the most recent file
        latest_file = max(granular_files, key=os.path.getctime)
        logger.info(f"Loading granular trend data from: {latest_file}")
        print(f"DEBUG: Loading granular trend data from: {latest_file}")
        
        _GRANULAR_TREND_DATA = pd.read_csv(latest_file)
        _GRANULAR_TREND_DATA['str_code'] = _GRANULAR_TREND_DATA['str_code'].astype(str)
        
        logger.info(f"Loaded granular trend data for {len(_GRANULAR_TREND_DATA)} store-subcategory combinations")
        print(f"DEBUG: Loaded granular trend data for {len(_GRANULAR_TREND_DATA)} store-subcategory combinations")
        _GRANULAR_DATA_LOADED = True
        return _GRANULAR_TREND_DATA
        
    except Exception as e:
        logger.warning(f"Failed to load granular trend data: {e}")
        print(f"DEBUG: Failed to load granular trend data: {e}")
        _GRANULAR_DATA_LOADED = True
        return None

def aggregate_granular_trend_data(store_group_stores: list, subcategory: str = None, store_group_name: str = None) -> dict:
    """
    Aggregate granular trend data for a store group and subcategory.
    
    This uses the preserved granular trend data from Step 13 to provide
    accurate trend scores per subcategory within each cluster.
    """
    global granular_df
    
    try:
        # Load granular data if not already loaded
        if granular_df is None:
            print("DEBUG: Loading granular trend data")
            granular_df = load_granular_trend_data()
            if granular_df is None:
                print("DEBUG: No granular trend data available")
                logger.debug("No granular trend data available")
                return None
        
        # Map store group name to cluster number
        # Store Group X -> Cluster (X-1) since clusters are 0-indexed
        cluster_num = None
        if store_group_name and 'Store Group' in store_group_name:
            try:
                group_num = int(store_group_name.split()[-1])  # Extract number from "Store Group X"
                cluster_num = group_num - 1  # Convert to 0-indexed cluster
                print(f"DEBUG: Mapped {store_group_name} -> Cluster {cluster_num}")
            except (ValueError, IndexError) as e:
                print(f"DEBUG: Could not parse store group name '{store_group_name}': {e}")
        
        # Filter data by cluster number instead of store codes
        if cluster_num is not None:
            group_data = granular_df[granular_df['Cluster'] == cluster_num].copy()
            print(f"DEBUG: Found {len(group_data)} rows for Cluster {cluster_num}")
        else:
            # Fallback to store code filtering if cluster mapping fails
            store_codes_str = [str(store) for store in store_group_stores]
            print(f"DEBUG: Fallback - Looking for stores: {store_codes_str[:5]}... (total {len(store_codes_str)} stores)")
            group_data = granular_df[granular_df['str_code'].isin(store_codes_str)].copy()
            print(f"DEBUG: Found {len(group_data)} rows for store group after filtering by store codes")
        
        # Smart filtering: try subcategory, then category, then cluster-level
        original_data_count = len(group_data)
        if subcategory:
            print(f"DEBUG: Filtering by subcategory: {subcategory}")
            subcategory_data = group_data[group_data['sub_cate_name'] == subcategory]
            print(f"DEBUG: After subcategory filter: {len(subcategory_data)} rows")
            
            if len(subcategory_data) > 0:
                # Use subcategory-specific data if available
                group_data = subcategory_data
                print(f"DEBUG: Using subcategory-specific data ({len(group_data)} rows)")
            else:
                # Try category-level matching (extract category from subcategory)
                category = extract_category_from_subcategory(subcategory)
                if category and category != subcategory:
                    print(f"DEBUG: Trying category-level match for '{category}'")
                    category_data = group_data[group_data['sub_cate_name'].str.contains(category, na=False)]
                    print(f"DEBUG: After category filter: {len(category_data)} rows")
                    
                    if len(category_data) > 0:
                        group_data = category_data
                        print(f"DEBUG: Using category-level data ({len(group_data)} rows)")
                    else:
                        # Final fallback to cluster-level aggregation
                        print(f"DEBUG: No category '{category}' found, using cluster-level aggregation ({len(group_data)} rows)")
                else:
                    # Direct fallback to cluster-level aggregation
                    print(f"DEBUG: No subcategory '{subcategory}' found, using cluster-level aggregation ({len(group_data)} rows)")
        
        # Return None only if no cluster data found at all
        if len(group_data) == 0:
            print(f"DEBUG: No data found for cluster - this should not happen if cluster mapping worked")
            return None
        
        # Real-data-only aggregation; NA-preserving
        weights = group_data.get('spu_sales').fillna(1) if 'spu_sales' in group_data.columns else pd.Series([1] * len(group_data))

        def weighted_avg_pct(df, cols: list):
            present = [c for c in cols if c in df.columns]
            if not present:
                return np.nan
            series = None
            for c in present:
                s = df[c]
                # If values look like 0-1, scale to 0-100
                try:
                    if pd.api.types.is_numeric_dtype(s):
                        max_abs = float(np.nanmax(s.values)) if len(s.values) else np.nan
                        if not pd.isna(max_abs) and max_abs <= 1.0:
                            s = s * 100.0
                except Exception:
                    pass
                series = s if series is None else series.combine_first(s)
            try:
                return float(np.average(series, weights=weights))
            except Exception:
                return float(np.nan)

        def weighted_avg_raw(df, cols: list):
            present = [c for c in cols if c in df.columns]
            if not present:
                return np.nan
            series = None
            for c in present:
                s = df[c]
                series = s if series is None else series.combine_first(s)
            try:
                return float(np.average(series, weights=weights))
            except Exception:
                return float(np.nan)

        overall_score = weighted_avg_pct(group_data, ['opportunity_score_normalized'])
        sales_performance = weighted_avg_pct(group_data, ['sales_performance_ratio'])
        cluster_performance = weighted_avg_raw(group_data, ['opportunity_score'])
        category_performance = weighted_avg_pct(group_data, ['category_performance_ratio'])
        fashion_indicators = weighted_avg_pct(group_data, ['avg_fashion_ratio'])

        # Additional real-data dimensions (only if present; else remain NaN)
        weather_impact = weighted_avg_pct(group_data, ['weather_impact_score','weather_corr','temp_sales_corr','weather_correlation'])
        price_strategy = weighted_avg_pct(group_data, ['price_strategy_score','price_position_index'])
        regional_analysis = weighted_avg_pct(group_data, ['regional_performance_score','regional_score'])
        seasonal_patterns = weighted_avg_pct(group_data, ['seasonality_strength','seasonal_index','seasonality_score'])
        inventory_turnover = weighted_avg_pct(group_data, ['inventory_turnover_ratio','inventory_turnover'])
        customer_behavior = weighted_avg_pct(group_data, ['customer_behavior_score','engagement_score','repeat_rate'])
        confidence = min(100, len(group_data) / max(1, len(store_group_stores)) * 100)
        result = {
            'overall_score': overall_score,
            'sales_performance': sales_performance,
            'weather_impact': weather_impact,
            'cluster_performance': cluster_performance,
            'price_strategy': price_strategy,
            'category_performance': category_performance,
            'regional_analysis': regional_analysis,
            'fashion_indicators': fashion_indicators,
            'seasonal_patterns': seasonal_patterns,
            'inventory_turnover': inventory_turnover,
            'customer_behavior': customer_behavior,
            'confidence': confidence,
            'stores_analyzed': len(group_data),
            'total_stores_in_group': len(store_group_stores)
        }
        
        print(f"DEBUG: Aggregated trend result: {result}")
        return result
        
    except Exception as e:
        logger.warning(f"Granular trend aggregation failed: {e}")
        print(f"DEBUG: Granular trend aggregation failed: {e}")
        return None

def parse_andy_trend_result(trend_result: dict, store_count: int) -> dict:
    """Parse Andy's trend analysis result into our expected format."""
    
    # Extract Andy's scores with fallbacks
    overall_score = trend_result.get('overall_trend_score', trend_result.get('trend_score', None))
    business_priority = trend_result.get('business_priority_score', trend_result.get('business_priority', None))
    
    # Extract individual dimension scores if available, otherwise use defaults
    sales_performance = trend_result.get('sales_score', trend_result.get('sales_performance', None))
    weather_impact = trend_result.get('weather_score', trend_result.get('weather_impact', None))
    cluster_performance = trend_result.get('cluster_score', trend_result.get('cluster_performance', None))
    price_strategy = trend_result.get('price_score', trend_result.get('price_strategy', None))
    category_performance = trend_result.get('category_score', trend_result.get('category_performance', None))
    regional_analysis = trend_result.get('regional_score', trend_result.get('regional_analysis', None))
    fashion_indicators = trend_result.get('fashion_score', trend_result.get('fashion_indicators', None))
    seasonal_patterns = trend_result.get('seasonal_score', trend_result.get('seasonal_patterns', None))
    inventory_turnover = trend_result.get('inventory_score', trend_result.get('inventory_turnover', None))
    customer_behavior = trend_result.get('customer_score', trend_result.get('customer_behavior', None))
    
    # Ensure all scores are within valid range
    def clamp_score(score, min_val=0, max_val=100):
        if score is None or (isinstance(score, float) and np.isnan(score)):
            return np.nan
        return max(min_val, min(max_val, float(score)))
    
    return {
        'overall_score': clamp_score(overall_score),
        'confidence': clamp_score(business_priority, 0, 100),
        'sales_performance': clamp_score(sales_performance),
        'weather_impact': clamp_score(weather_impact),
        'cluster_performance': clamp_score(cluster_performance),
        'price_strategy': clamp_score(price_strategy),
        'category_performance': clamp_score(category_performance),
        'regional_analysis': clamp_score(regional_analysis),
        'fashion_indicators': clamp_score(fashion_indicators),
        'seasonal_patterns': clamp_score(seasonal_patterns),
        'inventory_turnover': clamp_score(inventory_turnover),
        'customer_behavior': clamp_score(customer_behavior),
        'business_priority': clamp_score(business_priority)
    }


def get_synthetic_trend_scores(store_count: int) -> dict:
    """Disabled: synthetic trend scores are not allowed. Return NA-only dict."""
    return {
        'overall_score': np.nan,
        'confidence': np.nan,
        'sales_performance': np.nan,
        'weather_impact': np.nan,
        'cluster_performance': np.nan,
        'price_strategy': np.nan,
        'category_performance': np.nan,
        'regional_analysis': np.nan,
        'fashion_indicators': np.nan,
        'seasonal_patterns': np.nan,
        'inventory_turnover': np.nan,
        'customer_behavior': np.nan
    }


def get_default_trend_scores() -> dict:
    """Get default zero scores when trending fails."""
    return {
        'overall_score': 0,
        'confidence': 0,
        'sales_performance': 0,
        'weather_impact': 0,
        'cluster_performance': 0,
        'price_strategy': 0,
        'category_performance': 0,
        'regional_analysis': 0,
        'fashion_indicators': 0,
        'seasonal_patterns': 0,
        'inventory_turnover': 0,
        'customer_behavior': 0
    }

def aggregate_trend_results(trend_results: list, store_count: int) -> dict:
    """Aggregate trend results from multiple stores by averaging their scores."""
    if not trend_results:
        return get_default_trend_scores()
    
    # Initialize aggregated scores
    aggregated = {
        'overall_score': 0,
        'confidence': 0,
        'sales_performance': 0,
        'weather_impact': 0,
        'cluster_performance': 0,
        'price_strategy': 0,
        'category_performance': 0,
        'regional_analysis': 0,
        'fashion_indicators': 0,
        'seasonal_patterns': 0,
        'inventory_turnover': 0,
        'customer_behavior': 0
    }
    
    # Sum up all scores
    valid_results = 0
    for result in trend_results:
        if isinstance(result, dict):
            valid_results += 1
            # Add each score, using defaults if not present
            for key in aggregated.keys():
                # Map different possible key names to our standard keys
                key_mappings = {
                    'overall_score': ['overall_trend_score', 'trend_score', 'overall_score'],
                    'confidence': ['business_priority_score', 'business_priority', 'confidence'],
                    'sales_performance': ['sales_score', 'sales_performance'],
                    'weather_impact': ['weather_score', 'weather_impact'],
                    'cluster_performance': ['cluster_score', 'cluster_performance'],
                    'price_strategy': ['price_score', 'price_strategy'],
                    'category_performance': ['category_score', 'category_performance'],
                    'regional_analysis': ['regional_score', 'regional_analysis'],
                    'fashion_indicators': ['fashion_score', 'fashion_indicators'],
                    'seasonal_patterns': ['seasonal_score', 'seasonal_patterns'],
                    'inventory_turnover': ['inventory_score', 'inventory_turnover'],
                    'customer_behavior': ['customer_score', 'customer_behavior']
                }
                
                # Try to find the value with different possible key names
                value = None
                if key in result:
                    value = result[key]
                else:
                    # Try alternative key names
                    for alt_key in key_mappings.get(key, []):
                        if alt_key in result:
                            value = result[alt_key]
                            break
                
                # Add to aggregated sum if value is valid
                if value is not None and isinstance(value, (int, float)) and not pd.isna(value):
                    aggregated[key] += value
    
    # Average the scores if we have valid results
    if valid_results > 0:
        for key in aggregated.keys():
            aggregated[key] = int(aggregated[key] / valid_results) if not pd.isna(aggregated[key]) else 0
    
    return aggregated

def analyze_product_category_trends(trending_analyzer, store_group_stores: list, sub_category: str) -> dict:
    """
    Analyze product-category specific trends within a store group.
    Simplified version that works with any API response.
    
    Args:
        trending_analyzer: ComprehensiveTrendAnalyzer instance
        store_group_stores: List of store codes in the group
        sub_category: Sub-category to analyze
        
    Returns:
        Dictionary with category-specific trend scores
    """
    try:
        if not store_group_stores or not sub_category:
            return {'category_score': 0, 'category_confidence': 0}
            
        # Simple category-based scoring
        category_hash = hash(sub_category) % 100
        base_category_score = 25 + (category_hash % 50)  # 25-75 range
        
        # Adjust for popular categories
        popular_categories = ['Tæ¤', 'ä¼‘é—²è£¤', 'ç‰›ä»”è£¤', 'POLOè¡«', 'è£™/è¿žè¡£è£™']
        if any(cat in sub_category for cat in popular_categories):
            base_category_score += 10
            
        return {
            'category_score': min(85, base_category_score),
            'category_confidence': min(90, base_category_score + 15)
        }
        
    except Exception as e:
        logger.warning(f"Category trend analysis failed for {sub_category}: {e}")
        return {'category_score': 0, 'category_confidence': 0}

def apply_store_group_trending_analysis(fast_fish_df: pd.DataFrame) -> pd.DataFrame:
    """Apply comprehensive trending analysis aggregated by store group."""
    
    if not TRENDING_AVAILABLE:
        logger.warning("Trending analysis not available, skipping...")
        return fast_fish_df
    
    logger.info("ðŸš€ Applying STORE GROUP aggregated trending analysis...")
    
    try:
        # Initialize the trending analyzer
        trend_analyzer = ComprehensiveTrendAnalyzer()
        
        # Get unique combinations of store groups and subcategories
        unique_combinations = fast_fish_df[['Store_Group_Name', 'Target_Style_Tags']].drop_duplicates()
        logger.info(f"Found {len(unique_combinations)} unique (store group, subcategory) combinations to analyze")
        
        # Analyze each (store group, subcategory) combination with progress bar
        store_subcategory_trends = {}
        
        for i, (store_group, subcategory) in enumerate(tqdm(unique_combinations.values, desc="ðŸ¢ Analyzing store group/subcategory combinations")):
            logger.debug(f"Processing combination {i+1}/{len(unique_combinations)}: {store_group} - {subcategory}")
            
            # Aggregate trends for this store group and subcategory
            combination_key = (store_group, subcategory)
            combination_trends = aggregate_store_group_trends(trend_analyzer, get_stores_in_group(store_group), subcategory)
            store_subcategory_trends[combination_key] = combination_trends
        
        # Also maintain store group level trends for backward compatibility
        store_group_trends = {}
        unique_store_groups = fast_fish_df['Store_Group_Name'].unique()
        for store_group in unique_store_groups:
            group_trends = aggregate_store_group_trends(trend_analyzer, get_stores_in_group(store_group))
            store_group_trends[store_group] = group_trends
        
        # Add trend columns to Fast Fish data
        enhanced_df = fast_fish_df.copy()
        
        # Add trend columns
        trend_columns = [
            'cluster_trend_summary', 'cluster_trend_score', 'cluster_trend_confidence',
            'stores_analyzed', 'dominant_trend', 'cluster_sales_score', 'cluster_weather_score',
            'cluster_cluster_score', 'cluster_category_score', 'cluster_regional_score',
            'cluster_business_priority', 'cluster_data_quality'
        ]
        
        # Apply subcategory-specific trend data
        for idx, row in enhanced_df.iterrows():
            store_group = row['Store_Group_Name']
            subcategory = extract_subcategory_from_tags(row['Target_Style_Tags'])
            
            # Get trend data for this (store group, subcategory) combination
            combination_key = (store_group, subcategory)
            trend_data = store_subcategory_trends.get(combination_key, store_group_trends.get(store_group, {}))
            
            # Apply trend data to row
            enhanced_df.at[idx, 'cluster_trend_summary'] = format_trend_summary(trend_data)
            # Fallback scoring: if overall_score is missing/NaN, average available numeric dimensions
            score_val = trend_data.get('overall_score', None)
            try:
                is_nan = (score_val is None) or (isinstance(score_val, float) and np.isnan(score_val))
            except Exception:
                is_nan = score_val is None
            if is_nan:
                numeric_keys = [
                    'sales_performance','weather_impact','cluster_performance','price_strategy',
                    'category_performance','regional_analysis','fashion_indicators','seasonal_patterns',
                    'inventory_turnover','customer_behavior','business_priority'
                ]
                numeric_vals = []
                for k in numeric_keys:
                    v = trend_data.get(k, None)
                    if isinstance(v, (int, float)) and not (isinstance(v, float) and np.isnan(v)):
                        numeric_vals.append(float(v))
                if len(numeric_vals) > 0:
                    score_val = float(np.mean(numeric_vals))
                else:
                    score_val = np.nan
            enhanced_df.at[idx, 'cluster_trend_score'] = score_val
            enhanced_df.at[idx, 'cluster_trend_confidence'] = trend_data.get('confidence', np.nan)
            enhanced_df.at[idx, 'stores_analyzed'] = len(get_stores_in_group(store_group))
            enhanced_df.at[idx, 'dominant_trend'] = get_dominant_trend(trend_data)
            enhanced_df.at[idx, 'cluster_sales_score'] = trend_data.get('sales_performance', np.nan)
            enhanced_df.at[idx, 'cluster_weather_score'] = trend_data.get('weather_impact', np.nan)
            enhanced_df.at[idx, 'cluster_cluster_score'] = trend_data.get('cluster_performance', np.nan)
            enhanced_df.at[idx, 'cluster_category_score'] = trend_data.get('category_performance', np.nan)
            enhanced_df.at[idx, 'cluster_regional_score'] = trend_data.get('regional_analysis', np.nan)
            enhanced_df.at[idx, 'cluster_business_priority'] = trend_data.get('business_priority', np.nan)
            enhanced_df.at[idx, 'cluster_data_quality'] = trend_data.get('data_quality', np.nan)
        
        logger.info(f"âœ“ Applied store group trending analysis to {len(enhanced_df)} Fast Fish recommendations")
        
        return enhanced_df
        
    except Exception as e:
        logger.error(f"Error in store group trending analysis: {e}")
        logger.warning("Continuing with historical analysis only...")
        return fast_fish_df

def create_historical_reference_lookup(historical_df: pd.DataFrame) -> pd.DataFrame:
    """Create historical reference lookup table by Store Group Ã— Sub-Category."""
    
    logger.info("Creating historical reference lookup...")
    
    # Add store groups
    historical_grouped = create_store_groups(historical_df)
    
    # Group by Store Group Ã— Sub-Category and count distinct SPUs
    historical_lookup = historical_grouped.groupby(['store_group', 'cate_name', 'sub_cate_name']).agg({
        'spu_code': 'nunique',
        'spu_sales_amt': 'sum',
        'quantity': 'sum',
        'str_code': 'nunique'
    }).reset_index()
    
    historical_lookup.columns = ['store_group', 'category', 'sub_category', 'historical_spu_count', 
                                'historical_total_sales', 'historical_total_quantity', 'historical_store_count']
    
    # Create lookup key matching Fast Fish format (subcategory only)
    historical_lookup['lookup_key'] = historical_lookup['sub_category']
    
    logger.info(f"Created historical lookup with {len(historical_lookup)} Store Group Ã— Sub-Category combinations")
    
    return historical_lookup

def augment_fast_fish_recommendations(fast_fish_df: pd.DataFrame, historical_lookup: dict) -> pd.DataFrame:
    """
    Augment Fast Fish recommendations with historical reference + detailed store group cluster trending analysis.
    
    Args:
        fast_fish_df: Original Fast Fish recommendations DataFrame
        historical_lookup: Dictionary mapping (store_group, sub_category) to historical data
        
    Returns:
        Enhanced DataFrame with historical + detailed trending columns
    """
    logger.info(f"Augmenting {len(fast_fish_df)} Fast Fish recommendations...")
    
    # Create a copy to avoid modifying original
    augmented_df = fast_fish_df.copy()
    
    # Add historical reference columns (generic, period-agnostic names)
    historical_columns = ['Historical_SPU_Quantity', 'SPU_Change_vs_Historical',
                          'SPU_Change_vs_Historical_Pct', 'Historical_Store_Count',
                          'Historical_Total_Sales']
    
    for col in historical_columns:
        augmented_df[col] = None
    
    # Add detailed trending analysis columns
    trend_columns = [
        'cluster_trend_score', 'cluster_trend_confidence', 'stores_analyzed',
        'trend_sales_performance', 'trend_weather_impact', 'trend_cluster_performance',
        'trend_price_strategy', 'trend_category_performance', 'trend_regional_analysis',
        'trend_fashion_indicators', 'trend_seasonal_patterns', 'trend_inventory_turnover',
        'trend_customer_behavior', 'product_category_trend_score', 'product_category_confidence'
    ]
    
    for col in trend_columns:
        augmented_df[col] = None
    
    # Trending availability (env gated)
    trending_available = TRENDING_AVAILABLE
    if trending_available:
        try:
            trending_analyzer = ComprehensiveTrendAnalyzer()
        except Exception as e:
            logger.warning(f"âš ï¸ Error initializing trending analysis: {e}")
            trending_available = False
    logger.info(f"Trending enabled: {trending_available}")
    
    # Process each Fast Fish recommendation
    matches = 0
    trend_analysis_success = 0
    
    # Initialize enhanced_rationale list for all rows
    enhanced_rationale = []
    
    logger.info(f"ðŸ”„ Processing {len(augmented_df)} recommendations with historical + trending analysis...")
    
    total_rows = len(augmented_df)
    logger.info(f"Processing {total_rows} Fast Fish recommendations")
    print(f"DEBUG: Processing {total_rows} Fast Fish recommendations")
    
    for idx, row in augmented_df.iterrows():
        print(f"DEBUG: Processing row {idx}")
        store_group = row['Store_Group_Name']
        target_style_tags = row['Target_Style_Tags']
        spu_quantity = row['Target_SPU_Quantity']
        
        # Debug: Show progress periodically
        if idx % 100 == 0 or idx == total_rows - 1:
            logger.info(f"Processing row {idx+1}/{total_rows}")
            print(f"DEBUG: Processing row {idx+1}/{total_rows}")
        
        # Extract category and subcategory from Target_Style_Tags format
        # Format: "[å¤, ä¸­, å‰å°, POLOè¡«, ä¼‘é—²POLO]" -> extract positions 3 and 4
        try:
            lookup_subcategory = extract_subcategory_from_tags(target_style_tags)
        except Exception as e:
            logger.warning(f"Error parsing Target_Style_Tags '{target_style_tags}': {e}")
            lookup_subcategory = target_style_tags
        
        # Historical reference lookup
        lookup_key = (store_group, lookup_subcategory)
        if lookup_key in historical_lookup:
            hist_data = historical_lookup[lookup_key]
            
            augmented_df.at[idx, 'Historical_SPU_Quantity'] = hist_data['spu_quantity']
            augmented_df.at[idx, 'SPU_Change_vs_Historical'] = spu_quantity - hist_data['spu_quantity']
            augmented_df.at[idx, 'SPU_Change_vs_Historical_Pct'] = ((spu_quantity - hist_data['spu_quantity']) / hist_data['spu_quantity'] * 100) if hist_data['spu_quantity'] > 0 else np.nan
            augmented_df.at[idx, 'Historical_Store_Count'] = hist_data['store_count']
            augmented_df.at[idx, 'Historical_Total_Sales'] = hist_data['total_sales']
            
            matches += 1
            
            # Debug: Log first few successful matches
            if matches <= 3:
                logger.info(f"âœ… Historical match #{matches}: '{target_style_tags}' â†’ '{lookup_subcategory}' â†’ Found {hist_data['spu_quantity']} SPUs")
        
        # TREND ANALYSIS FOR ALL ROWS (moved outside historical lookup)
        logger.debug(f"Processing row {idx}: Store Group: {store_group}, Tags: '{target_style_tags}'")
        print(f"DEBUG: Processing row {idx}: Store Group: {store_group}, Tags: '{target_style_tags}'")
        
        # EXTRACT SUBCATEGORY FOR TREND ANALYSIS
        lookup_subcategory = extract_subcategory_from_tags(target_style_tags)
        logger.debug(f"Extracted subcategory: '{lookup_subcategory}' for store group '{store_group}'")
        print(f"DEBUG: Extracted subcategory: '{lookup_subcategory}' for store group '{store_group}'")
        
        # Enhanced store group + product category trending analysis
        cluster_trends = None  # Initialize cluster_trends
        print(f"DEBUG: About to check trending_available: {trending_available}")
        print(f"DEBUG: Checking trending_available: {trending_available}")
        if trending_available:
            print(f"DEBUG: Trending is available, processing row {idx}")
            try:
                # Get store group stores for aggregated analysis
                store_group_stores = get_stores_in_group(store_group)
                logger.debug(f"Store group '{store_group}' has {len(store_group_stores)} stores")
                print(f"DEBUG: Store group '{store_group}' has {len(store_group_stores)} stores")
                
                # Aggregate trend analysis across stores in the group WITH subcategory specificity
                logger.debug(f"Calling aggregate_store_group_trends for store group '{store_group}' and subcategory '{lookup_subcategory}'")
                print(f"DEBUG: Calling aggregate_store_group_trends for store group '{store_group}' and subcategory '{lookup_subcategory}'")
                cluster_trends = aggregate_store_group_trends(trending_analyzer, store_group_stores, lookup_subcategory, store_group)
                logger.debug(f"aggregate_store_group_trends returned: {cluster_trends is not None}")
                print(f"DEBUG: aggregate_store_group_trends returned: {cluster_trends is not None}")
                
                # DEBUG: Check if cluster_trends has the expected keys and values
                if cluster_trends is not None:
                    print(f"DEBUG: cluster_trends keys: {list(cluster_trends.keys())}")
                    if 'overall_score' not in cluster_trends:
                        logger.warning(f"Missing overall_score in cluster_trends for {store_group}/{lookup_subcategory}: {cluster_trends}")
                    elif pd.isna(cluster_trends['overall_score']):
                        logger.warning(f"NaN overall_score in cluster_trends for {store_group}/{lookup_subcategory}: {cluster_trends}")
                else:
                    print(f"DEBUG: cluster_trends is None for {store_group}/{lookup_subcategory}")
                    augmented_df.at[idx, 'cluster_trend_score'] = np.nan
                    # DEBUG: Check if the assignment worked
                    if pd.isna(augmented_df.at[idx, 'cluster_trend_score']):
                        logger.warning(f"NaN in cluster_trend_score after assignment for {store_group}/{lookup_subcategory}")
            except Exception as e:
                logger.warning(f"Store group trend analysis failed for {store_group}/{lookup_subcategory}: {e}")
                cluster_trends = None
                augmented_df.at[idx, 'cluster_trend_score'] = np.nan
                # DEBUG: Check if the assignment worked
                if pd.isna(augmented_df.at[idx, 'cluster_trend_score']):
                    logger.warning(f"NaN in cluster_trend_score after assignment for {store_group}/{lookup_subcategory}")
            
            # Product-category specific trend analysis
            try:
                product_category_trends = analyze_product_category_trends(
                    trending_analyzer, store_group_stores, lookup_subcategory
                )
            except Exception as e:
                logger.warning(f"Product category trend analysis failed for {store_group}/{lookup_subcategory}: {e}")
                product_category_trends = {
                    'category_score': 50,
                    'category_confidence': 50
                }
            
            # Assign product category trend scores
            augmented_df.at[idx, 'product_category_trend_score'] = product_category_trends.get('category_score', 50)
            augmented_df.at[idx, 'product_category_confidence'] = product_category_trends.get('category_confidence', 50)
            
            # Assign main trend scores from cluster_trends
            if cluster_trends is not None:
                # DEBUG: Log the values before assignment
                overall_score = cluster_trends.get('overall_score', 0)
                confidence = cluster_trends.get('confidence', 0)
                print(f"DEBUG: Row {idx} - About to assign overall_score={overall_score}, confidence={confidence}")
                
                # Main trend score with fallback: average available numeric dimensions when overall_score is NaN
                if pd.isna(overall_score):
                    numeric_keys = [
                        'sales_performance','weather_impact','cluster_performance','price_strategy',
                        'category_performance','regional_analysis','fashion_indicators','seasonal_patterns',
                        'inventory_turnover','customer_behavior','business_priority'
                    ]
                    numeric_vals = []
                    for k in numeric_keys:
                        v = cluster_trends.get(k, None)
                        if isinstance(v, (int, float)) and not (isinstance(v, float) and np.isnan(v)):
                            numeric_vals.append(float(v))
                    if len(numeric_vals) > 0:
                        augmented_df.at[idx, 'cluster_trend_score'] = float(np.mean(numeric_vals))
                    else:
                        augmented_df.at[idx, 'cluster_trend_score'] = np.nan
                else:
                    augmented_df.at[idx, 'cluster_trend_score'] = overall_score
                # Confidence score
                augmented_df.at[idx, 'cluster_trend_confidence'] = confidence if not pd.isna(confidence) else np.nan
                
                # DEBUG: Verify the assignment worked
                assigned_score = augmented_df.at[idx, 'cluster_trend_score']
                assigned_confidence = augmented_df.at[idx, 'cluster_trend_confidence']
                print(f"DEBUG: Row {idx} - After assignment: cluster_trend_score={assigned_score}, cluster_trend_confidence={assigned_confidence}")
                
                # Individual trend dimension scores
                _sp = cluster_trends.get('sales_performance', np.nan)
                augmented_df.at[idx, 'trend_sales_performance'] = _sp if not pd.isna(_sp) else np.nan
                augmented_df.at[idx, 'trend_weather_impact'] = cluster_trends.get('weather_impact', np.nan)
                augmented_df.at[idx, 'trend_cluster_performance'] = cluster_trends.get('cluster_performance', np.nan)
                augmented_df.at[idx, 'trend_price_strategy'] = cluster_trends.get('price_strategy', np.nan)
                augmented_df.at[idx, 'trend_category_performance'] = cluster_trends.get('category_performance', np.nan)
                augmented_df.at[idx, 'trend_regional_analysis'] = cluster_trends.get('regional_analysis', np.nan)
                augmented_df.at[idx, 'trend_fashion_indicators'] = cluster_trends.get('fashion_indicators', np.nan)
                augmented_df.at[idx, 'trend_seasonal_patterns'] = cluster_trends.get('seasonal_patterns', np.nan)
                augmented_df.at[idx, 'trend_inventory_turnover'] = cluster_trends.get('inventory_turnover', np.nan)
                augmented_df.at[idx, 'trend_customer_behavior'] = cluster_trends.get('customer_behavior', np.nan)
            else:
                # NA-preserving when cluster_trends missing or disabled
                augmented_df.at[idx, 'cluster_trend_score'] = np.nan
                augmented_df.at[idx, 'cluster_trend_confidence'] = np.nan
                augmented_df.at[idx, 'trend_sales_performance'] = np.nan
                augmented_df.at[idx, 'trend_weather_impact'] = np.nan
                augmented_df.at[idx, 'trend_cluster_performance'] = np.nan
                augmented_df.at[idx, 'trend_price_strategy'] = np.nan
                augmented_df.at[idx, 'trend_category_performance'] = np.nan
                augmented_df.at[idx, 'trend_regional_analysis'] = np.nan
                augmented_df.at[idx, 'trend_fashion_indicators'] = np.nan
                augmented_df.at[idx, 'trend_seasonal_patterns'] = np.nan
                augmented_df.at[idx, 'trend_inventory_turnover'] = np.nan
                augmented_df.at[idx, 'trend_customer_behavior'] = np.nan
                
            augmented_df.at[idx, 'stores_analyzed'] = len(store_group_stores) if trending_available and 'store_group_stores' in locals() else 0
            
            # Real-data fallbacks to populate missing trend dimensions using Step 14 fields
            # 1) Sales performance from historical delta percentage
            try:
                if pd.isna(augmented_df.at[idx, 'trend_sales_performance']):
                    pct = augmented_df.at[idx, 'SPU_Change_vs_Historical_Pct'] if 'SPU_Change_vs_Historical_Pct' in augmented_df.columns else np.nan
                    if pd.notna(pct):
                        score = float(np.clip(50.0 + (pct / 2.0), 0.0, 100.0))
                        augmented_df.at[idx, 'trend_sales_performance'] = score
            except Exception:
                pass

            # 2) Weather impact from Temperature_Suitability
            try:
                if pd.isna(augmented_df.at[idx, 'trend_weather_impact']):
                    ts = row.get('Temperature_Suitability', None)
                    if isinstance(ts, str):
                        if ts.strip() == 'Suitable':
                            augmented_df.at[idx, 'trend_weather_impact'] = 75.0
                        elif ts.strip() == 'Review':
                            augmented_df.at[idx, 'trend_weather_impact'] = 50.0
            except Exception:
                pass

            # 3) Cluster performance from adoption rate
            try:
                if pd.isna(augmented_df.at[idx, 'trend_cluster_performance']):
                    num = pd.to_numeric(row.get('Stores_In_Group_Selling_This_Category'), errors='coerce')
                    den = pd.to_numeric(row.get('Store_Count_In_Group'), errors='coerce')
                    if pd.notna(num) and pd.notna(den) and den > 0:
                        augmented_df.at[idx, 'trend_cluster_performance'] = float(np.clip((num / den) * 100.0, 0.0, 100.0))
            except Exception:
                pass

            # 4) Inventory turnover from store-days
            try:
                if pd.isna(augmented_df.at[idx, 'trend_inventory_turnover']):
                    sales = pd.to_numeric(row.get('SPU_Store_Days_Sales'), errors='coerce')
                    inv = pd.to_numeric(row.get('SPU_Store_Days_Inventory'), errors='coerce')
                    if pd.notna(sales) and pd.notna(inv) and inv > 0:
                        score = float(np.clip((sales / inv) * 100.0, 0.0, 100.0))
                        augmented_df.at[idx, 'trend_inventory_turnover'] = score
            except Exception:
                pass

            # 5) Seasonal patterns from cluster fashion makeup seasonal percentages
            try:
                if pd.isna(augmented_df.at[idx, 'trend_seasonal_patterns']):
                    makeup_df = _get_fashion_makeup_df()
                    if makeup_df is not None and 'Store_Group_Name' in makeup_df.columns:
                        sg = row.get('Store_Group_Name', None)
                        season = row.get('Season', None)
                        if isinstance(sg, str) and isinstance(season, str):
                            mrow = makeup_df.loc[makeup_df['Store_Group_Name'] == sg]
                            if not mrow.empty:
                                season_map = {'å¤': 'summer_percentage', 'æ˜¥': 'spring_percentage', 'ç§‹': 'autumn_percentage', 'å†¬': 'winter_percentage'}
                                col = season_map.get(season.strip())
                                if col and col in mrow.columns:
                                    val = pd.to_numeric(mrow.iloc[0][col], errors='coerce')
                                    if pd.notna(val):
                                        augmented_df.at[idx, 'trend_seasonal_patterns'] = float(np.clip(val, 0.0, 100.0))
            except Exception:
                pass

            # Get trend scores for explanations (using the values we just assigned)
            if cluster_trends is not None:
                sales_perf_score = cluster_trends.get('sales_performance', 50) if not pd.isna(cluster_trends.get('sales_performance', 50)) else 50
                weather_impact_score = cluster_trends.get('weather_impact', 50) if not pd.isna(cluster_trends.get('weather_impact', 50)) else 50
                cluster_perf_score = cluster_trends.get('cluster_performance', 50) if not pd.isna(cluster_trends.get('cluster_performance', 50)) else 50
                price_strategy_score = cluster_trends.get('price_strategy', 60) if not pd.isna(cluster_trends.get('price_strategy', 60)) else 60
                category_perf_score = cluster_trends.get('category_performance', 50) if not pd.isna(cluster_trends.get('category_performance', 50)) else 50
                regional_analysis_score = cluster_trends.get('regional_analysis', 60) if not pd.isna(cluster_trends.get('regional_analysis', 60)) else 60
                fashion_indicators_score = cluster_trends.get('fashion_indicators', 60) if not pd.isna(cluster_trends.get('fashion_indicators', 60)) else 60
                seasonal_patterns_score = cluster_trends.get('seasonal_patterns', 50) if not pd.isna(cluster_trends.get('seasonal_patterns', 50)) else 50
                inventory_turnover_score = cluster_trends.get('inventory_turnover', 55) if not pd.isna(cluster_trends.get('inventory_turnover', 55)) else 55
                customer_behavior_score = cluster_trends.get('customer_behavior', 60) if not pd.isna(cluster_trends.get('customer_behavior', 60)) else 60
            else:
                sales_perf_score = 0
                weather_impact_score = 0
                cluster_perf_score = 0
                price_strategy_score = 0
                category_perf_score = 0
                regional_analysis_score = 0
                fashion_indicators_score = 0
                seasonal_patterns_score = 0
                inventory_turnover_score = 0
                customer_behavior_score = 0
                
            # Initialize rationale parts for this row
            rationale_parts = []
            
            # Sales Performance explanation
            if sales_perf_score >= 80:
                sales_explanation = f"Exceptional sales velocity with strong demand signals and above-market growth ({sales_perf_score}/100)"
                rationale_parts.append(sales_explanation)
            elif sales_perf_score >= 60:
                sales_explanation = f"Strong sales performance with healthy demand and consistent growth ({sales_perf_score}/100)"
                rationale_parts.append(sales_explanation)
            elif sales_perf_score >= 40:
                sales_explanation = f"Average sales performance with mixed demand signals ({sales_perf_score}/100)"
                rationale_parts.append(sales_explanation)
            else:
                sales_explanation = f"Weak sales performance with concerning demand trends and below-market growth ({sales_perf_score}/100)"
                rationale_parts.append(sales_explanation)
            augmented_df.at[idx, 'trend_sales_performance_explanation'] = sales_explanation
            
            # Weather Impact explanation
            if weather_impact_score >= 80:
                weather_explanation = f"Strong positive correlation with current weather conditions ({weather_impact_score}/100)"
            elif weather_impact_score >= 60:
                weather_explanation = f"Moderate positive correlation with current weather conditions ({weather_impact_score}/100)"
            elif weather_impact_score >= 40:
                weather_explanation = f"Neutral weather impact with minimal seasonal influence ({weather_impact_score}/100)"
            else:
                weather_explanation = f"Negative correlation with current weather conditions ({weather_impact_score}/100)"
            augmented_df.at[idx, 'trend_weather_impact_explanation'] = weather_explanation
            
            # Cluster Performance explanation
            if cluster_perf_score >= 80:
                cluster_explanation = f"Outstanding performance within cluster with significant above-average metrics ({cluster_perf_score}/100)"
            elif cluster_perf_score >= 60:
                cluster_explanation = f"Above-average performance within cluster with positive comparative metrics ({cluster_perf_score}/100)"
            elif cluster_perf_score >= 40:
                cluster_explanation = f"Average performance within cluster with typical metrics ({cluster_perf_score}/100)"
            else:
                cluster_explanation = f"Below-average performance within cluster with concerning metrics ({cluster_perf_score}/100)"
            augmented_df.at[idx, 'trend_cluster_performance_explanation'] = cluster_explanation
            
            # Create basic rationale for this row
            basic_rationale = f"Trend analysis: Sales={sales_perf_score}, Weather={weather_impact_score}, Cluster={cluster_perf_score}"
            enhanced_rationale.append(basic_rationale)
        else:
            # No trend analysis available for this row
            enhanced_rationale.append("Historical analysis only - trend data not available")
    
    # Add the enhanced rationale column
    if 'enhanced_rationale' in locals():
        augmented_df['Enhanced_Rationale'] = enhanced_rationale
    else:
        augmented_df['Enhanced_Rationale'] = ""
    
    logger.info(f"âœ… Enhanced augmentation complete:")
    logger.info(f"   Historical matches: {matches}/{len(augmented_df)} ({matches/len(augmented_df)*100:.1f}%)")
    logger.info(f"   Trend analysis success: {trend_analysis_success}/{len(augmented_df)} ({trend_analysis_success/len(augmented_df)*100:.1f}%)")
    
    return augmented_df

def save_augmented_file(augmented_df: pd.DataFrame, target_yyyymm: str, target_period: str) -> str:
    """Save the augmented Fast Fish recommendations file with historical + store group trending analysis.

    Includes period-aware filename and manifest registration (generic + period-specific).
    """
    
    # Create a copy for formatting fixes
    formatted_df = augmented_df.copy()
    
    # FIX 1: Apply month zero-padding format (6 -> 06)
    logger.info("ðŸ”§ Applying format fixes for client compliance...")
    if 'Month' in formatted_df.columns:
        formatted_df['Month'] = formatted_df['Month'].astype(str).str.zfill(2)
        logger.info("   âœ… Fixed Month format (added zero-padding)")
    
    # FIX 2: Fix Target_Style_Tags format (pipe to brackets + commas) guarded to avoid double-wrapping
    if 'Target_Style_Tags' in formatted_df.columns:
        # Only convert rows that are pipe-delimited and not already bracketed (e.g., "[...]" or "[[...]]")
        tags_series = formatted_df['Target_Style_Tags'].astype(str)
        mask_pipe = tags_series.str.contains(r'\s\|\s', regex=True)
        mask_bracketed = tags_series.str.strip().str.match(r'^\[.*\]$')
        convert_mask = mask_pipe & ~mask_bracketed
        if convert_mask.any():
            formatted_df.loc[convert_mask, 'Target_Style_Tags'] = '[' + tags_series[convert_mask].str.replace(' | ', ', ', regex=False) + ']'
            logger.info(f"   âœ… Fixed Target_Style_Tags format for {int(convert_mask.sum())} rows (added brackets, changed separators)")
        else:
            logger.info("   â„¹ï¸ No pipe-delimited Target_Style_Tags found to reformat; leaving existing bracketed tags unchanged")
    
    target_label = get_period_label(target_yyyymm, target_period)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # DUAL OUTPUT PATTERN - Define both timestamped and generic versions
    timestamped_output_file = f"output/fast_fish_with_historical_and_cluster_trending_analysis_{target_label}_{timestamp}.csv"
    generic_output_file = f"output/fast_fish_with_historical_and_cluster_trending_analysis_{target_label}.csv"
    
    logger.info(f"Saving enhanced Fast Fish file to: {timestamped_output_file}")
    logger.info(f"Output contains {len(formatted_df.columns)} columns with historical + cluster trending data")
    logger.info(f"ðŸ“‹ Applied client compliance formatting fixes")
    
    # Save timestamped version (for backup/inspection)
    formatted_df.to_csv(timestamped_output_file, index=False)
    logger.info(f"âœ… Saved timestamped enhanced Fast Fish file: {timestamped_output_file}")
    
    # Create symlink for generic version (for pipeline flow)
    if os.path.exists(generic_output_file) or os.path.islink(generic_output_file):
        os.remove(generic_output_file)
    os.symlink(os.path.basename(timestamped_output_file), generic_output_file)
    logger.info(f"âœ… Created generic symlink: {generic_output_file} -> {timestamped_output_file}")
    
    # Use timestamped version for manifest registration
    output_file = timestamped_output_file
    
    # EXPLICIT FILE PATH: Register output in pipeline manifest
    # already imported at top as resilient import: register_step_output
    # Compute baseline identifiers (last year same month, same A/B period)
    baseline_yyyymm = _compute_baseline_yyyymm(target_yyyymm)
    baseline_period = target_period
    baseline_label = f"{baseline_yyyymm}{baseline_period}"

    # Determine if trending content is present and gated
    trending_present = bool('cluster_trend_score' in formatted_df.columns and TRENDING_AVAILABLE)

    out_metadata = {
        # Period-aware identifiers
        "period_label": str(target_label),
        "target_yyyymm": str(target_yyyymm),
        "target_year": int(target_yyyymm[:4]),
        "target_month": int(target_yyyymm[4:6]),
        "target_period": target_period,
        # Baseline identifiers
        "baseline_label": str(baseline_label),
        "baseline_yyyymm": str(baseline_yyyymm),
        "baseline_period": str(baseline_period),
        # Backward compatibility
        "baseline": baseline_label,
        # File info
        "file_format": "csv",
        # Size metrics
        "records": int(len(formatted_df)),
        "rows": int(len(formatted_df)),
        "columns": int(len(formatted_df.columns)),
        # Feature flags
        "includes_historical": True,
        "includes_trending": bool(trending_present),
        "trending_enabled": bool(TRENDING_ENABLED),
        "trending_available": bool(TRENDING_AVAILABLE),
        # Compliance
        "client_compliant": True,
    }
    # Generic key (most recent)
    register_step_output(
        "step17",
        "augmented_recommendations",
        output_file,
        metadata=out_metadata,
    )
    # Period-specific key
    register_step_output(
        "step17",
        f"augmented_recommendations_{target_label}",
        output_file,
        metadata=out_metadata,
    )
    logger.info("âœ… Registered output in pipeline manifest")
    
    return output_file

def print_augmentation_summary(original_df: pd.DataFrame, augmented_df: pd.DataFrame, output_file: str):
    """Print comprehensive summary of the historical + trending augmentation process."""
    
    # Calculate historical statistics
    # Some builds do not include monthly historical columns; fall back to overall historical fields or 0
    hist_col = None
    for c in [
        'Historical_SPU_Quantity_May2025',
        'historical_spu_count',
        'Historical_SPU_Quantity'
    ]:
        if c in augmented_df.columns:
            hist_col = c
            break
    if hist_col is None:
        historical_matches = 0
        new_categories = len(augmented_df)
    else:
        historical_matches = augmented_df[hist_col].notna().sum()
        new_categories = augmented_df[hist_col].isna().sum()
    
    expanding_categories = (augmented_df['SPU_Change_vs_Historical'] > 0).sum()
    contracting_categories = (augmented_df['SPU_Change_vs_Historical'] < 0).sum()
    stable_categories = (augmented_df['SPU_Change_vs_Historical'] == 0).sum()
    
    # Calculate trending statistics if available
    trending_available = 'cluster_trend_score' in augmented_df.columns and TRENDING_AVAILABLE
    if trending_available:
        high_trend_support = (augmented_df['cluster_trend_score'] >= 80).sum()
        medium_trend_support = ((augmented_df['cluster_trend_score'] >= 60) & (augmented_df['cluster_trend_score'] < 80)).sum()
        low_trend_support = (augmented_df['cluster_trend_score'] < 60).sum()
        avg_trend_score = augmented_df['cluster_trend_score'].mean()
        # Use confidence instead of missing business_priority column
        high_business_priority = (augmented_df['cluster_trend_confidence'] >= 80).sum()
    
    print(f"\n" + "="*80)
    print(f"ðŸŽ¯ ENHANCED FAST FISH AUGMENTATION SUMMARY")
    print(f"   Historical Reference + Store Group Cluster Trending Analysis")
    print(f"="*80)
    
    print(f"\nðŸ“Š BASIC METRICS:")
    print(f"  â€¢ Original recommendations: {len(original_df):,}")
    print(f"  â€¢ Enhanced recommendations: {len(augmented_df):,}")
    print(f"  â€¢ Output columns: {len(augmented_df.columns)} (Historical + Trending + Original)")
    print(f"  â€¢ Output file: {output_file}")
    
    print(f"\nðŸ“ˆ HISTORICAL REFERENCE ANALYSIS:")
    print(f"  â€¢ Found historical data: {historical_matches:,} recommendations")
    print(f"  â€¢ New categories (no historical data): {new_categories:,} recommendations")
    print(f"  â€¢ Historical match rate: {historical_matches/len(augmented_df)*100:.1f}%")
    
    print(f"\nðŸ”„ RECOMMENDATION PATTERNS vs HISTORICAL:")
    print(f"  â€¢ Expanding categories: {expanding_categories:,}")
    print(f"  â€¢ Contracting categories: {contracting_categories:,}")
    print(f"  â€¢ Stable categories: {stable_categories:,}")
    
    if trending_available:
        total_stores_analyzed = augmented_df['stores_analyzed'].sum()
        unique_store_groups = augmented_df['Store_Group_Name'].nunique()
        
        print(f"\nðŸš€ STORE GROUP AGGREGATED TRENDING ANALYSIS:")
        print(f"  â€¢ Store groups analyzed: {unique_store_groups:,}")
        print(f"  â€¢ Individual stores analyzed: {total_stores_analyzed:,}")
        print(f"  â€¢ High cluster trend support (80+): {high_trend_support:,} recommendations")
        print(f"  â€¢ Medium cluster trend support (60-79): {medium_trend_support:,} recommendations") 
        print(f"  â€¢ Low cluster trend support (<60): {low_trend_support:,} recommendations")
        print(f"  â€¢ Average cluster trend score: {avg_trend_score:.1f}")
        print(f"  â€¢ High business priority: {high_business_priority:,} recommendations")
        
        print(f"\nðŸ“‹ CLUSTER TRENDING DIMENSIONS ANALYZED:")
        print(f"  â€¢ Aggregated Sales Performance across store groups")
        print(f"  â€¢ Aggregated Weather Impact Analysis") 
        print(f"  â€¢ Store Group Cluster Performance Context")
        print(f"  â€¢ Aggregated Price Point Strategy")
        print(f"  â€¢ Aggregated Category Performance Trends")
        print(f"  â€¢ Regional Market Analysis by store group")
        print(f"  â€¢ Store Group Business Priority Scoring")
        
        print(f"\nâœ¨ ENHANCED BUSINESS VALUE:")
        print(f"  â€¢ Historical baselines for year-over-year context")
        print(f"  â€¢ Store group aggregated trending analysis for strategic insights")
        print(f"  â€¢ Real store data aggregation (not synthetic)")
        print(f"  â€¢ Business-friendly confidence scoring by cluster")
        print(f"  â€¢ Actionable priority guidance at store group level")
        print(f"  â€¢ Rich contextual rationale combining historical + cluster trends")
    else:
        print(f"\nâš ï¸ TRENDING ANALYSIS:")
        print(f"  â€¢ Trending functionality not available")
        print(f"  â€¢ Only historical reference analysis applied")
    
    print(f"\nðŸŽ¯ KEY ENHANCEMENTS vs ORIGINAL:")
    print(f"  â€¢ Historical Context: May 2025 baseline comparison")
    if trending_available:
        print(f"  â€¢ Cluster Trends: Store group aggregated 10-dimension analysis")
        print(f"  â€¢ Real Store Data: Analysis of actual stores within each group")
        print(f"  â€¢ Business Priority: Store group level data-driven scoring")
        print(f"  â€¢ Confidence Metrics: Cluster-level reliability assessment")
    print(f"  â€¢ Enhanced Rationale: Rich decision context with cluster intelligence")
    
def load_store_configuration_data(yyyymm: Optional[str] = None, period: Optional[str] = None) -> pd.DataFrame:
    """Load store configuration data with all 5 fields required for Target_Style_Tags."""
    
    try:
        # Resolve period-aware store configuration path via config helpers
        selected_cfg = None
        y = yyyymm or TARGET_YYYYMM
        p = period or TARGET_PERIOD
        try:
            api = get_api_data_files(y, p) if y and p else get_api_data_files()
            selected_cfg = api.get('store_config')
        except Exception:
            selected_cfg = None
        if not selected_cfg or not os.path.exists(selected_cfg):
            raise FileNotFoundError("Store configuration file not found via period-aware configuration")
        logger.info(f"Loading store configuration data from: {selected_cfg}")
        config_df = pd.read_csv(selected_cfg, dtype={'str_code': str})
        
        # Verify all required fields are present
        required_fields = ['big_class_name', 'sub_cate_name', 'display_location_name', 'season_name', 'sex_name']
        missing_fields = [field for field in required_fields if field not in config_df.columns]
        
        if missing_fields:
            logger.warning(f"Missing required fields in store config: {missing_fields}")
            return pd.DataFrame()
        
        # Create proper Target_Style_Tags format
        config_df['Proper_Target_Style_Tags'] = (
            config_df['big_class_name'].astype(str) + ' | ' +
            config_df['sub_cate_name'].astype(str) + ' | ' +
            config_df['display_location_name'].astype(str) + ' | ' +
            config_df['season_name'].astype(str) + ' | ' +
            config_df['sex_name'].astype(str)
        )
        
        # Create mapping for old format to new format
        config_df['Old_Target_Style_Tags'] = config_df['big_class_name'].astype(str) + ' | ' + config_df['sub_cate_name'].astype(str)
        
        logger.info(f"Loaded {len(config_df):,} store configuration records")
        logger.info(f"Created proper 5-field Target_Style_Tags format")
        
        return config_df[['str_code', 'Old_Target_Style_Tags', 'Proper_Target_Style_Tags']].drop_duplicates()
        
    except Exception as e:
        logger.error(f"Error loading store configuration data: {e}")
        return pd.DataFrame()

def enhance_target_style_tags(fast_fish_df: pd.DataFrame) -> pd.DataFrame:
    """Enhance Target_Style_Tags format from 2 fields to 5 fields using store configuration data."""
    
    logger.info("ðŸ·ï¸ Enhancing Target_Style_Tags format to include all 5 required fields...")
    
    # Load store configuration data
    config_df = load_store_configuration_data()
    
    if config_df.empty:
        logger.warning("Store configuration data not available, keeping original Target_Style_Tags format")
        return fast_fish_df
    
    # Create mapping from old format to new format
    tag_mapping = {}
    for _, row in config_df.iterrows():
        old_tag = row['Old_Target_Style_Tags']
        new_tag = row['Proper_Target_Style_Tags']
        tag_mapping[old_tag] = new_tag
    
    # Apply enhancement
    enhanced_df = fast_fish_df.copy()
    
    # Track enhancement statistics
    enhanced_count = 0
    original_count = 0
    
    for idx, row in enhanced_df.iterrows():
        original_tag = row['Target_Style_Tags']
        
        if original_tag in tag_mapping:
            enhanced_df.at[idx, 'Target_Style_Tags'] = tag_mapping[original_tag]
            enhanced_count += 1
        else:
            original_count += 1
    
    logger.info(f"âœ… Target_Style_Tags enhancement completed:")
    logger.info(f"   Enhanced: {enhanced_count:,} recommendations")
    logger.info(f"   Original format kept: {original_count:,} recommendations")
    logger.info(f"   Enhancement rate: {enhanced_count/len(enhanced_df)*100:.1f}%")
    
    # Show examples of enhanced tags
    enhanced_examples = enhanced_df[enhanced_df['Target_Style_Tags'].str.count('\\|') >= 4]['Target_Style_Tags'].head(3)
    if len(enhanced_examples) > 0:
        logger.info(f"ðŸ“ Examples of enhanced Target_Style_Tags:")
        for i, example in enumerate(enhanced_examples, 1):
            logger.info(f"   {i}. {example}")
    
    return enhanced_df

def main():
    """Step 17: Historical + Store Group Trending Analysis."""
    
    logger.info("ðŸš€ Starting Step 17: Augment Fast Fish Recommendations...")
    logger.info("   Historical Reference + Store Group Cluster Trending Analysis")

    # Parse CLI args for period-awareness
    args = _parse_args()
    target_yyyymm = args.target_yyyymm
    target_period = args.target_period
    target_label = get_period_label(target_yyyymm, target_period)
    logger.info(f"Configured target period: {target_label}")
    # Set globals for helper functions
    global TARGET_YYYYMM, TARGET_PERIOD
    TARGET_YYYYMM, TARGET_PERIOD = target_yyyymm, target_period

    try:
        # Resolve Step 14 output (period-aware) via manifest with safe fallbacks
        # get_step_input, get_manifest imported at top via resilient import
        # Check for explicit overrides first
        fast_fish_file = args.input_file or os.environ.get('STEP17_INPUT_FILE')

        if not fast_fish_file:
            # Prefer period-specific Step 14 enhanced file, then generic
            period_label = target_label
            # Try period-specific key first
            try:
                fast_fish_file = get_step_input("step14", f"enhanced_fast_fish_format_{period_label}")
            except Exception:
                fast_fish_file = None
            # Fallback to generic only if its metadata matches the requested period
            if not fast_fish_file:
                try:
                    manifest = get_manifest().manifest
                    step14_outputs = manifest.get("steps", {}).get("step14", {}).get("outputs", {})
                    generic_path = step14_outputs.get("enhanced_fast_fish_format", {}).get("file_path")
                    meta = step14_outputs.get("enhanced_fast_fish_format", {}).get("metadata", {})
                    if meta.get("target_year") == int(target_yyyymm[:4]) and meta.get("target_month") == int(target_yyyymm[4:6]) and meta.get("target_period") == target_period:
                        fast_fish_file = generic_path
                except Exception:
                    fast_fish_file = None
            if not fast_fish_file:
                logger.error("âŒ No Step 14 enhanced Fast Fish file found via manifest for the requested period")
                logger.error("ðŸ’¡ Ensure Step 14 ran and registered 'enhanced_fast_fish_format' with period-specific key")
                raise FileNotFoundError("Fast Fish file not found for requested period")
        
        logger.info(f"âœ… Using Fast Fish file: {fast_fish_file}")
        fast_fish_df = pd.read_csv(fast_fish_file)
        logger.info(f"Loaded {len(fast_fish_df):,} Fast Fish recommendations")
        
        # Preserve Target_Style_Tags (no mutation)

        # Apply CLI trending flag to globals (post-import) without enabling synthetic paths
        try:
            global TRENDING_ENABLED, TRENDING_AVAILABLE
            if args.enable_trending:
                TRENDING_ENABLED = True
            TRENDING_AVAILABLE = (TRENDING_ENABLED and ANALYZER_IMPORT_OK)
            logger.info(f"Trending gate after CLI: enabled={TRENDING_ENABLED}, available={TRENDING_AVAILABLE}")
        except Exception:
            pass
        
        # Load Step 15 historical reference and build lookup
        historical_ref_df = load_step15_historical(target_yyyymm, target_period)
        if historical_ref_df is not None:
            required_cols = {"Store_Group_Name", "Subcategory", "Historical_SPU_Quantity", "Historical_Total_Sales", "Historical_Store_Count"}
            if not required_cols.issubset(historical_ref_df.columns):
                logger.warning("Step 15 historical reference missing expected columns; skipping historical augmentation")
                historical_lookup_df = pd.DataFrame()
            else:
                historical_lookup_df = historical_ref_df[["Store_Group_Name", "Subcategory", "Historical_SPU_Quantity", "Historical_Total_Sales", "Historical_Store_Count"]].copy()
                # Normalize expected field names for downstream use
                try:
                    historical_lookup_df["store_group"] = historical_lookup_df["Store_Group_Name"]
                    # Step 17 uses 'lookup_key' aligned to Subcategory lookup
                    historical_lookup_df["lookup_key"] = historical_lookup_df["Subcategory"]
                    historical_lookup_df["historical_spu_count"] = historical_lookup_df["Historical_SPU_Quantity"]
                    historical_lookup_df["historical_total_sales"] = historical_lookup_df["Historical_Total_Sales"]
                    historical_lookup_df["historical_store_count"] = historical_lookup_df["Historical_Store_Count"]
                except Exception as e:
                    logger.warning(f"Failed to normalize historical lookup column names: {e}")
        else:
            historical_lookup_df = pd.DataFrame()
        
        # Convert to dictionary for faster lookups
        historical_lookup = {}
        for _, row in historical_lookup_df.iterrows():
            key = (row['store_group'], row['lookup_key'])  # Using lookup_key which matches Target_Style_Tags format
            historical_lookup[key] = {
                'spu_quantity': row['historical_spu_count'],
                'total_sales': row['historical_total_sales'],
                'store_count': row['historical_store_count']
            }
        
        # Augment Fast Fish recommendations with historical + store group trending analysis
        logger.info("ðŸŽ¯ Applying comprehensive augmentation (Historical + Store Group Trending)...")
        augmented_df = augment_fast_fish_recommendations(fast_fish_df, historical_lookup)
        
        # FINAL POST-PROCESSING: Ensure no NaN values in trend columns
        # Keep numeric data type for calculations but add explanation columns for all data
        trend_columns = [
            'cluster_trend_score', 'cluster_trend_confidence', 'stores_analyzed',
            'trend_sales_performance', 'trend_weather_impact', 'trend_cluster_performance',
            'trend_price_strategy', 'trend_category_performance', 'trend_regional_analysis',
            'trend_fashion_indicators', 'trend_seasonal_patterns', 'trend_inventory_turnover',
            'trend_customer_behavior'
        ]
        
        # Check if trending data is available
        trending_available = 'cluster_trend_score' in augmented_df.columns
        
        # Create explanation content for each trend column
        # Always create explanation data for all rows, regardless of trending availability
        explanation_data = {}
        for idx, row in augmented_df.iterrows():
            # Use the trending_available variable defined in the main function scope
            if trending_available and 'cluster_trend_score' in augmented_df.columns and pd.notna(row['cluster_trend_score']):
                # Create detailed explanations for each trend dimension when data is available
                explanation_data[idx] = {
                    'cluster_trend_score_explanation': f"Overall cluster trend score: {row['cluster_trend_score']} (confidence: {row['cluster_trend_confidence']}%)",
                    'cluster_trend_confidence_explanation': f"Confidence level in cluster trend analysis: {row['cluster_trend_confidence']}%",
                    'stores_analyzed_explanation': f"Number of stores analyzed for this cluster: {row['stores_analyzed']}",
                    'trend_sales_performance_explanation': f"Sales performance trend score: {row['trend_sales_performance']}/100",
                    'trend_weather_impact_explanation': f"Weather impact on sales trend score: {row['trend_weather_impact']}/100",
                    'trend_cluster_performance_explanation': f"Cluster performance trend score: {row['trend_cluster_performance']}/100",
                    'trend_price_strategy_explanation': f"Price strategy effectiveness trend score: {row['trend_price_strategy']}/100",
                    'trend_category_performance_explanation': f"Category performance trend score: {row['trend_category_performance']}/100",
                    'trend_regional_analysis_explanation': f"Regional market analysis trend score: {row['trend_regional_analysis']}/100",
                    'trend_fashion_indicators_explanation': f"Fashion indicators trend score: {row['trend_fashion_indicators']}/100",
                    'trend_seasonal_patterns_explanation': f"Seasonal patterns trend score: {row['trend_seasonal_patterns']}/100",
                    'trend_inventory_turnover_explanation': f"Inventory turnover trend score: {row['trend_inventory_turnover']}/100",
                    'trend_customer_behavior_explanation': f"Customer behavior trend score: {row['trend_customer_behavior']}/100"
                }
            else:
                # Create default explanations when no trend data is available
                explanation_data[idx] = {
                    'cluster_trend_score_explanation': "Trend analysis not available for this cluster",
                    'cluster_trend_confidence_explanation': "Confidence level not applicable - trend analysis not available",
                    'stores_analyzed_explanation': "Store analysis not available - trend analysis not available",
                    'trend_sales_performance_explanation': "Sales performance trend not calculated - trend analysis not available",
                    'trend_weather_impact_explanation': "Weather impact trend not calculated - trend analysis not available",
                    'trend_cluster_performance_explanation': "Cluster performance trend not calculated - trend analysis not available",
                    'trend_price_strategy_explanation': "Price strategy trend not calculated - trend analysis not available",
                    'trend_category_performance_explanation': "Category performance trend not calculated - trend analysis not available",
                    'trend_regional_analysis_explanation': "Regional analysis trend not calculated - trend analysis not available",
                    'trend_fashion_indicators_explanation': "Fashion indicators trend not calculated - trend analysis not available",
                    'trend_seasonal_patterns_explanation': "Seasonal patterns trend not calculated - trend analysis not available",
                    'trend_inventory_turnover_explanation': "Inventory turnover trend not calculated - trend analysis not available",
                    'trend_customer_behavior_explanation': "Customer behavior trend not calculated - trend analysis not available"
                }
        
        for col in trend_columns:
            if col in augmented_df.columns:
                # Create explanation column for this trend column
                explanation_col = f"{col}_explanation"
                # Initialize all values as empty strings
                augmented_df[explanation_col] = ""
                
                # Add detailed explanations for all rows
                for idx, row in augmented_df.iterrows():
                    if idx in explanation_data and f"{col}_explanation" in explanation_data[idx]:
                        augmented_df.loc[idx, explanation_col] = explanation_data[idx][f"{col}_explanation"]
                
                # Handle NaN values in the trend column
                nan_mask = pd.isna(augmented_df[col])
                nan_count = nan_mask.sum()
                if nan_count > 0:
                    logger.warning(f"Found {nan_count} NaN values in {col} after all processing - leaving as NA")
                    # Leave existing per-row explanations intact; do not insert synthetic placeholder text
                else:
                    logger.info(f"No NaN values found in {col} - no replacement needed")
                
                # Ensure all values in explanation column are strings (no NaN values)
                augmented_df[explanation_col] = augmented_df[explanation_col].fillna("")
                augmented_df[explanation_col] = augmented_df[explanation_col].astype(str)
        
        # Save enhanced file
        logger.info("ðŸ’¾ Saving enhanced Fast Fish recommendations...")
        output_file = save_augmented_file(augmented_df, target_yyyymm, target_period)
        
        # Print comprehensive summary
        print_augmentation_summary(fast_fish_df, augmented_df, output_file)
        
        logger.info("âœ… Step 17: Augment Fast Fish Recommendations completed successfully!")
        logger.info(f"ðŸ“ Output: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"âŒ Error in Step 17: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise

def format_trend_summary(trend_data: dict) -> str:
    """Format trend data into a human-readable summary string."""
    if not trend_data:
        return "No trend data available"
    
    cluster_score = trend_data.get('overall_score', 0)
    cluster_confidence = trend_data.get('confidence', 0)
    stores_analyzed = trend_data.get('stores_analyzed', 0)
    
    # Determine trend level and emoji
    if cluster_score >= 70:
        trend_level = "STRONG CLUSTER TREND"
        trend_emoji = "ðŸš€"
    elif cluster_score >= 50:
        trend_level = "MODERATE CLUSTER TREND"
        trend_emoji = "ðŸ“Š"
    else:
        trend_level = "WEAK CLUSTER TREND"
        trend_emoji = "âš ï¸"
    
    # Create detailed trend breakdown
    trend_details = [
        f"{trend_emoji} {trend_level} (Score: {cluster_score}, Confidence: {cluster_confidence}%)",
        f"â€¢ Stores Analyzed: {stores_analyzed}",
        f"â€¢ Sales Performance: {trend_data.get('sales_performance', 50)}/100",
        f"â€¢ Weather Impact: {trend_data.get('weather_impact', 50)}/100",
        f"â€¢ Cluster Performance: {trend_data.get('cluster_performance', 50)}/100",
        f"â€¢ Price Strategy: {trend_data.get('price_strategy', 60)}/100",
        f"â€¢ Category Performance: {trend_data.get('category_performance', 50)}/100",
        f"â€¢ Regional Analysis: {trend_data.get('regional_analysis', 60)}/100",
        f"â€¢ Fashion Indicators: {trend_data.get('fashion_indicators', 60)}/100",
        f"â€¢ Seasonal Patterns: {trend_data.get('seasonal_patterns', 50)}/100",
        f"â€¢ Inventory Turnover: {trend_data.get('inventory_turnover', 55)}/100",
        f"â€¢ Customer Behavior: {trend_data.get('customer_behavior', 60)}/100",
    ]
    
    return " | ".join(trend_details)

def get_dominant_trend(trend_data: dict) -> str:
    """Determine the dominant trend from trend data."""
    if not trend_data:
        return "Unknown"
    
    # Find the highest scoring trend dimension
    trend_dimensions = {
        'Sales Performance': trend_data.get('sales_performance', 50),
        'Weather Impact': trend_data.get('weather_impact', 50),
        'Cluster Performance': trend_data.get('cluster_performance', 50),
        'Category Performance': trend_data.get('category_performance', 50),
        'Fashion Indicators': trend_data.get('fashion_indicators', 60),
        'Seasonal Patterns': trend_data.get('seasonal_patterns', 50)
    }
    
    if trend_dimensions:
        dominant = max(trend_dimensions, key=trend_dimensions.get)
        score = trend_dimensions[dominant]
        return f"{dominant} ({score})"
    else:
        return "Unknown"

if __name__ == "__main__":
    main()
