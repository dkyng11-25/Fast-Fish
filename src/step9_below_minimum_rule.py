#!/usr/bin/env python3
"""
Step 9: Below Minimum Rule - FIXED VERSION
==========================================

CRITICAL FIXES APPLIED:
1. ‚úÖ No more negative quantities (below minimum should INCREASE, not decrease)
2. ‚úÖ Proper business logic alignment
3. ‚úÖ Simplified data processing (no heavy file loading)
4. ‚úÖ Fast execution for testing

This version takes the current Rule 9 logic and applies the key fixes while
maintaining compatibility with the existing pipeline.

Author: Data Pipeline Team  
Date: 2025-07-23 (Critical Fix Version)
"""

import pandas as pd
import numpy as np
import os
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import warnings
from tqdm import tqdm
import argparse
import json
import ast

# Config-driven paths and periods
from src.config import (
    initialize_pipeline_config,
    get_current_period,
    get_api_data_files,
    get_output_files,
    get_period_label,
)
from src.pipeline_manifest import register_step_output, get_step_input
from src.output_utils import create_output_with_symlinks

# FAST FISH ENHANCEMENT: Import sell-through validation
try:
    # Prefer absolute import so running as a script with PYTHONPATH=. works
    from src.sell_through_validator import (
        SellThroughValidator,
        load_historical_data_for_validation,
    )
    SELLTHROUGH_VALIDATION_AVAILABLE = True
    print("‚úÖ Fast Fish sell-through validation: ENABLED (absolute)")
except Exception:
    try:
        # Fallback to relative import when executed as part of the src package
        from .sell_through_validator import (
            SellThroughValidator,
            load_historical_data_for_validation,
        )
        SELLTHROUGH_VALIDATION_AVAILABLE = True
        print("‚úÖ Fast Fish sell-through validation: ENABLED (relative)")
    except Exception as e:
        SELLTHROUGH_VALIDATION_AVAILABLE = False
        print(f"‚ö†Ô∏è Fast Fish sell-through validation: DISABLED (validator not found) - {e}")

warnings.filterwarnings('ignore')

# Configuration
ANALYSIS_LEVEL = "spu"
DATA_PERIOD_DAYS = 15
TARGET_PERIOD_DAYS = 15
SCALING_FACTOR = TARGET_PERIOD_DAYS / DATA_PERIOD_DAYS

# File paths (resolved dynamically via src.config)
OUTPUT_DIR = "output"

def log_progress(message: str) -> None:
    """Log progress with timestamp."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

# Seasonal blending controls (overridden via CLI)
USE_BLENDED_SEASONAL: bool = False
SEASONAL_YYYYMM: Optional[str] = None
SEASONAL_YYYYMM_LIST: Optional[List[str]] = None
SEASONAL_PERIOD: Optional[str] = None
SEASONAL_WEIGHT: float = 0.6
RECENT_WEIGHT: float = 0.4  # Typically 1 - seasonal_weight

# FIXED BUSINESS LOGIC: Below minimum should INCREASE allocation
MINIMUM_STYLE_THRESHOLD = 0.03  # Reduced from 0.05 for more inclusivity (3% vs 5%)
MIN_BOOST_QUANTITY = 0.5  # Reduced minimum increase for smaller adjustments
NEVER_DECREASE_BELOW_MINIMUM = True  # CRITICAL: Never recommend decreases

# Unit-based threshold and caps
MINIMUM_UNIT_RATE = 1.0  # Minimum units per 15 days when > 0
MAX_TOTAL_ADJUSTMENTS_PER_STORE = None  # No artificial cap by default; can be provided via CLI
MIN_SALES_UNITS = 0.1  # Minimum observed units to consider an SPU active

# Default unit price (preserve NA when unknown; do not synthesize)
UNIT_PRICE_DEFAULT = 50.0

# Create output directory
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Diagnostics (populated during runtime for summary reporting)
LAST_SPU_SALES_FILE_USED: Optional[str] = None
UNIT_RATE_NA_COUNT: int = 0
UNIT_DIAGNOSTICS_TOTAL_ROWS: int = 0

def blend_seasonal_data(recent_df: pd.DataFrame, seasonal_df: pd.DataFrame, 
                       recent_weight: float, seasonal_weight: float, data_type: str) -> pd.DataFrame:
    """
    Blend recent trends data with seasonal patterns using weighted aggregation.
    
    Args:
        recent_df: Recent trends data (May-July 2025)
        seasonal_df: Seasonal patterns data (August 2024)
        recent_weight: Weight for recent data (e.g., 0.4 = 40%)
        seasonal_weight: Weight for seasonal data (e.g., 0.6 = 60%)
        data_type: Type of data being blended ("config" or "quantity")
        
    Returns:
        Blended DataFrame with weighted aggregation
    """
    log_progress(f"üîÑ Blending {data_type} data: {recent_weight:.0%} recent + {seasonal_weight:.0%} seasonal")
    
    # Identify common columns for blending
    if data_type == "config":
        # For config data, blend on store and category dimensions
        blend_cols = ['str_code', 'season_name', 'sex_name', 'display_location_name', 'big_class_name', 'sub_cate_name']
        value_cols = ['sty_sal_amt']  # Main column to blend
    else:
        # For quantity data, blend on store and SPU dimensions
        blend_cols = ['str_code', 'spu_code']
        value_cols = ['spu_sales_amt', 'quantity'] if 'quantity' in recent_df.columns else ['spu_sales_amt']
    
    # Filter to common blend columns that exist in both datasets
    available_blend_cols = [col for col in blend_cols if col in recent_df.columns and col in seasonal_df.columns]
    available_value_cols = [col for col in value_cols if col in recent_df.columns and col in seasonal_df.columns]
    
    if not available_blend_cols or not available_value_cols:
        log_progress(f"‚ö†Ô∏è Insufficient common columns for blending {data_type} data, using recent data only")
        return recent_df
    
    log_progress(f"   Blending on: {available_blend_cols}")
    log_progress(f"   Value columns: {available_value_cols}")
    
    # Prepare recent data with weights
    recent_clean = recent_df[available_blend_cols + available_value_cols].copy()
    for col in available_value_cols:
        if col == 'sty_sal_amt':  # Handle JSON string column
            continue  # Skip weighting for JSON columns, handle separately
        else:
            recent_clean[col] = pd.to_numeric(recent_clean[col], errors='coerce').fillna(0) * recent_weight
    
    # Prepare seasonal data with weights
    seasonal_clean = seasonal_df[available_blend_cols + available_value_cols].copy()
    for col in available_value_cols:
        if col == 'sty_sal_amt':  # Handle JSON string column
            continue  # Skip weighting for JSON columns, handle separately
        else:
            seasonal_clean[col] = pd.to_numeric(seasonal_clean[col], errors='coerce').fillna(0) * seasonal_weight
    
    # For config data, combine both datasets (JSON columns need special handling)
    if data_type == "config":
        # Combine datasets and let prepare_spu_data handle the JSON parsing
        combined = pd.concat([recent_clean, seasonal_clean], ignore_index=True)
        log_progress(f"   ‚úÖ Combined {len(combined):,} records from {len(recent_df):,} recent + {len(seasonal_df):,} seasonal")
        return combined
    else:
        # For quantity data, aggregate numerically
        combined = pd.concat([recent_clean, seasonal_clean], ignore_index=True)
        agg_dict = {col: 'sum' for col in available_value_cols if col != 'sty_sal_amt'}
        if agg_dict:  # Only aggregate if we have numeric columns
            blended = combined.groupby(available_blend_cols).agg(agg_dict).reset_index()
        else:
            blended = combined.drop_duplicates(subset=available_blend_cols)
        
        # Add back any additional columns from recent data that weren't blended
        additional_cols = [col for col in recent_df.columns if col not in available_blend_cols + available_value_cols]
        if additional_cols:
            recent_extra = recent_df[available_blend_cols + additional_cols].drop_duplicates(subset=available_blend_cols)
            blended = blended.merge(recent_extra, on=available_blend_cols, how='left')
        
        log_progress(f"   ‚úÖ Blended {len(blended):,} records from {len(recent_df):,} recent + {len(seasonal_df):,} seasonal")
        return blended

def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Load clustering results and data with enhanced seasonal blending for August recommendations.
    """
    try:
        log_progress("Loading cluster assignments...")
        yyyymm, period = get_current_period()
        cluster_path = get_output_files(ANALYSIS_LEVEL, yyyymm, period)['clustering_results']
        candidates = [
            cluster_path,
            os.path.join(OUTPUT_DIR, "clustering_results_spu.csv"),
            os.path.join(OUTPUT_DIR, "enhanced_clustering_results.csv"),
            os.path.join(OUTPUT_DIR, "clustering_results.csv"),
        ]
        cluster_path_final = next((p for p in candidates if os.path.exists(p)), None)
        if cluster_path_final is None:
            raise FileNotFoundError(
                f"Clustering results not found. Checked: {', '.join(candidates)}"
            )
        log_progress(f"Cluster file: {cluster_path_final}")
        cluster_df = pd.read_csv(cluster_path_final, dtype={'str_code': str})
        log_progress(f"Loaded cluster assignments for {len(cluster_df)} stores")
        # Normalize/mirror cluster columns for downstream compatibility
        if 'Cluster' not in cluster_df.columns and 'cluster_id' in cluster_df.columns:
            cluster_df['Cluster'] = cluster_df['cluster_id']
            log_progress("Detected 'cluster_id' without 'Cluster'; created 'Cluster' for compatibility")
        if 'cluster_id' not in cluster_df.columns and 'Cluster' in cluster_df.columns:
            cluster_df['cluster_id'] = cluster_df['Cluster']
            log_progress("Detected 'Cluster' without 'cluster_id'; created 'cluster_id' for standardized outputs")
        
        # SEASONAL BLENDING ENHANCEMENT: Load and blend data when enabled
        if USE_BLENDED_SEASONAL:
            log_progress("üçÇ BLENDED SEASONAL LOADING: Combining recent trends + seasonal patterns")
            period_label = get_period_label(yyyymm, period)
            recent_primary = get_api_data_files(yyyymm, period)['store_config']
            recent_candidates = [
                recent_primary,
                os.path.join(OUTPUT_DIR, f"store_config_{period_label}.csv"),
            ]
            recent_path = next((p for p in recent_candidates if os.path.exists(p)), None)
            if not recent_path:
                raise FileNotFoundError(f"Store config not found. Checked: {', '.join(recent_candidates)}")
            log_progress(f"Loading recent trends from {recent_path}")
            recent_data = pd.read_csv(recent_path, dtype={'str_code': str}, low_memory=False)
            log_progress(f"Recent trends: {len(recent_data)} config records")

            # Resolve multi-year seasonal frames (same month/period for prior N years)
            seasonal_frames: List[pd.DataFrame] = []
            added_labels: List[str] = []
            years_back_env = os.environ.get("SEASONAL_YEARS_BACK")
            # Allow explicit list override for seasonal months (comma-separated YYYYMM)
            list_env = os.environ.get("SEASONAL_YYYYMM_LIST")
            s_period = SEASONAL_PERIOD or period

            if list_env:
                try:
                    explicit_months = [s.strip() for s in list_env.split(',') if s.strip()]
                except Exception:
                    explicit_months = []
                for yyyymm_i in explicit_months:
                    try:
                        files_i = get_api_data_files(yyyymm_i, s_period)
                        cfg_i = files_i.get('store_config')
                        if cfg_i and os.path.exists(cfg_i):
                            df_i = pd.read_csv(cfg_i, dtype={'str_code': str}, low_memory=False)
                            seasonal_frames.append(df_i)
                            added_labels.append(get_period_label(yyyymm_i, s_period))
                    except Exception:
                        continue
            else:
                try:
                    years_back = int(years_back_env) if years_back_env and years_back_env.isdigit() else 2
                except Exception:
                    years_back = 2
                if years_back < 1:
                    years_back = 1

                # Base derived from current configured period unless overridden by SEASONAL_YYYYMM
                base_year = int(str(yyyymm)[:4])
                base_month = int(str(yyyymm)[4:6])

                for i in range(1, years_back + 1):
                    y = base_year - i
                    yyyymm_i = f"{y}{base_month:02d}"
                    if SEASONAL_YYYYMM and i == 1:
                        yyyymm_i = SEASONAL_YYYYMM
                    try:
                        files_i = get_api_data_files(yyyymm_i, s_period)
                        cfg_i = files_i.get('store_config')
                        if cfg_i and os.path.exists(cfg_i):
                            df_i = pd.read_csv(cfg_i, dtype={'str_code': str}, low_memory=False)
                            seasonal_frames.append(df_i)
                            added_labels.append(get_period_label(yyyymm_i, s_period))
                    except Exception:
                        continue

            if seasonal_frames:
                seasonal_weight_total = float(os.environ.get("SEASONAL_WEIGHT", SEASONAL_WEIGHT))
                recent_weight = 1.0 - seasonal_weight_total
                per_year_weight = seasonal_weight_total / max(1, len(seasonal_frames))
                log_progress(
                    f"üîÑ BLENDING: {recent_weight:.0%} recent + {seasonal_weight_total:.0%} seasonal across {len(seasonal_frames)} year(s): {added_labels}"
                )
                data_df = recent_data.copy()
                for df_season in seasonal_frames:
                    data_df = blend_seasonal_data(
                        data_df,
                        df_season,
                        recent_weight=recent_weight,
                        seasonal_weight=per_year_weight,
                        data_type="config",
                    )
                    # After first blend, keep accumulated weights stable
                    recent_weight = 1.0
                log_progress(f"‚úÖ BLENDED RESULT: {len(data_df)} config records")
            else:
                log_progress("‚ö†Ô∏è FALLBACK: Using recent data only (seasonal data unavailable)")
                data_df = recent_data
        else:
            # Standard single-dataset loading
            log_progress("Loading store configuration data...")
            period_label = get_period_label(yyyymm, period)
            primary = get_api_data_files(yyyymm, period)['store_config']
            candidates = [
                primary,
                os.path.join(OUTPUT_DIR, f"store_config_{period_label}.csv"),
            ]
            data_path = next((p for p in candidates if os.path.exists(p)), None)
            if not data_path:
                raise FileNotFoundError(f"Store config not found. Checked: {', '.join(candidates)}")
            log_progress(f"Using store configuration file: {data_path}")
            data_df = pd.read_csv(data_path, dtype={'str_code': str}, low_memory=False)
            log_progress(f"Loaded data with {len(data_df)} rows")
        
        # Ensure str_code is string for consistent joining
        cluster_df['str_code'] = cluster_df['str_code'].astype(str)
        data_df['str_code'] = data_df['str_code'].astype(str)
        
        return cluster_df, data_df
        
    except Exception as e:
        log_progress(f"Error loading data: {str(e)}")
        raise

def parse_sty_sal_amt(value: str) -> Optional[dict]:
    """Safely parse JSON-like SPU sales dict from string."""
    try:
        if value is None or (isinstance(value, float) and np.isnan(value)) or value == "":
            return None
        try:
            return json.loads(value)
        except Exception:
            return ast.literal_eval(value)
    except Exception:
        return None

def prepare_spu_data(data_df: pd.DataFrame, cluster_df: pd.DataFrame) -> pd.DataFrame:
    """Prepare SPU data with simplified processing."""
    log_progress("Preparing SPU data with simplified processing...")
    
    # Merge with cluster information
    data_with_clusters = data_df.merge(
        cluster_df[['str_code', 'Cluster']], on='str_code', how='left', indicator=True
    )
    matched = (data_with_clusters['_merge'] == 'both').sum()
    unmatched = (data_with_clusters['_merge'] == 'left_only').sum()
    log_progress(
        f"Merged store config with clusters (left join): matched={matched:,}, unmatched={unmatched:,}"
    )
    # Drop merge indicator to avoid carrying diagnostics forward
    data_with_clusters = data_with_clusters.drop(columns=['_merge'])
    
    # Filter records with SPU sales data - sample for performance
    spu_records = data_df[data_df['sty_sal_amt'].notna() & (data_df['sty_sal_amt'] != '')].copy()
    
    # PERFORMANCE FIX: Sample for testing if too large
    if len(spu_records) > 10000:
        log_progress(f"Sampling {10000} records for performance (from {len(spu_records):,})")
        spu_records = spu_records.sample(n=10000, random_state=42)
    
    log_progress(f"Processing {len(spu_records)} records with SPU sales data")
    
    # Simplified SPU expansion
    expanded_records = []
    
    for idx, row in spu_records.iterrows():
        try:
            spu_data = parse_sty_sal_amt(row['sty_sal_amt'])  # Safe parse JSON-like string
            if isinstance(spu_data, dict):
                for sty_code, sal_amt in spu_data.items():
                    if sal_amt > 0:  # Only consider SPUs with sales
                        expanded_record = {
                            'str_code': row['str_code'],
                            'season_name': row.get('season_name', ''),
                            'sex_name': row.get('sex_name', ''),
                            'display_location_name': row.get('display_location_name', ''),
                            'big_class_name': row.get('big_class_name', ''),
                            'sub_cate_name': row.get('sub_cate_name', ''),
                            'sty_code': sty_code,
                            # Preserve monetary sales for diagnostics only; NOT used for quantities
                            'spu_sales_amt': float(sal_amt)
                        }
                        expanded_records.append(expanded_record)
        except:
            continue
        
        # Progress update
        if idx % 1000 == 0:
            log_progress(f"Processed {idx:,} records, generated {len(expanded_records)} SPU records")
    
    if not expanded_records:
        log_progress("No valid SPU sales data found")
        return pd.DataFrame()
    
    # Convert to DataFrame
    expanded_df = pd.DataFrame(expanded_records)
    log_progress(f"Expanded to {len(expanded_df)} SPU-level records")
    
    # Merge with cluster information (preserve stores without cluster; report diagnostics)
    store_data = expanded_df.merge(
        cluster_df[['str_code', 'Cluster']], on='str_code', how='left', indicator=True
    )
    matched_sd = (store_data['_merge'] == 'both').sum()
    unmatched_sd = (store_data['_merge'] == 'left_only').sum()
    log_progress(
        f"SPU expansion merged with clusters (left join): matched={matched_sd:,}, unmatched={unmatched_sd:,}"
    )
    store_data = store_data.drop(columns=['_merge'])

    # Join real unit quantities from SPU sales file (period-aware, inputs use CURRENT analysis period)
    # Target period (if provided) is ONLY for output naming; do not use it for input resolution.
    cur_yyyymm, cur_period = get_current_period()
    yyyymm = cur_yyyymm
    period = cur_period
    period_label = get_period_label(yyyymm, period)

    spu_primary = get_api_data_files(yyyymm, period)['spu_sales']
    spu_candidates = [
        spu_primary,
        os.path.join(OUTPUT_DIR, f"complete_spu_sales_{period_label}.csv"),
    ]
    spu_path = next((p for p in spu_candidates if os.path.exists(p)), None)
    if not spu_path:
        raise FileNotFoundError(
            f"SPU sales (units) file not found. Checked: {', '.join(spu_candidates)}"
        )
    log_progress(f"Using SPU sales (units) file: {spu_path}")
    spu_df = pd.read_csv(spu_path, dtype={'str_code': str}, low_memory=False)
    # Record diagnostics
    global LAST_SPU_SALES_FILE_USED
    LAST_SPU_SALES_FILE_USED = spu_path

    # Ensure expected columns exist
    if 'spu_code' not in spu_df.columns:
        # Some files may use 'sty_code' instead of 'spu_code'
        if 'sty_code' in spu_df.columns:
            spu_df = spu_df.rename(columns={'sty_code': 'spu_code'})
        else:
            raise KeyError("SPU sales file missing 'spu_code' column")

    # Coerce numeric fields
    if 'quantity' in spu_df.columns:
        spu_df['quantity'] = pd.to_numeric(spu_df['quantity'], errors='coerce')
    else:
        # Fail fast: we require real unit quantities
        raise KeyError("SPU sales file missing 'quantity' column (real units)")

    # Merge quantities and unit_price
    merge_cols = ['str_code']
    right_cols = ['str_code', 'spu_code', 'quantity'] + (["unit_price"] if 'unit_price' in spu_df.columns else [])
    store_data = store_data.merge(
        spu_df[right_cols], left_on=['str_code', 'sty_code'], right_on=['str_code', 'spu_code'], how='left'
    )

    # Compute unit-based rate and maintain backward-compat column alias
    store_data['unit_rate'] = pd.to_numeric(store_data['quantity'], errors='coerce') * SCALING_FACTOR
    # Backward compatibility: retain 'style_count' as alias to unit_rate
    store_data['style_count'] = store_data['unit_rate']

    # Diagnostics
    na_qty = store_data['unit_rate'].isna().sum()
    log_progress(f"Joined unit quantities: {len(store_data)} rows; NA unit_rate: {na_qty}")
    # Record diagnostics for summary
    global UNIT_RATE_NA_COUNT, UNIT_DIAGNOSTICS_TOTAL_ROWS
    UNIT_RATE_NA_COUNT = int(na_qty)
    UNIT_DIAGNOSTICS_TOTAL_ROWS = int(len(store_data))

    return store_data

def prepare_subcategory_data(data_df: pd.DataFrame, cluster_df: pd.DataFrame) -> pd.DataFrame:
    """Prepare subcategory data using real planning counts only (no synthetic units)."""
    log_progress("Preparing subcategory data with planning counts...")
    data_with_clusters = data_df.merge(
        cluster_df[['str_code', 'Cluster']], on='str_code', how='left', indicator=True
    )
    matched = (data_with_clusters['_merge'] == 'both').sum()
    unmatched = (data_with_clusters['_merge'] == 'left_only').sum()
    log_progress(
        f"Merged store config with clusters (left join): matched={matched:,}, unmatched={unmatched:,}"
    )
    data_with_clusters = data_with_clusters.drop(columns=['_merge'])
    # Real planning signal
    data_with_clusters['target_sty_cnt_avg'] = pd.to_numeric(
        data_with_clusters.get('target_sty_cnt_avg'), errors='coerce'
    )
    data_with_clusters['style_count'] = data_with_clusters['target_sty_cnt_avg']
    # Category key
    grouping_cols = ['season_name', 'sex_name', 'display_location_name', 'big_class_name', 'sub_cate_name']
    available = [c for c in grouping_cols if c in data_with_clusters.columns]
    if available:
        data_with_clusters['category_key'] = data_with_clusters[available].apply(
            lambda x: '|'.join(x.astype(str)), axis=1
        )
    else:
        data_with_clusters['category_key'] = data_with_clusters['sub_cate_name'].astype(str)
    # Filter positive counts
    store_data = data_with_clusters[data_with_clusters['style_count'] > 0].copy()
    log_progress(f"Prepared {len(store_data)} store-subcategory records")
    return store_data

def identify_below_minimum_cases(store_data: pd.DataFrame) -> pd.DataFrame:
    """
    Identify cases that are below minimum threshold with FAST FISH sell-through validation.
    
    Args:
        store_data: Store data with style counts
        
    Returns:
        DataFrame with below minimum cases (only Fast Fish compliant)
    """
    feature_type = "subcategories" if ANALYSIS_LEVEL == "subcategory" else "SPUs"
    log_progress(f"Identifying below minimum {ANALYSIS_LEVEL} cases...")
    
    # Find cases where unit rate is positive but below minimum threshold (units, not money)
    # APPLY ADDITIONAL FILTERS FOR SPU LEVEL TO REDUCE EXCESSIVE FLAGGING
    if ANALYSIS_LEVEL == "spu":
        # Add unit volume filter and cluster size filter for SPU level (relaxed for more opportunities)
        units_ok = store_data['unit_rate'].ge(MIN_SALES_UNITS).fillna(False)
        cluster_size = store_data.groupby('Cluster', dropna=True)['str_code'].transform('count')
        cluster_ok = cluster_size.ge(2).reindex(store_data.index, fill_value=False)
        below_minimum = store_data[
            (store_data['unit_rate'] > 0)
            & (store_data['unit_rate'] < MINIMUM_UNIT_RATE)
            & units_ok
            & cluster_ok
        ].copy()
        log_progress(f"Applied additional filters: min units {MIN_SALES_UNITS}, min cluster size 2")
    else:
        # Original logic for subcategory level
        below_minimum = store_data[
            (store_data['unit_rate'] > 0) & 
            (store_data['unit_rate'] < MINIMUM_UNIT_RATE)
        ].copy()
    
    if len(below_minimum) == 0:
        log_progress("No below minimum cases found")
        return pd.DataFrame()
    
    # Calculate recommended target (minimum viable units per 15 days)
    below_minimum['recommended_target'] = MINIMUM_UNIT_RATE
    below_minimum['increase_needed'] = below_minimum['recommended_target'] - below_minimum['unit_rate']
    
    # Add issue classification
    below_minimum['issue_type'] = 'BELOW_MINIMUM'
    
    # Calculate severity based on analysis level
    if ANALYSIS_LEVEL == "subcategory":
        # Traditional thresholds for subcategory
        below_minimum['issue_severity'] = np.where(
            below_minimum['unit_rate'] < 0.5 * MINIMUM_UNIT_RATE, 'HIGH', 'MEDIUM'
        )
    else:
        # Adjusted thresholds for SPU (more granular)
        below_minimum['issue_severity'] = np.where(
            below_minimum['unit_rate'] < 0.25 * MINIMUM_UNIT_RATE, 'CRITICAL',
            np.where(below_minimum['unit_rate'] < 0.50 * MINIMUM_UNIT_RATE, 'HIGH', 
                     np.where(below_minimum['unit_rate'] < 0.75 * MINIMUM_UNIT_RATE, 'MEDIUM', 'LOW'))
        )
    
    # FIXED ENHANCEMENT: Add positive quantity recommendations
    below_minimum['current_quantity'] = below_minimum['unit_rate']
    below_minimum['target_quantity'] = below_minimum['recommended_target']
    
    # CRITICAL FIX: Ensure ALL quantity changes are POSITIVE
    # Compute raw delta, then ceil to enforce integer units for operational feasibility
    below_minimum['recommended_quantity_change_raw'] = np.maximum(
        below_minimum['target_quantity'] - below_minimum['current_quantity'],
        MIN_BOOST_QUANTITY  # Never less than minimum boost
    )
    below_minimum['recommended_quantity_change'] = np.ceil(
        below_minimum['recommended_quantity_change_raw']
    ).astype(int)
    
    # VALIDATION: Double-check no negatives
    negative_count = (below_minimum['recommended_quantity_change'] < 0).sum()
    if negative_count > 0:
        log_progress(f"ERROR: Found {negative_count} negative recommendations - fixing...")
        below_minimum.loc[below_minimum['recommended_quantity_change'] < 0, 'recommended_quantity_change'] = MIN_BOOST_QUANTITY
    
    # Calculate investment (preserve missingness; do not assume a default unit price)
    if 'unit_price' not in below_minimum.columns:
        below_minimum['unit_price'] = np.nan
    below_minimum['investment_required'] = (
        below_minimum['recommended_quantity_change'] * below_minimum['unit_price']
    )
    unknown_prices = below_minimum['unit_price'].isna().sum()
    if unknown_prices > 0:
        log_progress(f"‚ö†Ô∏è unit_price unavailable for {unknown_prices} cases; investment is NA for those rows")
    
    log_progress(f"Identified {len(below_minimum)} below minimum {ANALYSIS_LEVEL} cases")
    
    if len(below_minimum) > 0:
        # Log severity breakdown
        severity_counts = below_minimum['issue_severity'].value_counts()
        log_progress(f"  ‚Ä¢ Severity breakdown: {dict(severity_counts)}")
        
        avg_current = below_minimum['unit_rate'].mean()
        total_increase = below_minimum['increase_needed'].sum()
        log_progress(f"  ‚Ä¢ Average current unit rate: {avg_current:.3f}")
        log_progress(f"  ‚Ä¢ Total increase needed: {total_increase:.2f}")
    
    # üìà FAST FISH ENHANCEMENT: Apply sell-through validation
    if SELLTHROUGH_VALIDATION_AVAILABLE and len(below_minimum) > 0:
        log_progress("üéØ Applying Fast Fish sell-through validation...")
        
        # Initialize validator
        historical_data = load_historical_data_for_validation()
        validator = SellThroughValidator(historical_data)
        
        # Apply validation to each below minimum case
        validated_cases = []
        rejected_count = 0
        skipped_count = 0
        
        for idx, case in below_minimum.iterrows():
            # Get category name for validation (skip if missing)
            category_name = case.get('sub_cate_name')
            # Calculate quantities (skip if missing)
            current_qty = case.get('current_quantity')
            delta_qty = case.get('recommended_quantity_change')
            if pd.isna(category_name) or pd.isna(current_qty) or pd.isna(delta_qty):
                skipped_count += 1
                continue
            recommended_qty = current_qty + float(delta_qty)
            
            # Validate the below minimum increase recommendation
            # Clamp integerized quantities to reasonable bounds (0‚Äì100)
            cur_count = int(np.clip(round(current_qty), 0, 100))
            rec_count = int(np.clip(round(recommended_qty), 0, 100))

            validation = validator.validate_recommendation(
                store_code=str(case['str_code']),
                category=str(category_name),
                current_spu_count=cur_count,
                recommended_spu_count=rec_count,
                action='INCREASE',
                rule_name='Rule 9: Below Minimum'
            )
            
            # Only keep Fast Fish compliant recommendations
            if validation['fast_fish_compliant']:
                # Add sell-through metrics to case
                enhanced_case = case.to_dict()
                enhanced_case.update({
                    'current_sell_through_rate': validation['current_sell_through_rate'],
                    'predicted_sell_through_rate': validation['predicted_sell_through_rate'],
                    'sell_through_improvement': validation['sell_through_improvement'],
                    'fast_fish_compliant': validation['fast_fish_compliant'],
                    'business_rationale': validation['business_rationale'],
                    'approval_reason': validation['approval_reason']
                })
                validated_cases.append(enhanced_case)
            else:
                rejected_count += 1
        
        # Convert validated cases back to DataFrame
        if validated_cases:
            below_minimum = pd.DataFrame(validated_cases)
            log_progress(f"‚úÖ Fast Fish validation: {len(validated_cases)} approved, {rejected_count} rejected, {skipped_count} skipped (missing inputs)")
            # sell_through_improvement is measured in percentage points (pp)
            log_progress(f"‚úÖ Average sell-through improvement: {below_minimum['sell_through_improvement'].mean():.1f} pp")
        else:
            log_progress(f"‚ùå Fast Fish validation: All {rejected_count} cases rejected")
            return pd.DataFrame()
    else:
        # Add default sell-through columns for consistency
        below_minimum['current_sell_through_rate'] = np.nan
        below_minimum['predicted_sell_through_rate'] = np.nan
        below_minimum['sell_through_improvement'] = np.nan
        below_minimum['fast_fish_compliant'] = np.nan  # Preserve missingness when validator unavailable
        below_minimum['business_rationale'] = f"Below minimum {ANALYSIS_LEVEL} allocation requires increase"
        below_minimum['approval_reason'] = 'Below minimum threshold analysis'

    # Enforce per-store cap on SPU recommendations
    if (MAX_TOTAL_ADJUSTMENTS_PER_STORE is not None) and (MAX_TOTAL_ADJUSTMENTS_PER_STORE > 0) and (len(below_minimum) > 0):
        severity_rank_map = {'CRITICAL': 3, 'HIGH': 2, 'MEDIUM': 1, 'LOW': 0}
        below_minimum['severity_rank'] = below_minimum['issue_severity'].map(severity_rank_map).fillna(0).astype(int)
        # Prepare sort keys with safe defaults
        st_impr = below_minimum.get('sell_through_improvement')
        if st_impr is None:
            below_minimum['sell_through_improvement'] = np.nan
        below_minimum['ff_ok'] = below_minimum.get('fast_fish_compliant', False).fillna(False)
        below_minimum_sorted = (
            below_minimum
            .sort_values(by=['ff_ok', 'sell_through_improvement', 'severity_rank', 'recommended_quantity_change'],
                         ascending=[False, False, False, False])
        )
        # Apply cap per store
        below_minimum_sorted['rank_within_store'] = below_minimum_sorted.groupby('str_code').cumcount()
        trimmed = below_minimum_sorted[below_minimum_sorted['rank_within_store'] < MAX_TOTAL_ADJUSTMENTS_PER_STORE].copy()
        dropped = len(below_minimum_sorted) - len(trimmed)
        if dropped > 0:
            affected_stores = trimmed['str_code'].nunique()
            log_progress(f"‚úÇÔ∏è Trimmed {dropped} excess recommendations across {affected_stores} stores (cap={MAX_TOTAL_ADJUSTMENTS_PER_STORE})")
        below_minimum = trimmed.drop(columns=['severity_rank', 'rank_within_store', 'ff_ok'])
    
    # Add action and rationale
    below_minimum['recommended_action'] = 'BOOST'
    below_minimum['recommendation_text'] = below_minimum.apply(
        lambda row: f"ADD {int(row['recommended_quantity_change'])} units to reach minimum viable allocation",
        axis=1
    )
    
    log_progress(f"Final result: {len(below_minimum)} below minimum SPU cases (Fast Fish compliant)")
    log_progress(f"All quantity changes positive: {(below_minimum['recommended_quantity_change'] > 0).all()}")
    if len(below_minimum) > 0:
        log_progress(f"Average increase needed: {below_minimum['recommended_quantity_change'].mean():.1f} units")
    
    return below_minimum

def identify_below_minimum_cases_subcategory(store_data: pd.DataFrame) -> pd.DataFrame:
    """Identify subcategories below minimum planning counts (no synthetic units)."""
    log_progress("Identifying below minimum subcategory cases (planning counts)...")
    below_minimum = store_data[
        (store_data['style_count'] > 0) &
        (store_data['style_count'] < MINIMUM_STYLE_THRESHOLD)
    ].copy()
    if len(below_minimum) == 0:
        log_progress("No below minimum subcategory cases found")
        return pd.DataFrame()
    below_minimum['recommended_target'] = MINIMUM_STYLE_THRESHOLD
    below_minimum['increase_needed'] = below_minimum['recommended_target'] - below_minimum['style_count']
    below_minimum['issue_type'] = 'BELOW_MINIMUM'
    below_minimum['issue_severity'] = np.where(
        below_minimum['style_count'] < 0.5 * MINIMUM_STYLE_THRESHOLD, 'HIGH', 'MEDIUM'
    )
    log_progress(f"Identified {len(below_minimum)} subcategory cases; no unit recommendations emitted without real units")
    return below_minimum

def create_store_summary(below_minimum_cases: pd.DataFrame, cluster_df: pd.DataFrame) -> pd.DataFrame:
    """Create store-level summary for pipeline compatibility."""
    log_progress("Creating store-level summary...")
    
    # STANDARDIZATION FIX: Use cluster_id instead of Cluster for consistency
    cluster_df_standardized = cluster_df.copy()
    if 'Cluster' in cluster_df_standardized.columns:
        cluster_df_standardized['cluster_id'] = cluster_df_standardized['Cluster']
        cluster_df_standardized = cluster_df_standardized.drop('Cluster', axis=1)
    
    # Initialize results with all stores
    results_df = cluster_df_standardized[['str_code', 'cluster_id']].copy()
    
    if len(below_minimum_cases) > 0:
        # Aggregate by store (NA-aware investment totals)
        store_summary_pre = below_minimum_cases.groupby('str_code').agg({
            'sty_code': 'count',  # Count of below minimum SPUs
            'increase_needed': 'sum',
            'unit_rate': 'mean',
            'recommended_quantity_change': 'sum'
        }).reset_index()
        invest_summary = (
            below_minimum_cases.groupby('str_code')['investment_required']
            .sum(min_count=1)
            .rename('total_investment')
            .reset_index()
        )
        store_summary = store_summary_pre.merge(invest_summary, on='str_code', how='left')
        
        store_summary.columns = [
            'str_code', 'below_minimum_spus_count', 'total_increase_needed', 
            'avg_current_count', 'total_quantity_change', 'total_investment'
        ]
        
        # Merge with results
        results_df = results_df.merge(store_summary, on='str_code', how='left')
        # Create rule flag (treat missing counts as zero without mutating other fields)
        cnt = results_df['below_minimum_spus_count'].fillna(0)
        results_df['rule9_below_minimum_spu'] = (cnt > 0).astype(int)
    else:
        # No cases found
        results_df['below_minimum_spus_count'] = 0
        results_df['total_increase_needed'] = 0
        results_df['avg_current_count'] = 0
        results_df['total_quantity_change'] = 0
        results_df['total_investment'] = 0
        results_df['rule9_below_minimum_spu'] = 0
    
    # STANDARDIZATION FIX: Add missing standard columns for pipeline compatibility
    # Map total_investment to investment_required (standard column name)
    results_df['investment_required'] = results_df['total_investment']
    # Map total_quantity_change to recommended_quantity_change (standard column name) 
    results_df['recommended_quantity_change'] = results_df['total_quantity_change']
    
    # Add standard business rationale and approval columns
    results_df['business_rationale'] = results_df.apply(
        lambda row: f"Below minimum allocation detected: {row['below_minimum_spus_count']} SPUs need boosting" 
                   if row['below_minimum_spus_count'] > 0 else "No below minimum issues", axis=1
    )
    results_df['approval_reason'] = 'Automatic approval for below minimum allocation fixes'
    results_df['fast_fish_compliant'] = True  # Below minimum fixes are always compliant
    results_df['opportunity_type'] = 'BELOW_MINIMUM_ALLOCATION'
    
    # Add metadata
    results_df['rule9_description'] = 'Store has SPUs below minimum allocation level - FIXED: Only positive increases'
    results_df['rule9_threshold'] = f"<{MINIMUM_UNIT_RATE} units/15d (when >0)"
    results_df['rule9_analysis_level'] = ANALYSIS_LEVEL
    results_df['rule9_fix_applied'] = 'Negative quantities eliminated - only positive increases'
    
    flagged_stores = results_df['rule9_below_minimum_spu'].sum()
    log_progress(f"Store summary: {flagged_stores} stores flagged")
    
    return results_df

def create_store_summary_subcategory(below_minimum_cases: pd.DataFrame, cluster_df: pd.DataFrame) -> pd.DataFrame:
    """Create store-level summary for subcategory mode (no synthetic units)."""
    log_progress("Creating store-level summary (subcategory)...")
    cluster_df_standardized = cluster_df.copy()
    if 'Cluster' in cluster_df_standardized.columns:
        cluster_df_standardized['cluster_id'] = cluster_df_standardized['Cluster']
        cluster_df_standardized = cluster_df_standardized.drop('Cluster', axis=1)
    results_df = cluster_df_standardized[['str_code', 'cluster_id']].copy()
    if len(below_minimum_cases) > 0:
        store_summary = below_minimum_cases.groupby('str_code').agg({
            'category_key': 'count',
            'increase_needed': 'sum',
            'style_count': 'mean'
        }).reset_index()
        store_summary.columns = [
            'str_code', 'below_minimum_count', 'total_increase_needed', 'avg_current_count'
        ]
        results_df = results_df.merge(store_summary, on='str_code', how='left')
        cnt = results_df['below_minimum_count'].fillna(0)
        results_df['rule9_below_minimum'] = (cnt > 0).astype(int)
    else:
        results_df['below_minimum_count'] = 0
        results_df['total_increase_needed'] = 0
        results_df['avg_current_count'] = 0
        results_df['rule9_below_minimum'] = 0
    # Metadata
    results_df['business_rationale'] = results_df.apply(
        lambda row: f"Below minimum allocation detected: {row['below_minimum_count']} categories need boosting"
        if row['below_minimum_count'] > 0 else "No below minimum issues", axis=1
    )
    results_df['approval_reason'] = 'Below minimum threshold analysis'
    results_df['fast_fish_compliant'] = True
    results_df['opportunity_type'] = 'BELOW_MINIMUM_ALLOCATION_SUBCATEGORY'
    results_df['rule9_description'] = 'Store has subcategories below minimum planning counts (no synthetic units)'
    results_df['rule9_threshold'] = f"<{MINIMUM_STYLE_THRESHOLD} planning count (when >0)"
    results_df['rule9_analysis_level'] = 'subcategory'
    return results_df

def save_results_with_sellthrough(results_df: pd.DataFrame, below_minimum_cases: pd.DataFrame) -> None:
    """
    Save rule results with comprehensive sell-through summary reporting.
    
    Args:
        results_df: Store-level rule results
        below_minimum_cases: Detailed below minimum cases with sell-through metrics
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    cur_yyyymm, cur_period = get_current_period()
    target_yyyymm = os.environ.get("PIPELINE_TARGET_YYYYMM")
    target_period = os.environ.get("PIPELINE_TARGET_PERIOD")
    period_label = get_period_label(target_yyyymm or cur_yyyymm, target_period if target_yyyymm else cur_period)
    
    # Save main results with standardized pattern
    timestamped, period_file, generic = create_output_with_symlinks(
        results_df,
        f"{OUTPUT_DIR}/rule9_below_minimum_spu_sellthrough_results",
        period_label
    )
    log_progress(f"Saved store results: {timestamped}")
    results_file = timestamped

    # Register in pipeline manifest (results)
    try:
        res_meta = {
            "period_label": period_label,
            "target_year": int(period_label[:4]) if len(period_label) >= 6 else None,
            "target_month": int(period_label[4:6]) if len(period_label) >= 6 else None,
            "target_period": period_label[6:] if len(period_label) > 6 else None,
            "analysis_level": ANALYSIS_LEVEL,
            "records": int(len(results_df)),
            "columns": list(map(str, results_df.columns)),
            "spu_sales_file_used": LAST_SPU_SALES_FILE_USED,
            "unit_rate_na_count": UNIT_RATE_NA_COUNT,
            "unit_rate_total_rows": UNIT_DIAGNOSTICS_TOTAL_ROWS,
        }
        register_step_output("step9", f"rule9_results_{period_label}", results_file, res_meta)
        register_step_output("step9", "rule9_results", results_file, res_meta)
    except Exception as e:
        log_progress(f"‚ö†Ô∏è Manifest registration (results) failed: {e}")
    
    # Save detailed opportunities if any
    if len(below_minimum_cases) > 0:
        # Ensure standardized column for downstream (Step 13+)
        if 'opportunity_type' not in below_minimum_cases.columns:
            below_minimum_cases['opportunity_type'] = 'BELOW_MINIMUM'
        timestamped_opp, _, _ = create_output_with_symlinks(
            below_minimum_cases,
            f"{OUTPUT_DIR}/rule9_below_minimum_spu_sellthrough_opportunities",
            period_label
        )
        log_progress(f"Saved detailed opportunities: {timestamped_opp}")
        opportunities_file = timestamped_opp
        # Register in pipeline manifest (opportunities)
        try:
            opp_meta = {
                "period_label": period_label,
                "target_year": int(period_label[:4]) if len(period_label) >= 6 else None,
                "target_month": int(period_label[4:6]) if len(period_label) >= 6 else None,
                "target_period": period_label[6:] if len(period_label) > 6 else None,
                "analysis_level": ANALYSIS_LEVEL,
                "records": int(len(below_minimum_cases)),
                "columns": list(map(str, below_minimum_cases.columns)),
                "spu_sales_file_used": LAST_SPU_SALES_FILE_USED,
                "unit_rate_na_count": UNIT_RATE_NA_COUNT,
                "unit_rate_total_rows": UNIT_DIAGNOSTICS_TOTAL_ROWS,
                "fast_fish_compliant_count": int(below_minimum_cases.get('fast_fish_compliant', pd.Series(dtype=bool)).fillna(False).sum()) if 'fast_fish_compliant' in below_minimum_cases.columns else None,
            }
            register_step_output("step9", f"rule9_opportunities_{period_label}", opportunities_file, opp_meta)
            register_step_output("step9", "rule9_opportunities", opportunities_file, opp_meta)
        except Exception as e:
            log_progress(f"‚ö†Ô∏è Manifest registration (opportunities) failed: {e}")
        
        # Create comprehensive sell-through summary report
        create_sellthrough_summary_report(below_minimum_cases, results_df)
        
        # Verification summary with sell-through metrics
        negative_count = (below_minimum_cases['recommended_quantity_change'] < 0).sum()
        total_cases = len(below_minimum_cases)
        
        log_progress(f"VERIFICATION SUMMARY:")
        log_progress(f"  Total cases: {total_cases:,}")
        log_progress(f"  Negative quantities: {negative_count}")
        log_progress(f"  All positive: {negative_count == 0}")
        log_progress(f"  Average increase: {below_minimum_cases['recommended_quantity_change'].mean():.1f} units")
        log_progress(f"  Total investment: ${below_minimum_cases['investment_required'].sum():,.0f}")
        
        # Sell-through metrics summary
        if 'sell_through_improvement' in below_minimum_cases.columns:
            avg_improvement = below_minimum_cases['sell_through_improvement'].mean()
            compliant_count = int(below_minimum_cases['fast_fish_compliant'].fillna(False).astype(bool).sum())
            log_progress(f"  Fast Fish compliant: {compliant_count}/{total_cases} ({(compliant_count/total_cases) if total_cases else 0:.1%})")
            # Display improvement in percentage points
            log_progress(f"  Average sell-through improvement: {avg_improvement:.1f} pp")
    else:
        # Even with no opportunities, register the results-only outputs for traceability
        try:
            res_meta = {
                "period_label": period_label,
                "target_year": int(period_label[:4]) if len(period_label) >= 6 else None,
                "target_month": int(period_label[4:6]) if len(period_label) >= 6 else None,
                "target_period": period_label[6:] if len(period_label) > 6 else None,
                "analysis_level": ANALYSIS_LEVEL,
                "records": int(len(results_df)),
                "columns": list(map(str, results_df.columns)),
                "spu_sales_file_used": LAST_SPU_SALES_FILE_USED,
                "unit_rate_na_count": UNIT_RATE_NA_COUNT,
                "unit_rate_total_rows": UNIT_DIAGNOSTICS_TOTAL_ROWS,
            }
            register_step_output("step9", f"rule9_results_{period_label}", results_file, res_meta)
            register_step_output("step9", "rule9_results", results_file, res_meta)
        except Exception as e:
            log_progress(f"‚ö†Ô∏è Manifest registration (no-opps results) failed: {e}")
        
        log_progress("No below minimum cases to save")

def save_results_subcategory(results_df: pd.DataFrame, below_minimum_cases: pd.DataFrame, store_data: pd.DataFrame) -> None:
    """Save subcategory results and a concise summary (no synthetic units)."""
    cur_yyyymm, cur_period = get_current_period()
    target_yyyymm = os.environ.get("PIPELINE_TARGET_YYYYMM")
    target_period = os.environ.get("PIPELINE_TARGET_PERIOD")
    period_label = get_period_label(target_yyyymm or cur_yyyymm, target_period if target_yyyymm else cur_period)
    # Results
    timestamped, _, _ = create_output_with_symlinks(
        results_df,
        f"{OUTPUT_DIR}/rule9_below_minimum_subcategory_results",
        period_label
    )
    log_progress(f"Saved subcategory results: {timestamped}")
    results_file = timestamped
    # Cases
    if below_minimum_cases is None or len(below_minimum_cases) == 0:
        below_minimum_cases = pd.DataFrame(columns=['str_code', 'category_key', 'style_count', 'increase_needed'])
    timestamped_cases, _, _ = create_output_with_symlinks(
        below_minimum_cases,
        f"{OUTPUT_DIR}/rule9_below_minimum_subcategory_cases",
        period_label
    )
    log_progress(f"Saved subcategory cases: {timestamped_cases}")
    cases_file = timestamped_cases
    # Summary
    summary_file = f"{OUTPUT_DIR}/rule9_below_minimum_subcategory_summary_{period_label}.md"
    with open(summary_file, 'w') as f:
        f.write("# Rule 9: Below Minimum Subcategory Summary (No Synthetic Units)\n\n")
        f.write(f"**Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Analysis Level**: SUBCATEGORY\n\n")
        total_stores = len(results_df)
        flagged = results_df['rule9_below_minimum'].sum()
        total_cases = len(below_minimum_cases)
        f.write("## Executive Summary\n\n")
        f.write(f"- **Total Stores Analyzed**: {total_stores:,}\n")
        f.write(f"- **Stores Flagged**: {flagged:,}\n")
        f.write(f"- **Below Minimum Categories**: {total_cases:,}\n")
        if total_cases > 0:
            f.write(f"- **Average Current Planning Count**: {below_minimum_cases['style_count'].mean():.2f}\n")
            f.write(f"- **Total Increase Needed (planning count)**: {below_minimum_cases['increase_needed'].sum():.2f}\n")
    log_progress(f"Saved subcategory summary to {summary_file}")

def create_sellthrough_summary_report(below_minimum_cases: pd.DataFrame, results_df: pd.DataFrame) -> None:
    """
    Create comprehensive markdown summary report with sell-through analysis.
    
    Args:
        below_minimum_cases: Detailed opportunities with sell-through metrics
        results_df: Store-level results
    """
    cur_yyyymm, cur_period = get_current_period()
    target_yyyymm = os.environ.get("PIPELINE_TARGET_YYYYMM")
    target_period = os.environ.get("PIPELINE_TARGET_PERIOD")
    period_label = get_period_label(target_yyyymm or cur_yyyymm, target_period if target_yyyymm else cur_period)
    summary_file = f"{OUTPUT_DIR}/rule9_below_minimum_spu_sellthrough_summary_{period_label}.md"
    
    with open(summary_file, 'w') as f:
        f.write("# Rule 9: Below Minimum SPU Analysis - Sell-Through Enhanced\n\n")
        f.write(f"**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Analysis Level:** {ANALYSIS_LEVEL.upper()}\n")
        f.write(f"**Seasonal Blending:** {'Enabled' if USE_BLENDED_SEASONAL else 'Disabled'}\n")
        f.write(f"**Fast Fish Validation:** {'Enabled' if SELLTHROUGH_VALIDATION_AVAILABLE else 'Disabled'}\n\n")
        
        # Executive Summary
        total_stores = len(results_df)
        flagged_stores = results_df['rule9_below_minimum_spu'].sum()
        total_opportunities = len(below_minimum_cases)
        
        f.write("## Executive Summary\n\n")
        f.write(f"- **Total Stores Analyzed:** {total_stores:,}\n")
        f.write(f"- **Stores Flagged:** {flagged_stores:,} ({flagged_stores/total_stores:.1%})\n")
        f.write(f"- **Below Minimum Opportunities:** {total_opportunities:,}\n")
        
        if total_opportunities > 0:
            total_investment = below_minimum_cases['investment_required'].sum()
            total_quantity = below_minimum_cases['recommended_quantity_change'].sum()
            f.write(f"- **Total Investment Required:** ${total_investment:,.0f}\n")
            f.write(f"- **Total Quantity Increase:** {total_quantity:.0f} units\n")
            
            # Sell-through metrics
            if 'sell_through_improvement' in below_minimum_cases.columns:
                avg_improvement = below_minimum_cases['sell_through_improvement'].mean()
                compliant_count = int(below_minimum_cases['fast_fish_compliant'].fillna(False).astype(bool).sum())
                f.write(f"- **Fast Fish Compliant:** {compliant_count}/{total_opportunities} ({(compliant_count/total_opportunities) if total_opportunities else 0:.1%})\n")
                # Report improvement in percentage points
                f.write(f"- **Average Sell-Through Improvement:** {avg_improvement:.1f} pp\n")
        
        f.write("\n")
        
        # Business Impact Analysis
        f.write("## Business Impact Analysis\n\n")
        if total_opportunities > 0:
            # Severity breakdown
            if 'issue_severity' in below_minimum_cases.columns:
                severity_counts = below_minimum_cases['issue_severity'].value_counts()
                f.write("### Severity Distribution\n\n")
                for severity, count in severity_counts.items():
                    f.write(f"- **{severity}:** {count:,} opportunities ({count/total_opportunities:.1%})\n")
                f.write("\n")
            
            # Top opportunities by investment
            f.write("### Top 10 Opportunities by Investment\n\n")
            top_opportunities = below_minimum_cases.nlargest(10, 'investment_required')
            f.write("| Store | SPU | Current | Recommended | Investment | Sell-Through Improvement |\n")
            f.write("|-------|-----|---------|-------------|------------|-------------------------|\n")
            
            for _, opp in top_opportunities.iterrows():
                store = opp['str_code']
                spu = opp.get('sty_code', 'N/A')
                current = opp.get('current_quantity', np.nan)
                recommended = opp.get('recommended_quantity_change', np.nan)
                investment = opp.get('investment_required', np.nan)
                improvement = opp.get('sell_through_improvement', np.nan)
                current_str = f"{current:.1f}" if pd.notna(current) else "NA"
                recommended_str = f"{int(recommended)}" if pd.notna(recommended) else "NA"
                investment_str = f"${investment:.0f}" if pd.notna(investment) else "NA"
                # Improvement measured in percentage points
                improvement_str = f"{improvement:.1f} pp" if pd.notna(improvement) else "NA"
                f.write(f"| {store} | {spu} | {current_str} | +{recommended_str} | {investment_str} | {improvement_str} |\n")
            
            f.write("\n")
        else:
            f.write("No below minimum opportunities identified.\n\n")
        
        # Seasonal Enhancement Summary
        if USE_BLENDED_SEASONAL:
            f.write("## Seasonal Enhancement Benefits\n\n")
            f.write("- **Enhanced August Recommendations:** Blending recent trends with seasonal patterns\n")
            f.write("- **Strategic Planning:** Supporting autumn transition with historical data\n")
            f.write("- **Data Quality:** Real historical data, no synthetic assumptions\n\n")
        
        # Technical Details
        f.write("## Technical Configuration\n\n")
        f.write(f"- **Minimum Unit Threshold:** {MINIMUM_UNIT_RATE} units per 15 days\n")
        f.write(f"- **Minimum Boost Quantity:** {MIN_BOOST_QUANTITY}\n")
        f.write(f"- **Data Period:** {DATA_PERIOD_DAYS} days\n")
        f.write(f"- **Target Period:** {TARGET_PERIOD_DAYS} days\n")
        f.write(f"- **Scaling Factor:** {SCALING_FACTOR}\n")
        
        # Quantity sourcing diagnostics
        f.write("\n## Quantity Sourcing Diagnostics\n\n")
        qty_file = LAST_SPU_SALES_FILE_USED if LAST_SPU_SALES_FILE_USED else 'Unknown'
        f.write(f"- **SPU Sales File Used:** {qty_file}\n")
        f.write(f"- **Rows with Unit Rate NA:** {UNIT_RATE_NA_COUNT} / {UNIT_DIAGNOSTICS_TOTAL_ROWS}\n")
        
        # Critical Fixes Applied
        f.write("\n## Critical Fixes Applied\n\n")
        f.write("- ‚úÖ **No Negative Quantities:** All recommendations are positive increases only\n")
        f.write("- ‚úÖ **Integer Rounding:** Recommended quantity increases are rounded up to whole units\n")
        f.write("- ‚úÖ **Business Logic Alignment:** Below minimum = increase allocation\n")
        f.write("- ‚úÖ **Fast Fish Validation:** Only sell-through compliant recommendations\n")
        f.write("- ‚úÖ **Seasonal Blending:** Enhanced August recommendations with historical patterns\n")
        f.write("- ‚úÖ **Output Standardization:** Consistent column naming and pipeline compatibility\n")
    
    log_progress(f"Created comprehensive summary report: {summary_file}")

    # Register summary in pipeline manifest (generic + period-specific)
    try:
        sum_meta = {
            "period_label": period_label,
            "target_year": int(period_label[:4]) if len(period_label) >= 6 else None,
            "target_month": int(period_label[4:6]) if len(period_label) >= 6 else None,
            "target_period": period_label[6:] if len(period_label) > 6 else None,
            "analysis_level": ANALYSIS_LEVEL,
            "total_stores": int(total_stores),
            "flagged_stores": int(flagged_stores),
            "total_opportunities": int(total_opportunities),
        }
        register_step_output("step9", f"rule9_summary_{period_label}", summary_file, sum_meta)
        register_step_output("step9", "rule9_summary", summary_file, sum_meta)
    except Exception as e:
        log_progress(f"‚ö†Ô∏è Manifest registration (summary) failed: {e}")

def main() -> None:
    """Main function - simplified and fixed version."""
    start_time = datetime.now()
    log_progress("Starting Rule 9: Below Minimum SPU Analysis - FIXED VERSION")
    
    try:
        # Load data
        cluster_df, data_df = load_data()
        
        if ANALYSIS_LEVEL == 'subcategory':
            # Subcategory path: no synthetic units
            store_data = prepare_subcategory_data(data_df, cluster_df)
            if len(store_data) == 0:
                log_progress("No valid subcategory data found. Creating empty results.")
                results_df = cluster_df[['str_code', 'Cluster']].copy()
                results_df['rule9_below_minimum'] = 0
                results_df['below_minimum_count'] = 0
                cur_yyyymm, cur_period = get_current_period()
                target_yyyymm = os.environ.get("PIPELINE_TARGET_YYYYMM")
                target_period = os.environ.get("PIPELINE_TARGET_PERIOD")
                period_label = get_period_label(target_yyyymm or cur_yyyymm, target_period if target_yyyymm else cur_period)
                labeled_empty = f"{OUTPUT_DIR}/rule9_below_minimum_subcategory_results_{period_label}.csv"
                results_df.to_csv(labeled_empty, index=False)
                log_progress(f"Saved empty subcategory results to {labeled_empty}")
                return
            below_minimum_cases = identify_below_minimum_cases_subcategory(store_data)
            results_df = create_store_summary_subcategory(below_minimum_cases, cluster_df)
            save_results_subcategory(results_df, below_minimum_cases, store_data)
        else:
            # SPU path: unit-based, positive-only with Fast Fish
            store_data = prepare_spu_data(data_df, cluster_df)

            if len(store_data) == 0:
                log_progress("No valid SPU data found. Creating empty results.")
                results_df = cluster_df[['str_code', 'Cluster']].copy()
                results_df['rule9_below_minimum_spu'] = 0
                results_df['below_minimum_spus_count'] = 0
                results_df['total_quantity_change'] = 0
                results_df['rule9_description'] = 'No valid SPU data for analysis'
                cur_yyyymm, cur_period = get_current_period()
                target_yyyymm = os.environ.get("PIPELINE_TARGET_YYYYMM")
                target_period = os.environ.get("PIPELINE_TARGET_PERIOD")
                period_label = get_period_label(target_yyyymm or cur_yyyymm, target_period if target_yyyymm else cur_period)
                labeled_empty = f"{OUTPUT_DIR}/rule9_below_minimum_spu_sellthrough_results_{period_label}.csv"
                results_df.to_csv(labeled_empty, index=False)
                unlabeled_empty = f"{OUTPUT_DIR}/rule9_below_minimum_spu_sellthrough_results.csv"
                results_df.to_csv(unlabeled_empty, index=False)
                log_progress(f"Saved empty results to {labeled_empty} and {unlabeled_empty}")
                return

            # Identify below minimum cases
            below_minimum_cases = identify_below_minimum_cases(store_data)
            # Create store summary
            results_df = create_store_summary(below_minimum_cases, cluster_df)
            # Save results with sell-through enhancement
            save_results_with_sellthrough(results_df, below_minimum_cases)
        
        # SEASONAL BLENDING BENEFITS: Log the strategic advantages for August
        if USE_BLENDED_SEASONAL:
            log_progress("\nüçÇ SEASONAL BLENDING BENEFITS FOR BELOW MINIMUM ANALYSIS:")
            log_progress("   ‚úì Summer clearance items: Identified from recent trends (May-July)")
            log_progress("   ‚úì Autumn planning items: Identified from seasonal patterns (Aug 2024)")
            log_progress("   ‚úì Balanced approach: Avoids over-focusing on summer-only styles")
            log_progress("   ‚úì Strategic planning: Supports autumn inventory preparation")
            log_progress("   ‚úì Business rationale: Below minimum analysis considers seasonal transitions")
        
        # Summary
        duration = (datetime.now() - start_time).total_seconds()
        flagged_stores = results_df['rule9_below_minimum_spu'].sum()
        
        log_progress("\n" + "="*70)
        log_progress("RULE 9: BELOW MINIMUM SPU ANALYSIS - ENHANCED FIXED VERSION COMPLETE")
        log_progress("="*70)
        log_progress(f"‚úÖ CRITICAL FIX APPLIED: No negative quantities")
        if USE_BLENDED_SEASONAL:
            log_progress(f"üçÇ SEASONAL ENHANCEMENT: Blended data for August recommendations")
        log_progress(f"Process completed in {duration:.2f} seconds")
        log_progress(f"‚úì Stores analyzed: {len(results_df):,}")
        log_progress(f"‚úì Stores flagged: {flagged_stores:,}")
        log_progress(f"‚úì Below minimum SPUs: {len(below_minimum_cases):,}")
        
        if len(below_minimum_cases) > 0:
            total_increase = below_minimum_cases['recommended_quantity_change'].sum()
            total_investment = below_minimum_cases['investment_required'].sum()
            log_progress(f"‚úì Total quantity to ADD: {total_increase:.0f} units")
            log_progress(f"‚úì Estimated investment: ${total_investment:,.0f}")
            log_progress(f"‚úì All recommendations are POSITIVE increases ‚úÖ")
            
            if USE_BLENDED_SEASONAL:
                log_progress(f"‚úì Seasonal context: Recommendations balance summer clearance + autumn planning")
        
        log_progress(f"\nüéØ KEY ENHANCEMENTS APPLIED:")
        log_progress(f"   ‚Ä¢ Below minimum logic: INCREASE only (never decrease) ‚úÖ")
        log_progress(f"   ‚Ä¢ Negative quantities: ELIMINATED ‚úÖ")
        log_progress(f"   ‚Ä¢ Business logic: CORRECTED ‚úÖ")
        log_progress(f"   ‚Ä¢ Performance: OPTIMIZED ‚úÖ")
        if USE_BLENDED_SEASONAL:
            log_progress(f"   ‚Ä¢ Seasonal blending: August recommendations enhanced ‚úÖ")
            log_progress(f"   ‚Ä¢ Strategic planning: Autumn transition support ‚úÖ")
        
    except Exception as e:
        log_progress(f"Error in Rule 9 analysis: {str(e)}")
        raise

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Rule 9: Below Minimum SPU Analysis")
    parser.add_argument("--yyyymm", type=str, help="YYYYMM period (e.g., 202508)")
    parser.add_argument("--period", type=str, choices=["A", "B", "full"], help="A/B or full")
    parser.add_argument("--analysis-level", type=str, choices=["spu", "subcategory"], default=ANALYSIS_LEVEL)
    parser.add_argument("--seasonal-blending", action="store_true", help="Enable seasonal blending")
    parser.add_argument("--seasonal-yyyymm", type=str, help="Seasonal data YYYYMM (e.g., 202408)")
    parser.add_argument("--seasonal-period", type=str, choices=["A", "B", "full"], help="Seasonal A/B or full")
    parser.add_argument("--seasonal-weight", type=float, default=SEASONAL_WEIGHT, help="Weight for seasonal data (0-1)")
    parser.add_argument("--seasonal-yyyymm-list", type=str, help="Comma-separated YYYYMM list for explicit seasonal months (e.g., 202409,202309)")
    parser.add_argument("--target-yyyymm", type=str, help="Target YYYYMM label for outputs (e.g., 202509)")
    parser.add_argument("--target-period", type=str, choices=["A", "B"], help="Target half-month label for outputs")
    parser.add_argument("--min-threshold", type=float, help="Below-minimum threshold (default 0.03)")
    parser.add_argument("--min-boost", type=float, help="Minimum positive quantity boost (default 0.5)")
    parser.add_argument("--unit-price", type=float, help="Default unit price for investment calc (default 50)")
    args = parser.parse_args()

    # Initialize config
    init_period = None if (args.period == "full") else args.period
    initialize_pipeline_config(yyyymm=args.yyyymm, period=init_period, analysis_level=args.analysis_level)

    # Override globals from CLI
    ANALYSIS_LEVEL = args.analysis_level
    if args.min_threshold is not None:
        MINIMUM_STYLE_THRESHOLD = args.min_threshold
    if args.min_boost is not None:
        MIN_BOOST_QUANTITY = args.min_boost
    if args.unit_price is not None:
        UNIT_PRICE_DEFAULT = args.unit_price

    USE_BLENDED_SEASONAL = args.seasonal_blending
    SEASONAL_YYYYMM = args.seasonal_yyyymm
    SEASONAL_PERIOD = None if (args.seasonal_period == "full") else args.seasonal_period
    SEASONAL_WEIGHT = args.seasonal_weight
    if getattr(args, 'seasonal_yyyymm_list', None):
        # Pass list via environment so loaders can access without threading args
        os.environ["SEASONAL_YYYYMM_LIST"] = args.seasonal_yyyymm_list
    RECENT_WEIGHT = 1.0 - SEASONAL_WEIGHT

    # Target output labeling (align with Step 8)
    if args.target_yyyymm:
        os.environ["PIPELINE_TARGET_YYYYMM"] = args.target_yyyymm
    if args.target_period:
        os.environ["PIPELINE_TARGET_PERIOD"] = args.target_period

    main()