"""
Step definitions for cluster balancing Gherkin scenarios.

These implement the steps defined in step6-cluster-balancing.feature
"""

from pytest_bdd import scenarios, given, when, then, parsers
import pytest
import numpy as np
import pandas as pd
import sys
sys.path.insert(0, 'src')

from steps.cluster_analysis_step import ClusterAnalysisStep
from core.config import ClusterAnalysisConfig
from core.context import StepContext

# Load all scenarios from the feature file
scenarios('../features/step6-cluster-balancing.feature')


# Fixtures for test context
@pytest.fixture
def test_context():
    """Shared test context for scenarios."""
    return {
        'config': None,
        'step': None,
        'pca_features': None,
        'initial_labels': None,
        'balanced_labels': None,
        'initial_quality': None,
        'final_quality': None,
        'context': None
    }


# Background steps
@given('the cluster analysis step is configured')
def cluster_analysis_configured(test_context):
    """Configure cluster analysis step."""
    test_context['config'] = ClusterAnalysisConfig(
        matrix_type='subcategory',
        target_yyyymm='202508',
        target_period='A',
        output_dir='output',
        pca_components=20,
        target_cluster_size=50,
        min_cluster_size=30,
        max_cluster_size=60,
        enable_cluster_balancing=True,
        max_balance_iterations=100,
        enable_temperature_constraints=False,
        random_state=42
    )
    test_context['step'] = ClusterAnalysisStep(test_context['config'])


@given(parsers.parse('the target cluster size is {size:d} stores'))
def set_target_cluster_size(test_context, size):
    """Set target cluster size."""
    test_context['config'].target_cluster_size = size


@given(parsers.parse('the minimum cluster size is {size:d} stores'))
def set_min_cluster_size(test_context, size):
    """Set minimum cluster size."""
    test_context['config'].min_cluster_size = size


@given(parsers.parse('the maximum cluster size is {size:d} stores'))
def set_max_cluster_size(test_context, size):
    """Set maximum cluster size."""
    test_context['config'].max_cluster_size = size


# Given steps
@given(parsers.parse('I have a dataset with {n_stores:d} stores'))
def create_dataset(test_context, n_stores):
    """Create a dataset with specified number of stores."""
    np.random.seed(42)
    test_context['pca_features'] = pd.DataFrame(
        np.random.randn(n_stores, 10),
        columns=[f'PC{i}' for i in range(10)]
    )


@given('I have performed initial KMeans clustering with 46 clusters')
def perform_initial_clustering_46(test_context):
    """Perform initial KMeans clustering."""
    n_stores = len(test_context['pca_features'])
    test_context['initial_labels'] = np.random.randint(0, 46, size=n_stores)


@given('I have performed initial KMeans clustering')
def perform_initial_clustering(test_context):
    """Perform initial KMeans clustering."""
    n_stores = len(test_context['pca_features'])
    n_clusters = max(2, n_stores // 50)  # ~50 stores per cluster
    test_context['initial_labels'] = np.random.randint(0, n_clusters, size=n_stores)


@given(parsers.parse('the initial clusters are unbalanced with sizes ranging from {min_size:d} to {max_size:d} stores'))
def verify_initial_unbalanced(test_context, min_size, max_size):
    """Verify initial clusters are unbalanced."""
    # Create intentionally unbalanced labels
    n_stores = len(test_context['pca_features'])
    # Create highly unbalanced distribution
    test_context['initial_labels'] = np.concatenate([
        np.full(max_size, 0),  # One large cluster
        np.full(min_size, 1),  # One small cluster
        np.random.randint(2, 10, size=n_stores - max_size - min_size)  # Rest distributed
    ])
    np.random.shuffle(test_context['initial_labels'])


@given(parsers.parse('the maximum balance iterations is set to {iterations:d}'))
def set_max_iterations(test_context, iterations):
    """Set maximum balance iterations."""
    test_context['config'].max_balance_iterations = iterations


@given('I have calculated the initial silhouette score')
def calculate_initial_quality(test_context):
    """Calculate initial clustering quality."""
    # This would call the actual quality calculation
    test_context['initial_quality'] = 0.5  # Placeholder


@given(parsers.parse('I configure target cluster size to {size:d} stores'))
def configure_target_size(test_context, size):
    """Configure target cluster size."""
    test_context['config'].target_cluster_size = size


@given(parsers.parse('I configure minimum cluster size to {size:d} stores'))
def configure_min_size(test_context, size):
    """Configure minimum cluster size."""
    test_context['config'].min_cluster_size = size


@given(parsers.parse('I configure maximum cluster size to {size:d} stores'))
def configure_max_size(test_context, size):
    """Configure maximum cluster size."""
    test_context['config'].max_cluster_size = size


@given('I enable temperature-aware clustering')
def enable_temperature_clustering(test_context):
    """Enable temperature-aware clustering."""
    test_context['config'].enable_temperature_constraints = True


@given('I enable cluster balancing')
def enable_balancing(test_context):
    """Enable cluster balancing."""
    test_context['config'].enable_cluster_balancing = True


@given('I disable cluster balancing')
def disable_balancing(test_context):
    """Disable cluster balancing."""
    test_context['config'].enable_cluster_balancing = False


# When steps
@when('I apply cluster balancing')
def apply_balancing(test_context):
    """Apply cluster balancing."""
    step = test_context['step']
    test_context['balanced_labels'] = step._balance_clusters(
        pca_features=test_context['pca_features'],
        initial_labels=test_context['initial_labels'],
        target_size=test_context['config'].target_cluster_size,
        min_size=test_context['config'].min_cluster_size,
        max_size=test_context['config'].max_cluster_size
    )


@when('I perform clustering analysis')
def perform_clustering_analysis(test_context):
    """Perform full clustering analysis."""
    step = test_context['step']
    context = StepContext()
    test_context['context'] = step.execute(context)


# Then steps
@then(parsers.parse('all clusters should have between {min_size:d} and {max_size:d} stores'))
def verify_cluster_sizes(test_context, min_size, max_size):
    """Verify all clusters are within size constraints."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    for cluster_id in unique_clusters:
        size = np.sum(labels == cluster_id)
        assert min_size <= size <= max_size, \
            f"Cluster {cluster_id} has {size} stores, expected [{min_size}, {max_size}]"


@then(parsers.parse('the mean cluster size should be approximately {target:d} stores'))
def verify_mean_cluster_size(test_context, target):
    """Verify mean cluster size is close to target."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    cluster_sizes = [np.sum(labels == c) for c in unique_clusters]
    mean_size = np.mean(cluster_sizes)
    
    # Allow Â±5 stores tolerance
    assert abs(mean_size - target) <= 5, \
        f"Mean cluster size {mean_size:.1f}, expected ~{target}"


@then(parsers.parse('the standard deviation should be less than {max_std:d} stores'))
def verify_std_dev(test_context, max_std):
    """Verify standard deviation is below threshold."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    cluster_sizes = [np.sum(labels == c) for c in unique_clusters]
    std_dev = np.std(cluster_sizes)
    
    assert std_dev < max_std, \
        f"Std dev {std_dev:.1f} >= {max_std}"


@then(parsers.parse('no cluster should have fewer than {min_size:d} stores'))
def verify_no_small_clusters(test_context, min_size):
    """Verify no clusters are too small."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    for cluster_id in unique_clusters:
        size = np.sum(labels == cluster_id)
        assert size >= min_size, \
            f"Cluster {cluster_id} has {size} stores < {min_size}"


@then(parsers.parse('no cluster should have more than {max_size:d} stores'))
def verify_no_large_clusters(test_context, max_size):
    """Verify no clusters are too large."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    for cluster_id in unique_clusters:
        size = np.sum(labels == cluster_id)
        assert size <= max_size, \
            f"Cluster {cluster_id} has {size} stores > {max_size}"


@then(parsers.parse('the balancing should converge within {max_iter:d} iterations'))
def verify_convergence(test_context, max_iter):
    """Verify balancing converged within max iterations."""
    # This would check iteration count tracked during balancing
    # For now, just verify constraints are met
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    min_size = test_context['config'].min_cluster_size
    max_size = test_context['config'].max_cluster_size
    
    for cluster_id in unique_clusters:
        size = np.sum(labels == cluster_id)
        assert min_size <= size <= max_size, \
            f"Cluster {cluster_id} not within constraints (not converged)"


@then('the final cluster sizes should meet all constraints')
def verify_all_constraints(test_context):
    """Verify all cluster size constraints are met."""
    labels = test_context['balanced_labels']
    unique_clusters = np.unique(labels)
    
    min_size = test_context['config'].min_cluster_size
    max_size = test_context['config'].max_cluster_size
    
    for cluster_id in unique_clusters:
        size = np.sum(labels == cluster_id)
        assert min_size <= size <= max_size, \
            f"Cluster {cluster_id} violates constraints"


@then(parsers.parse('the silhouette score should not decrease by more than {threshold:f}'))
def verify_quality_preserved(test_context, threshold):
    """Verify clustering quality is preserved."""
    initial_score = test_context.get('initial_quality', 0.5)
    final_score = test_context.get('final_quality', 0.45)
    
    degradation = initial_score - final_score
    assert degradation <= threshold, \
        f"Quality degraded by {degradation:.3f} > {threshold}"


@then('stores should be moved to their nearest available cluster')
def verify_nearest_cluster_assignment(test_context):
    """Verify stores moved to nearest clusters."""
    # This would require tracking which stores moved and verifying
    # they went to nearest non-full cluster
    # For now, just verify balancing occurred
    assert test_context['balanced_labels'] is not None


@then('the farthest stores from cluster centers should be moved first')
def verify_farthest_stores_moved(test_context):
    """Verify farthest stores are moved first."""
    # This would require tracking move order
    # For now, just verify balancing occurred
    assert test_context['balanced_labels'] is not None
